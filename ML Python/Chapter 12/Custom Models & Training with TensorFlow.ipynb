{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3a9202-230c-43d6-9f90-9f62bbfb857b",
   "metadata": {},
   "source": [
    "# Custom Models & Training with TensorFlow\n",
    "\n",
    "Up until now, we've only used TensorFlow's high-level API, tf.keras, but it already got us pretty far: we built various neural network architectures, including regression & classification nets, wide & deep nets, & self-normalising nets, using all sorts of techniques, such as batch normalisation, dropout, & learning rate schedules. In fact, 95% of the use cases you will encounter will not require anything other than tf.keras. But now it's time to dive deeper into TensorFlow & take a look at its lower-level Python API. This will be useful when you need extra control to write custom loss functions, custom metrics, layers, models, initializers, regularizers, weight constraints, & more. You may need to fully control the training loop itself, for example to apply special transformation constraints to the gradients (beyond just clipping them) or to use multiple optimisers for different parts of the network. We will cover all these cases & look at how we can boost our custom models & training algorithms using TensorFlow's automatic graph generation feature. First, we'll take a quick tour of TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b72e6-a99d-496d-b1d6-153287b7524c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c0f10-7fd0-4a1b-884e-3b6e88b9207a",
   "metadata": {},
   "source": [
    "# A Quick Tour of TensorFlow\n",
    "\n",
    "As you know, TensorFlow is a powerful library for numerical computation, particularly well suited & fine-tuned for large-scale machine learning (but you could use it for anything else that requires heavy computations). It was developed by the Google Brain team & it powers many of Google's large-scale services, such as Google Cloud Speech, Google Photos, & Google Search. It was open sourced in November 2015, & it is now the most popular deep learning library. Countless projects use TensorFlow for all sorts of machine learning tasks, such as image classification, natural language processing, recommender systems, & time series forecasting.\n",
    "\n",
    "So what does TensorFlow offer? Here's a summary:\n",
    "\n",
    "* Its core is very similar to numpy, but with GPU support.\n",
    "* It supports distributed computing (across multiple devices & servers).\n",
    "* It includes a kind of just-in-time (JIT) compiler that allows it to optimise computations for speed & memory usage. It works by extracting the *computation graph* from a Python function, then optimizing it (e.g., by pruning unused nodes), & running it efficiently (e.g., by automatically running independent operations in parallel).\n",
    "* Computation graphs can be extorted to a portable format, so you can train a TensorFlow model in one environment (e.g, using Python on Linux) & run it in another (e.g., using Java on an Android device).\n",
    "* It implements autodiff & provides some excellent optimizers, such as RMSProp & Nadam, so you can easily minimise all sorts of loss functions.\n",
    "\n",
    "TensorFlow offers many more features built on top of these core featuers: the most important is of course `tf.keras`, but it also has data loading & preprocessing ops (`tf.data`, `tf.io`, etc.), image processing ops (`tf.image`), signal preprocessing ops (`tf.signal`), & more.\n",
    "\n",
    "<img src = \"Images/TensorFlow Python API.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "At the lowest level, each TensorFlow operation (*op* for short) is implemented using highly efficiency C++ code. Many operations have multiple implementations called *kernels*: each kernel is dedicated to a specific device type, such as CPUs, GPUs, or even TPUs (*tensor processing units*). As you may know, GPUs can dramatically speed up computations by splitting them into many smaller chunks & running them in parallel across many GPU threads. TPUs are even faster: they are custom ASIC chips built specifically for deep learning operations.\n",
    "\n",
    "TensorFlow's architecture is shown below. Most of the time, your code will use the high-level APIs (especially tf.keras & tf.data); but when you need more flexibility, you will use the lower-level Python API, handling tensors directly. Note that APIs for other languages are also available. In any case, TensorFlow's execution engine will take care of running the operations efficiently, even across multiple devices & machines if you tell it to.\n",
    "\n",
    "<img src = \"Images/TensorFlow's Architecture.png\" width = \"550\" style = \"margin:auto\"/>\n",
    "\n",
    "TensorFlow runs not only on Windows, Linux, & macOS, but also on mobile devices (using *TensorFlow Lites*, including both iOS & Android. If you do not want to use the Python API, there are C++, Java, Go, & Swift APIs. There is even a Javascript implmentation called *TensorFlow.js* that makes it possible to run your models directly in your browser.\n",
    "\n",
    "There's more to TensorFlow than the library. TensorFlow is at the center of an extensive ecosystem of libraries. First, there's TensorBoard for visualisation. Next, there's TensorFlow Extended (TFX), which is a set of libraries built by Google to productionise TensorFlow projects: it includes tools for data validation, preprocessing, model analysis, & serving. Google's *TensorFlow Hub* provides a way to easily download & reuse pretrained neural networks. You can also get many neural network architectures, some of them pre-trained, in TensorFlow's model garden. Check out the TensorFlow Resources for more TensorFlow-based projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5dfa1-97dc-4284-851d-7a34e3e6134a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66494e-5c6e-4729-ad55-1951b6269692",
   "metadata": {},
   "source": [
    "# Using TensorFlow like NumPy\n",
    "\n",
    "TensorFlow's API revolves around *tensors*, which flow from operation to operation -- hence the name *TensorFlow*. A tensor is very similar to a NumPy `ndarray`: it is usually a multidimensional array, but it can also hold a scaler (a simple value, such as 42). These tensors will be important when we create custom cost functions, custom metrics, custom layers, & more, so let's see how to create & manipulate them.\n",
    "\n",
    "## Tensors & Operations\n",
    "\n",
    "You can create a tensor with `tf.constant()`. For example, here is a tensor representing a matrix with two rows & three columns of floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94a12bb-e6ad-4cdc-8734-b37cff288d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.constant([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a6f6a3-8549-43f3-b92f-d5af30837797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515fdab5-2543-413f-9dee-3fbeac994f18",
   "metadata": {},
   "source": [
    "Just like an `ndarray` a `tf.Tensor` has a shape & a data type (`dtype`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f18bf6-6916-4652-8e00-93564609dc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b74480-a940-4a27-84b5-f4a73d7025fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007e7c1-5562-41cf-8d55-fefc8be80ee8",
   "metadata": {},
   "source": [
    "Indexing works much like in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9bce478-992a-414a-9a25-7ec60fd80187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0471e5c-ff8c-4fb8-999d-a3bc97b2ae11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee41b1-a429-47a9-9e60-84ca22ea954a",
   "metadata": {},
   "source": [
    "Most importantly, all sorts of tensor operations are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8376d45-1faa-4b60-ad13-566786114b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be7dd71-be42-4f29-a15e-f89a620b2b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8e8867-dbda-453f-9529-a4de141419a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464091c-ff95-4d62-8218-27ecd823cbe2",
   "metadata": {},
   "source": [
    "Note that writing `t + 10` is equivalent to calling `tf.add(t, 10)` (indeed, Python calls the magic method `t._add__(10)`, which just calls `tf.add(t, 10)`. Other operators like - & * are also supported. The @ operator was added in Python 3.5, for matrix multiplication: it is equivalent to calling the `tf.matmul()` function.\n",
    "\n",
    "You will find all the basic math operations you need (`tf.add()`, `tf.multiply()`, `tf.square()`, `tf.exp()`, `tf.sqrt()`, etc.) & more operations that you can find in NumPy (e.g., `tf.reshape()`, `tf.squeeze()`, `tf.tile()`). Some functions have a different name than in NumPy; for instance, `tf.reduce_mean()`, `tf.reduce_sum()`, `tf.reduce_max()`, & `tf.math.log()` are equivalent of `np.mean()`, `np.sum()`, `np.max()`, & `np.log()`. When the name differs, there is often a good reason for it. For example, in TensorFlow, you must write `tf.transpose(t)`; you cannot just write `t.T` like in NumPy. The reason is that the `tf.transpose()` function does not do exactly the same thing as NumPy's `T` attribute: in TensorFlow, a new tensor is created with its own copy of the transposed data, while in NumPy, `t.T` is just a transposed view on the same data. Similarly, the `tf.reduce_sum()` operation is named this way because its GPU kernel (i.e., GPU implementation) uses a reduce algorithm that does not guarantee the order in which the elements are added: because 32-bit floats have limited precision, the result may change slightly every time you call this operation. The same is true of `tf.reduce_mean()` (but of course `tf.reduce_max()` is deterministic).\n",
    "\n",
    "## Tensors & NumPy\n",
    "\n",
    "Tensors play nice with NumPy: you can create a tensor from a NumPy array, & vice versa. You can even apply TensorFlow operations to NumPy arrays & NumPy operations to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a28756-9140-4e80-a884-49bd97bdbe76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2a3af7f-d694-4ace-98e9-5b30b89a4898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2bd1fbf-dcb3-4b4f-86a3-d4d6f92c515b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4904dc-814d-4541-836e-a3f829781bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbd6e7-2290-4ace-9f35-0ce6cdabd3a5",
   "metadata": {},
   "source": [
    "## Type Conversions\n",
    "\n",
    "Type conversion can significantly hurt performance, & they can easily go unnoticed when they are done automatically. To avoid this, TensorFlow does not perform any type conversions automatically: it just raises an exception if you try to execute an operation on tensors with incompatible types. For example, you cannot add a flow tensor & an integer tensor, & you canot even add a 32-bit float & a 64-bit float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6bc7081-8c4c-48a2-a740-4d773cc6581b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_45894/308422390.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "tf.constant(2.) + tf.constant(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e09e61a-f762-4c4f-b8cf-5f5975d7dd13",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_45894/3958370605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "tf.constant(2.) + tf.constant(40., dtype = tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916457ce-b79b-4553-9040-61176f0d32d0",
   "metadata": {},
   "source": [
    "This may be a bit annoying at first, but remember that it's for a good cause. & of course, you can use `tf.cast()` when you really need to convert types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82e3189d-15f0-44b6-8e1b-ed917cd8cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype = tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73999956-55d9-439e-8235-2270ba64cb44",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "The `tf.Tensor` values we've seen so far are immutable: you cannot modify them. This means that we cannot use regular tensors to implement weights in a neural network, since they need to be tweaked by backpropagation. Plus, other parameters may also need to change over time (e.g., a momentum optimizer keeps track of past gradients). What we need is a `tf.Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6140dd1-b8dc-4630-bea3-f239f926d4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a12dfd-7f30-4c4a-baad-8b2607037d91",
   "metadata": {},
   "source": [
    "A `tf.Variable` acts much like a `tf.Tensor`: you can perform the same operations with it, it plays nicely with NumPy as well, & it is just as picky with types. But it can also be modified in place using the `assign()` method (or `assign_add()` or `assign_sub()`, which increment or decrement the variable by the given value). You can also modify individual cells (or slices), by using the cell's (or slice's) `assign()` method (direct item assignment will not work) or by using the `scatter_update()` or `scatter_nd_update()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60392070-152a-4c3d-8aa7-75c3e278c1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bc68d26-db16-4e57-8097-9e2ce06fd0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d4abffa-52e9-49fb-b745-12f51ab1ccd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02c7760c-9bda-41f8-ba0b-f43e13c2b846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices = [[0, 0], [1, 2]], updates = [100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c0633-f0cb-4e29-bd8f-af1e586049c3",
   "metadata": {},
   "source": [
    "## Other Data Structures\n",
    "\n",
    "TensorFlow supports several other data structures, including the following:\n",
    "\n",
    "* *Sparse Tensors* (`tf.SparseTensor`)\n",
    "   - Efficiently represent tensors containing mostly zeros. The `tf.sparse` package contains operations for sparse tensors.\n",
    "* *Tensor Arrays* (`tf.TensorArray`)\n",
    "   - Are lists of tensors. They have a fixed size by default but can optionally be made dynamic. All tensors they contain must have the same shape & data type.\n",
    "* *Ragged Tensors* (`tf.RaggedTensor`)\n",
    "   - Represent static lists of lists of tensors, where every tensor has the same shape & data type. The `tf.ragged` package contains operations for ragged tensors.\n",
    "* *String Tensors*\n",
    "   - Are regular tensors of type `tf.string`. These represent byte strings, not Unicode strings, so if you create a string tensor using a Unicode string (e.g., a regular python 3 string like `\"cafe\"`), then it will get encoded to UTF-8 automatically (e.g., `b\"caf\\xc3\\xa9\"`). Alternatively, you can represent Unicode strings using tensors of type `tf.int32`, where each item represents a Unicode code point (e.g., `[99, 97, 102, 233]`). The `tf.strings` package (with an `s`) contains ops for byte strings & Unicode strings (& to convert one into other). It's important to note that a `tf.string` is atomic, meaning that its length does not appear in the tensor's shape. once you convert it to a Unicode tensor (i.e., a tensor of type `tf.int32` holding Unicode code points), the length appears in the shape.\n",
    "* *Sets*\n",
    "   - Are represented as regular tensors (or sparse tensors). For example, `tf.constant([[1, 2], [3, 4]])` represents the two sets {1, 2} & {3, 4}. More generally, each set is represented by a vector in the tensor's last axis. You can manipulate sets using operations for the `tf.sets` pacakge.\n",
    "* *Queues*\n",
    "   - Store tensors across multiple steps. TensorFlow offers various kinds of queues: simple First IN, First Out (FIFO) queues (FIFOQueue), queues that can prioritise some items (`PriorityQueue`), shuffle their items (`RandomShuffleQueue`), & batch items of different shapes by padding (`PaddingFIFOQueue`). These classes are all in the `tf.queue` package.\n",
    "   \n",
    "With tensors, operations, variables, & various data structures at your disposal, you are now ready to customise your models & training algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcee148-5de9-4c73-9349-254f4ab5b621",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c52f37-b0a1-46c4-bb09-b3308006d91d",
   "metadata": {},
   "source": [
    "# Customising Models & Training Algorithms\n",
    "\n",
    "Let's start by creating a custom loss function, which is a simple & common use case.\n",
    "\n",
    "## Custom Loss Functions\n",
    "\n",
    "Suppose you want to train a regression model, but your training set is a bit noisy. Of course, you start by trying to clean up your dataset by removing or fixing the outliers, but that turns out to be insufficient; the dataset is still noisy. Which loss function should you use? The mean squared error might penalise large errors too much & cause your model to be imprecise. The mean absolute error would not penalise outliers as much, but training might take a while to converge, & the trained model might not be very precise. This is probably a good time to use the Huber loss instead of the good old MSE. The Huber loss is not currently part of the official Keras API, but it is available in tf.keras (just use an instance of the `keras.losses.Huber` class). But let's pretend it's not there: implementing it is easy. Just create a function that takes the labels & predictions as arguments, & use TensorFlow operations to compute every instance's loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0545b12-e572-41fc-838f-d824cdb3c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146efff-6c85-4bf6-a9d5-8d54d004d35b",
   "metadata": {},
   "source": [
    "It is also preferable to return a tensor containing one loss per instance, rather than returning the mean loss. This way, Keras can apply class weights or sample weights when requested.\n",
    "\n",
    "Now, you can use this loss when you compile & train a Keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70dc7a5c-f91a-4f7b-9ae6-7b981ee98684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 53.0092 - mae: 53.4960 - val_loss: 5.5321 - val_mae: 6.0300\n",
      "Epoch 2/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.2891 - mae: 3.7648 - val_loss: 2.2028 - val_mae: 2.6741\n",
      "Epoch 3/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.8337 - mae: 3.3053 - val_loss: 2.7855 - val_mae: 3.2654\n",
      "Epoch 4/15\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.6389 - mae: 3.1121 - val_loss: 4.0657 - val_mae: 4.5613\n",
      "Epoch 5/15\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.3768 - mae: 2.8448 - val_loss: 3.1878 - val_mae: 3.6684\n",
      "Epoch 6/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.2493 - mae: 2.7162 - val_loss: 2.4290 - val_mae: 2.9135\n",
      "Epoch 7/15\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.1247 - mae: 2.5884 - val_loss: 0.7758 - val_mae: 1.1847\n",
      "Epoch 8/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.0167 - mae: 2.4808 - val_loss: 3.2452 - val_mae: 3.7242\n",
      "Epoch 9/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.9180 - mae: 2.3814 - val_loss: 0.3834 - val_mae: 0.7202\n",
      "Epoch 10/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8433 - mae: 2.3039 - val_loss: 0.4271 - val_mae: 0.7789\n",
      "Epoch 11/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.7230 - mae: 2.1776 - val_loss: 3.1662 - val_mae: 3.6528\n",
      "Epoch 12/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.6700 - mae: 2.1252 - val_loss: 2.5524 - val_mae: 3.0384\n",
      "Epoch 13/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.5921 - mae: 2.0477 - val_loss: 1.5852 - val_mae: 2.0574\n",
      "Epoch 14/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.5267 - mae: 1.9763 - val_loss: 1.3432 - val_mae: 1.7983\n",
      "Epoch 15/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.4550 - mae: 1.9020 - val_loss: 1.0826 - val_mae: 1.5446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb415005910>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"my_model_with_a_custom_loss.h5\", save_best_only = True)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"selu\", kernel_initializer = \"lecun_normal\",\n",
    "                       input_shape = input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = huber_fn, \n",
    "              optimizer = \"nadam\",\n",
    "              metrics = [\"mae\"])\n",
    "model.fit(X_train, y_train, epochs = 15,\n",
    "          validation_data = (X_val, y_val),\n",
    "          callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166d824-8d63-4a69-8408-77378614320d",
   "metadata": {},
   "source": [
    "That's it. For each batch during training, Keras will call the `huber_fn()` function to compute the loss & use it to perform a Gradient Descent step. Moreover, it will keep track of the total loss since the beginning of the epoch, & it will display the mean loss.\n",
    "\n",
    "But what happens to this custom loss when you save the model?\n",
    "\n",
    "## Saving & Loading Models That Contain Custom Components\n",
    "\n",
    "Saving a model containing a custom loss function works fine, as Keras saves the name of the function. When you load it, you'll need to provide a dictionary that maps the function name to the actual function. More generally, when you load a model containing custom objects, you need to map the names to the objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbc422bd-e6f9-4a39-ab50-314ab7d6f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "                                custom_objects = {\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad3203-a041-4064-ba16-99a5367c1642",
   "metadata": {},
   "source": [
    "With the current implementation, any error between -1 & 1 is considered \"small\". But what if you want a different threshold? One solution is to create a function that creates a configured loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59902913-d5e8-4c62-a5ac-d186077f8372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 4.7136 - val_loss: 2.3457\n",
      "Epoch 2/15\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 4.3291 - val_loss: 5.5327\n",
      "Epoch 3/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 4.0434 - val_loss: 1.3664\n",
      "Epoch 4/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.8441 - val_loss: 2.5445\n",
      "Epoch 5/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.5427 - val_loss: 6.9075\n",
      "Epoch 6/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.3125 - val_loss: 0.4325\n",
      "Epoch 7/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.1738 - val_loss: 3.3465\n",
      "Epoch 8/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.0466 - val_loss: 2.0669\n",
      "Epoch 9/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.8812 - val_loss: 1.6257\n",
      "Epoch 10/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.7073 - val_loss: 2.0173\n",
      "Epoch 11/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.4343 - val_loss: 1.9813\n",
      "Epoch 12/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.3655 - val_loss: 2.9760\n",
      "Epoch 13/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.1714 - val_loss: 1.3512\n",
      "Epoch 14/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.0981 - val_loss: 3.0107\n",
      "Epoch 15/15\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.0077 - val_loss: 1.4158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4150dcd60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_huber(threshold = 1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"my_model_with_a_custom_loss_threshold2.h5\", save_best_only = True)\n",
    "\n",
    "model.compile(loss = create_huber(2.0), optimizer = \"nadam\")\n",
    "model.fit(X_train, y_train, epochs = 15,\n",
    "          validation_data = (X_val, y_val),\n",
    "          callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a806ecd-d614-491c-a672-7889f08e2949",
   "metadata": {},
   "source": [
    "Unfortunately, when you save the model, the `threshold` will not be saved. This means that you will have to specify the `threshold` value when loading the model (note that the name to use is `\"huber_fn\"`, which is the name of the function you gave Keras, not the name of the function that created it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef604979-ce03-4889-92d6-392c5f30c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold2.h5\",\n",
    "                                custom_objects = {\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df763f43-c0ab-41dd-a17d-9c25cced9736",
   "metadata": {},
   "source": [
    "You can solve this by creating a subclass of the `keras.losses.Loss` class, & then implementing its `get_config()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "538fefe4-2d10-478c-b10c-ebe022f5109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold = 1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs) \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2 \n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c596d1-d0e4-45ec-800b-1633099866d4",
   "metadata": {},
   "source": [
    "Let's walk through this code:\n",
    "\n",
    "* The constructor accepts `**kwargs` & passes them to the parent constructor, which handles standard hyperparameters: the `name` of the loss & the `reduction` algorithm to use to aggregate the individual instance losses. By default, it is `\"sum_over_batch_size\"`, which means that the loss will be the sum of the instance losses, weighted by the sample weights, if any, & divided by the batch size (not by the sum of weights, so this is not the weighted mean). Other possible values are `\"sum\"` & `\"none\"`.\n",
    "* The `call()` method takes the labels & predictions, computes all the instance losses, & returns them.\n",
    "* The `get_config()` method returns a dictionary mapping each hyperparameter name to its value. It first calls the parent class's `get_config()` method, then addes the new hyperparameters to this dictionary.\n",
    "\n",
    "You can then use any instance of this class when you compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1df36755-cbf9-478f-9277-5490a7092a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 4.4732 - val_loss: 5.3828\n",
      "Epoch 2/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 4.2963 - val_loss: 3.3314\n",
      "Epoch 3/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 3.9448 - val_loss: 9.5919\n",
      "Epoch 4/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 3.6827 - val_loss: 6.4410\n",
      "Epoch 5/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.5201 - val_loss: 4.1632\n",
      "Epoch 6/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.3917 - val_loss: 4.7486\n",
      "Epoch 7/15\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.9586 - val_loss: 3.1032\n",
      "Epoch 8/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.9221 - val_loss: 4.9357\n",
      "Epoch 9/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.7319 - val_loss: 3.6812\n",
      "Epoch 10/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.5742 - val_loss: 1.4648\n",
      "Epoch 11/15\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.4733 - val_loss: 3.0138\n",
      "Epoch 12/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.3467 - val_loss: 3.7790\n",
      "Epoch 13/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.1121 - val_loss: 0.7416\n",
      "Epoch 14/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.0269 - val_loss: 0.4020\n",
      "Epoch 15/15\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.9660 - val_loss: 2.3477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3f67a29d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"my_model_with_a_custom_loss_class.h5\",\n",
    "                                                      save_best_only = True)\n",
    "\n",
    "model.compile(loss = HuberLoss(2.0), optimizer = \"nadam\")\n",
    "model.fit(X_train, y_train, epochs = 15,\n",
    "          validation_data = (X_val, y_val),\n",
    "          callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a816337-699e-40c5-9e95-c7121809ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
    "                                custom_objects = {\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c0067-b067-4cd5-bde3-0e65e2efec67",
   "metadata": {},
   "source": [
    "When you save a model, Keras calls the loss instance's `get_config()` method & saves the config as JSON in the HDF5 file. When you load the model, it calls the `from_config()` class method on the `HuberLoss` class: this method is implemented by the base class (`Loss`) & creates an instance of the class, passing `**config` to the constructor.\n",
    "\n",
    "That's it for losses. Pretty simple. Almost as simple as customer activation functions, initialisers, regularisers, & constraints.\n",
    "\n",
    "## Custom Activation Functions, Initialisers, Regularisers, & Constraints\n",
    "\n",
    "Most keras functionalities, such as losses, regularisers, constraints, initialisers, metrics, activation functions, layers, & even full models, can be customised in very much the same way. Most of the time, you will just need to write a simple function with the appropriate inputs & outputs. Here are examples of a custom activation function (equivalent to `keras.activations.softplus()` or `tf.nn.softplus()`), a custom Glorot initialiser (equivalent to `keras.initializers.glorot_normal()`), a custom $l_1$ regulariser (equivalent to `keras.regularizers.l1(0.01)`), & a custom constraint that ensures weights are all positive (equivalent to `keras.constraints.nonneg()` or `tf.nn.relu`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "768b62e4-064c-45bf-ace2-1918994016d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initialiser(shape, dtype = tf.float32):\n",
    "    stddev = tf.sqrt(2.0 / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev = stddev, dtype = dtype)\n",
    "\n",
    "def my_l1_regulariser(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0.0, tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1459d7-5885-40b5-95b7-93d0035ec000",
   "metadata": {},
   "source": [
    "As you can see, the argument depend on the type of custom function. These custom functions can then be used normally; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63a89a20-c025-4f37-9005-24f1c52d92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(30, activation = my_softplus, \n",
    "                           kernel_initializer = my_glorot_initialiser, \n",
    "                           kernel_regularizer = my_l1_regulariser,\n",
    "                           kernel_constraint = my_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c11c3-b5af-4f7c-aeac-de9bcfdef69d",
   "metadata": {},
   "source": [
    "The activation function will be applied to the output of this `Dense` layer, & its result will be passed on the next layer. The layer's weights will be initialised using the value returned by the initialiser. At each training step, the weights will be passed to the regularisation function to compute the regularisation loss, which will be added to the main loss to get the final loss used for training. Finally, the constraint function will be called after each training step, & the layer's weights will be replaced by the constrained weights.\n",
    "\n",
    "If a function has hyperparameters that need to be saved along with the model, then you will want to subclass the appropriate class, such as `keras.regularizers.Regularizer`, `keras.constraints.Constraints`, `keras.initializers.Initializer`, or `keras.layers.Layer` (for any layer, including activation functions). Much like we did for the custom loss, here is a simple class for $l_1$ regularisation that saves its `factor` hyperparameter (this time, we do not need to call the parent constructor or the `get_config()` method, as they are not defined by the parent class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43982e1c-a367-44a2-bc34-67e4e22bad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myL1Regulariser(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2ccc3-e291-47cc-8b1d-4e509bfc8c6d",
   "metadata": {},
   "source": [
    "Note that you must implement the `call` method for losses, layers (including activation functions), & models, or the `__call__` method for regularisers, initialisers, & constraints. For metrics, things are a bit different.\n",
    "\n",
    "## Custom Metrics\n",
    "\n",
    "Losses & metrics are conceptually not the same thing: losses (e.g., cross entropy) are used by gradient descent to *train* a model, so they must be differentiable (at least where they are evaluated), & there gradients should not be 0 everywhere. Plus it's okay if htey are not easily interpretable by humans. In contrast, metrics (e.g., accuracy) are used to *evaluate* a model: they must be more easily interpretable, & they can be non-differentiable or have 0 gradients everywhere.\n",
    "\n",
    "That said, in most cases, defining a custom metric function is exactly the same as defining a custom loss function. In fact, we could even use the Huber loss function we created earlier as a metric; it would work just fine (& persistence would also work the same way, in this case only saving the name of the function, `\"huber_fn\"`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c452f8a-a270-4da3-9dae-6d7cdb030608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"mse\", optimizer = \"nadam\", metrics = [create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6968a2-a9a7-40eb-a7ba-020a5bb19b77",
   "metadata": {},
   "source": [
    "\n",
    "For each batch during training, Keras will compute this metric & keeep track of its mean since the beginning of the epoch. Most of the time, this is exactly what you want. But not always! Consider a binary classifier's precision, for example. As we saw before, precision is the number of true positives divided the number of positive predictions (including both true positives & false positives). Suppose the model made 5 positive predictions in the first batch, 4 of which were correct: that's 80% precision. Then suppose the model made 3 positive predictions in the second batch, but they were all incorrect: that's 0% precision for the second batch. If you just compute the mean of these two precisions, you get 40%. But wait a second -- that's not the model's precision over these two batches! Indeed, there were a total of 4 true positives (4 + 0) out of eight positive predictions (5 + 3), so the overal prediction is 50%, not 40%. What we need is an object that can keep track of the number of true positives & the number of false positives & that can compute their ratio when requested. This is precisely what the `keras.metrics.Precision` class does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20e54da0-9ee3-4cf0-b7dc-00e3d1c11bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca6600c2-dfaa-463c-a771-27d45d9761b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83194187-4117-4576-9abe-9fa3d7e5d0c4",
   "metadata": {},
   "source": [
    "In this example, we created a `Precision` object, then we used it like a function, passing it the labels & predictions for the first batch, then the second batch (note that we could also have passed sample weights). We used the same number of true & false positives as in the example we just discussed. After the first batch, it returns a precision of 80%; then after the second batch, it returns 50% (which is the overall precision so far, not the second batch's precision). This is called a *streaming metric* (or *stateful metric*), as it is gradually updated, batch after batch.\n",
    "\n",
    "At any point, we can call the `result()` method to get the current value of the metric. We can also look at its variables (tracking the number of true & false positives) by using the `variables` attribute, & we can reset these variables using the `reset_states()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21a194e8-8d0c-4963-8067-a90153669ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a40d530-22a6-4a2e-905f-cfe34e68a173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b66cdb88-750f-4de0-abab-1a25a3f3051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6ead1-ec6b-402b-b4ae-d40bad68cddd",
   "metadata": {},
   "source": [
    "If you need to create such a streaming metric, create a subclass of the `keras.metrics.Metric` class. Here is a simple example that keeps track of the total Huber loss & the number of instances seen so far. When asked for the result, it returns the ratio, which is simply the mean Huber loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9d13892-dd49-4fc5-9c60-3c8c39a23aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init(self, threshold = 1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializers = \"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializers = \"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\":self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d870159-b995-490a-97d1-28d441e09e4c",
   "metadata": {},
   "source": [
    "Let's walk through this code:\n",
    "\n",
    "* The constructor uses the `add_weight()` method to create the variables needed to keep track of the metrics's state over multiple batches -- in this case, the sum of all Huber losses (`total`) & the number of instances seen so far (`count`). You could just create variables manually if you preferred. Keras tracks any `tf.Variable` that is set as an attribute (& more generally, any \"trackable\" object, such as layers or models).\n",
    "* The `update_state()` method is called when you use an instance of this class as a function (as we did with the `Precision` object). It updates the variables, given the labels & predictions for one batch (& sample weights, but in this case we ignore them).\n",
    "* The `result()` method computes & returns the final result, in this case the mean Huber metric over all instance. When you use the metric as a function, the `updated_state()` method gets called first, then the `result()` method is called, & its output is returned.\n",
    "* We also implement the `get_config()` method to ensure the `threshold` gets saved along with the model.\n",
    "* The default implementation of the `reset_states()` method resets all variables to 0.0 (but you can override it if needed).\n",
    "\n",
    "When you define a metric using a simple function, keras automatically calls it for each batch, & it keeps track of the mean during each epoch, just like we did manually. So the only benefit of our `HuberMetric` class is that the `threshold` will be saved. But of course, some metrics, like precision, cannot simply be averaged over batches: in those cases, there's no other option than to implement a streaming metric.  \n",
    "\n",
    "Now that we have built a streaming metrics, building a custom layer will seem like a walk in the park\n",
    "\n",
    "## Custom Layers\n",
    "\n",
    "You may occasionally want to build an architecture that contains an exotic layer for which TensorFlow does not provide a default implementation. In this case, you will need to create a custom layer. Or you may simply want to build a very repetitive architecture, containing identical blocks of layers repeated many times, & it would be convenient to treat each block of layers as a single layer. For example, if the model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to define a custom layer D containing layers A, B, C, so your model would simple be D, D, D. Let's see how to build customer layers.\n",
    "\n",
    "First, som layers have no weights, such as `keras.layers.Flatten` or `keras.layers.ReLU`. If you want to create a custom layer without any weights, the simplest option is to write a function & wrap it in a `keras.layers.Lambda` layer. For example, the following layer will apply the exponential function to its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45886bac-b520-4b5d-a2c9-09866cf9f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab0722-c8c4-44e5-9132-84b1fd147c46",
   "metadata": {},
   "source": [
    "This custom layer can then be used like any other layer, using the sequential API, the functional API, or the subclassing API. You can also use it as an activation function (or you could use `activation = tf.exp`, `activation = keras.activations.exponential`, or simply `activation = \"exponential\"`). The exponential layer is sometimes used in the output layer of a regression model when the values to predict have very different scales (e.g., 0.001, 10., 1,000.).\n",
    "\n",
    "As you've probably guessed by now, to build a custom stateful layer (i.e., a layer with weights), you need to create a subclass of the `keras.layers.Layer` class. For example, the following class implements a simplified version of the `Dense` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3ab3767-398b-4c59-a8cb-aa31f71b641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(name = \"kernel\", shape = [batch_input_shape[-1], self.units],\n",
    "                                      initializer = \"glorot_normal\")\n",
    "        self.bias = self.add_wieght(name = \"bias\", shape = [self.units], initializer = \"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9f858-48ed-4cfb-9721-d217cf33cdb6",
   "metadata": {},
   "source": [
    "Let's walk through this code:\n",
    "\n",
    "* The constructor takes all the hyperparameters as argument (in this example, `unitss` & `activation`), & importantly it also takes a `**kwargs` argument. It calls the parent constructor, passing it the `kwargs`: this takes care of standard arguments such as `input_shape`, `trainable`, & `name`. Then it saves the hyperparameters as attributes, converting the `activation` arguments to the appropriate activation function using the `keras.activations.get()` function (it accepts functions, standard strings like `\"relu\"` or `\"selu\"`, or simply `None`).\n",
    "* The `build()` method's role is to create the layer's variables by calling the `add_weight()` method for each weight. The `build()` method is called the first time the layer is used. At that point, Keras will know the shape of this layer's inputs, & it will pass it to the `build()` method, which is often necessary to create some of the weights. For example, we need to know the number of neurons in the previous layer in order to create the connection weights matrix (i.e., the `\"kernel\"`): this corresponds to the size of the last dimension of the inputs. At the end of the `build()` method (& only at the end), you must call the parent's `build()` method: this tells Keras that the layer is built (it just sets `self.built = True`).\n",
    "* The `call()` method performs the desired operations. In this case, we compute the matrix multiplication of the inputs `X` & the layer's kernel, we add the bias vector, & we apply the activation function to the result, & this gives us the output of the layer.\n",
    "* The `compute_output_shape()` method simply returns the shape of this layer's outputs. In this case, it is the same shpae as the inputs, except the last dimension is replaced with the number of neurons in the layer. Note that in tf.keras, shapes are instances of the `tf.TensorShape` class, which you can convert to Python lists using `as_list()`.\n",
    "* The `get_config()` method is just like in the previous custom classes. Note that we save the activation function's full configuration by calling `keras.activations.serialize().`\n",
    "\n",
    "You can now use a `myDense` layer just like any other layer.\n",
    "\n",
    "To create a layer with multiple inputs (e.g., `Concatenate`), the argument to the `call()` method should be a tuple containing all the input, & similarly the argument to the `compute_output_shape()` method should be a tuple containing each input's batch shape. To create a layer with multiple outputs, the `call()` method should return the list of outputs, & `compute_output_shape()` should return the list of batch output shapes (one per output). For example, the following toy layer takes two inputs & returns three outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77cfff7b-c7ba-4998-b19c-444760c42957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fbd52-77d6-4922-9385-e0e8dffd9aec",
   "metadata": {},
   "source": [
    "This layer may now be used like any other layer, but of course only using the functional & subclassing APIs, not the sequential API (which only accepts layers with one input & one output).\n",
    "\n",
    "If your layer needs to have a different behavior during training & during testing (e.g., if it uses `Dropout` or `BatchNormalization` layers), then you must add a `training` argument to the `call()` method & use this argument to decide what to do. For example, let's create a lyer that adds Gaussian noise during training (for regularisation) but does nothing during testing (Keras has a layer that does the same thing, `keras.layers.GaussianNoise`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7095db07-1d30-4cfa-a394-380878d12be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    def call(self, X, training = None):\n",
    "        if training: \n",
    "            noise = tf.random.normal(tf.shape(X), stddev = self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b08c87-046c-4fe0-a3f9-0525993126ab",
   "metadata": {},
   "source": [
    "With that, you can now build any custom layer you need. Now let's create custom models.\n",
    "\n",
    "## Custom Models\n",
    "\n",
    "We already looked at creating custom model classes before, when we discussed the subclassing API. It's straightforward: subclass the `keras.Model` class, create layers & variables in the constructor, & implement the `call(0` method to do whatever you want the model to do. Suppose you want to build the model represented below.\n",
    "\n",
    "<img src = \"Images/Custom Model Example.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "The inputs go through a first dense layer, then through a *residual block* composed of two dense layers & an additional operation, then through this same residual block three more times, then through a second residual black, & the final result goes through a dense output layer. Note that this model does not make much sense; it's just an example to illustrate the fact that you can easily build any kind of model you want, even on that contains loops & skip connections. To implement this model, it is best to first create a `ResidualBlock` layer, since we are going to create a couple of identical blocks (& we might want to reuse it in another model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c418969-704b-44b9-aeb4-f80ffd571e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class residualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation = \"elu\",\n",
    "                                          kernel_initializer = \"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757458f7-1971-4cf3-bc37-69275165d3d1",
   "metadata": {},
   "source": [
    "This layer is a bit special since it contains other layers. This is handled transparently by keras: it automatically detects that the `hidden` attribute contains trackable objects (layers in this case), so their variables are automatically added to this layer'slist of variables. The rest of this class is self-explanatory. Next, let's use the subclassing API to define the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7589a37-e70b-4711-a96e-fe58f709a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class residualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation = \"elu\",\n",
    "                                          kernel_initializer = \"he_normal\")\n",
    "        self.block1 = residualBlock(2, 30)\n",
    "        self.block2 = residualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range (1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a701b-7ae9-4cb6-8c9d-e2b2afa1633a",
   "metadata": {},
   "source": [
    "We create the layers in the constructor & use them in the `call()` method. This model can then be used like any other model (compile it, fit it, evaluate it, & use it to make predictions). If you also want to be able to save the model using the `save()` method & load it using the `keras.models_load_model()` function, you must implement the `get_config()` method in both the `residualBlock` class & the `residualRegressor` class. Alternatively, you can save & load the weights using the `save_weights()` & `load_weights()` methods.\n",
    "\n",
    "The `Model` class is a subclass of the `Layer` class, so models can be defined & used exactly like layers. But a model has some extra functionalities, including of course its `compile()`, `fit()`, `evaluate()`,  & `predict()` methods (& a few variants), plus the `get_layers()` method (which can return any of the model's layers by name or by index) & the `save()` method (& support for `keras.models.load_model()` & `keras.models.clone_model()`).\n",
    "\n",
    "With that, you can naturally & concisely build almost any model that you find in a paper, using the sequentail API, the functional API, the subclassing API, or even a mix of these. \"Almost\" any model? Yes, there are still a few things that we need to look at: first, how to define losses or metrics based on model internals, & second, how to build a custom training loop.\n",
    "\n",
    "## Losses & Metrics Based on Model Internals\n",
    "\n",
    "The custom losses & metrics we defined earlier were all based on labels & the predictions (& optionally sample weights). There will be times when you want to define losses based on other parts of your model, such as the weights or activations of its hidden layers. This may be useful for regularisation purposes or to monitor some internal aspect of your model.\n",
    "\n",
    "To define a fustom loss based on model internals, compute it based on any part of the model you want, then pass the result to the `add_loss()` method. For example, let's build a custom regression MLP model composed of a stack of five hidden layers plus an output layer. This custom model will also have an auxiliary output on top of the upper hidden layer. The loss associated to this auxiliary output will be called the *reconstruction loss*: it is the mean squared difference between the reconstruction & the inputs. By adding this reconstruction loss to the main loss, we will encourage the model to preserve as much information as possible through the hiddne layers -- even information that is not directly useful for the regression task itself. In practice, this loss sometimes improves genralisation (it is a regularisation loss). Here is the code for this custom model with a custom reconstruction loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94f97ae5-9fa8-4219-9cfa-46112390ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.Model): \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                      kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "def build(self, batch_input_shape):\n",
    "    n_inputs = batch_input_shape[-1] \n",
    "    self.reconstruct = keras.layers.Dense(n_inputs) \n",
    "    super().build(batch_input_shape)\n",
    "def call(self, inputs): \n",
    "    Z = inputs\n",
    "    for layer in self.hidden: \n",
    "        Z = layer(Z)\n",
    "    reconstruction = self.reconstruct(Z)\n",
    "    recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs)) \n",
    "    self.add_loss(0.05 * recon_loss)\n",
    "    return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fbcff4-51bb-4d50-b661-143c432db1d8",
   "metadata": {},
   "source": [
    "Let's go through the code:\n",
    "\n",
    "* The constructor creates the DNN with 5 dense hidden layers & one dense output layer.\n",
    "* The `build()` method creates an extra dense layer which will be used to reconstruct the inputs of the model. It must be created here because its number of units must be equal to the number of inputs, & this number is unknown before the `build()` method is called.\n",
    "* The `call()` method processes the inputs through all five hidden layers, then passes the result through the reconstruction layer, which produces the reconstruction.\n",
    "* Then the `call()` method computes the reconstruction loss (the mean squared difference between the reconstruction & the inputs), & adds it to the model's list of losses using the `add_loss()` method. Notice that we scale down the reconstruction loss by multiplying it by 0.05 (this is a hyperparameter you can tune). This ensures that the reconstruction loss does not dominate the main loss.\n",
    "* Finally, the `call()` method passes the output of the hidden layers to the output layer & returns its output.\n",
    "\n",
    "Similarly, you can add a custom metric based on model internals by computing it in anyway you want, as long as the result in the output of a metric object. For example, you can create a `keras.metrics.Mean` object in the constructor, then call it in the `call()` method, passing it the `recon_loss`, & finally add it to the model by calling the model's `add_metric()` method. This way, when you train the model, keras will display both the mean loss over each epoch (the loss is the sum of the main loss plus 0.05 times the reconstruction loss) & the mean reconstruction error over each epoch. Both will go down during training.\n",
    "\n",
    "In over 99% of cases, everything we have discussed will be sufficient to implement whatever model you want to build,e ven with complex architectures, losses, & metrics. However, in some rare cases, you may need to customise the training loop itself. Before we get there, we need to look at how to compute gradients automatically in TensorFlow.\n",
    "\n",
    "## Computing Gradients Using Autodiff\n",
    "\n",
    "To understand how to use autodiff to compute gradients automatically, let's consider a toy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23146828-b37b-4764-a3e2-643ef7fd5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc69474-bbb7-4d0e-82b5-dd8a7ca81164",
   "metadata": {},
   "source": [
    "If you know calculus, you can analytically find that the partial derivative of this function with regard to `w1` is `6 * w1 + 2 * w2`. You can also find that its partial derivative with regard to `w2` is `2 * w1`. For example, at the point `(w1, w2) = (5, 3)`, these partial derivatives equal to 36 & 10, respectively, so the gradient vector at this point is (36, 10). but if this were a neural network, the function would be much more complex, typically with tens of thousands of parameters, & finding the partial derivative analytically by hand would be an almost impossible task. One solution could be to compute an approximation of each partial derivative by measuring how much the function's output changes when you tweak the corresponding parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5aa1040-54bc-45e6-8cc1-b391b356db04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65f2a558-1c40-44d7-ba97-b3de014f352e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2f89a-8eb1-4201-ad95-507f6ee08a33",
   "metadata": {},
   "source": [
    "Looks about right. This works rather well & is easy to implement, but it is just an approximation, & importantly, you need to call `f()` at least once per parameter (not twice, since we could compute `f(w1, w2)` just once). Needing to call `f()` at least once makes this approach intractable for large neural networks. So instead, we should use autodiff. TensorFlow makes this pretty simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e44bcddd-f197-4f1e-a913-a49d4e702267",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.0), tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bf52b-16e5-4ad4-82f3-e747f8afeee4",
   "metadata": {},
   "source": [
    "We first define two variables `w1` & `w2`, then we create a `tf.GradientTape` context that will automatically record every operation that involves a variable, & finally we ask this tape to compute the gradients of the result `z` with regard to both variables `[w1, w2]`. Let's take a look at the gradeints that TensorFlow computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c548eb99-8cda-471c-875f-615ae9c7a7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646991d-6d20-4ac7-8b79-9bdd2f84dea9",
   "metadata": {},
   "source": [
    "Perfect! Not only is the result accurate (the precision is only limited by the floating-point errors), but the `gradient()` method only goes through the recorded computations once (in reverse order), no matter how many variables there are, so it is incredibly efficient. It's like magic.\n",
    "\n",
    "The tape is automatically erased immediately after you call its `gradient()` method, so you will get an exception if you try to call `gradient()` twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8348f26d-5bae-43c5-a28e-7e88052ed8b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_45894/432257738.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \"\"\"\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n\u001b[0m\u001b[1;32m   1041\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "gradients = tape.gradient(z, w1)\n",
    "gradients = tape.gradient(z, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea8523-49ee-4721-a77d-017caaaa200d",
   "metadata": {},
   "source": [
    "If you need to call `gradient()` more than once, you must make the tape persistent & delete it each time you are done with it to free resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79809763-d78f-4493-b234-0234059d600b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=36.0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6720415a-ee41-4cef-bfd0-5cd48b10f387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw2 = tape.gradient(z, w2)\n",
    "dz_dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76cb9536-dd99-4de5-a1a0-a3a38c51ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e88ef-5e64-4b27-8da4-dc3703a070a9",
   "metadata": {},
   "source": [
    "By default, the tape will only track operations involving variables, so if you try to compute the gradient of `z` with regard to anything other than a variable, the result will be `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc23760c-a37a-47da-8ce6-ce297058ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2 = tf.constant(5.0), tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "    \n",
    "gradients = tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7f4df-6ceb-4f27-b521-982c5bfb27ad",
   "metadata": {},
   "source": [
    "However, you can force the tape to watch any tensors you like, to record every operation that involves them. You can then compute gradients with regard to these tensors, as if they were variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d94f1ba-4605-46d8-a482-6e89dada1745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "    \n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a5367-a77b-4b2b-ad2b-cac39225250d",
   "metadata": {},
   "source": [
    "This can be useful in some cases, like if you want to implement a regularisation loss that penalises activations that vary a lot when the inputs vary little: the loss will be based on the gradient of the activations with regard to the inputs. Since the inputs are not variables, you need to tell the tape to watch them.\n",
    "\n",
    "Most of the time, a gradient tape is used to compute the gradients of a single value (usually the loss) with regard to a set of values (usually the model parameters). This is where reverse-mode autodiff shines, as it just needs to do one forward pass & one reverse pass to get all the gradients at once. If you try to compute the gradients of a vector, for example, a vector containing multiple losses, then TensorFlow will compute the gradients of the vector's sum. So if you ever need to get the individual gradients (e.g., the gradients of each loss with regard to the model parameters), you must call the tape's `jacobian()` method: it will perform reverse-mode autodiff once for each loss in the vector (all in parallel by default). It is even possible to compute second-order partial derivatives (the Hessians, i.e., the partial derivatives of partial derivatives), but this is rarely needed in practice.\n",
    "\n",
    "In some cases, you may want to stop the gradients from backpropagating through some part of your neural network. To do this, you must use the `tf.stop_gradient()` function. The unction returns its inputs during the forward pass (like `tf.identity()`), but it does not let gradients through during backpropagation (it acts like a constant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af91c047-a887-427c-9237-5c1cc13268d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient (2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae8db1-3e11-4614-8ba7-1348c501485c",
   "metadata": {},
   "source": [
    "Finally, you may occasionally run into some numerical issues when computing gradients. For example, if you compute the gradients of the `my_softplus()` function for large inputs, the result will be NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07a9cd20-a008-4e26-9f0c-c97fe8592edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([100.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "    \n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a92f14-2ca6-4b9b-a1e7-e34cb4c2c93c",
   "metadata": {},
   "source": [
    "This is because computing the gradients of this function using autodiff leads to some numerical difficulties: due to floating-point precision errors, autodiff ends up computing infinity divided by infinity (which returns NaN). Fortunately, we can analytically find that the derivative of the softplus function is just $\\frac{1}{1 + \\frac{1}{e^x}}$, which is numerically stable. Next, we can tell TensorFlow to use this stable function when computing the gradients of the `my_softplus()` function by decorating it with `@tf.custom_gradient` & making it return both its normal output & the function that computes the derivatives (not that it will receive as input the gradients that were backpropagated so far, down to the softplus function; & according to the chain rule, we should multiply them with this function's gradients):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74fefc37-b08f-4d70-a03d-5d67b8cff70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a723b1fa-1ed4-4ab6-a522-e1fb2b345be0",
   "metadata": {},
   "source": [
    "Now when we compute the gradients of the `my_better_softplus()`, function, we get the proper result, even for large input values (however, the main output still explodes because of the exponential; one workaround is to use `tf.where()` to return the inputs when they are large).\n",
    "\n",
    "Congratulations! You can now compute the gradients of any function (provided it is differentiable at the point where you compute it), even blocking backpropagation when needed, & write your own gradient functions! This is probably more flexibility than you will eer need, even if you build your own custom training loops, as we'll see now.\n",
    "\n",
    "## Custom Training Loops\n",
    "\n",
    "In some rare cases, the `fit()` method may not be flexible enough for what you need to do. For example, recall the wide & deep model in previous lessons that uses two different optimisers: one for the wide path & another for the deep path. Since the `fit()` method only uses one optimiser (the one that we specify when compuling the model), implementing this paper requires writing your own custom loop.\n",
    "\n",
    "You may also like to write custom training loops simply to feel more confident that they do precisely what you intend them to do (perhaps you are unsure about some details of the `fit()` method). It can sometimes feel safer to make everything explicity. However, remembers that writing a custom training loop will make your code longer, more error-prone, & harder to maintain.\n",
    "\n",
    "First, let's build a simple model. No need to compile it, since we will handle the training loop manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fdfad086-b02d-4d4e-a34d-45ad1f868070",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"elu\", kernel_initializer = \"he_normal\",\n",
    "                       kernel_regularizer = l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer = l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3439e-28d6-45fa-9395-1549e0a32c9c",
   "metadata": {},
   "source": [
    "Next, let's create a tiny function that will randomly sample a batch of instances from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb044b55-fdd2-4d3e-9abb-400072d19eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size = 32):\n",
    "    idx = np.random.randint(len(X), size = batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e27a3-b859-4c9c-9c61-78a7d255d947",
   "metadata": {},
   "source": [
    "Let's also define a function that will display the training status, including the number of steps, the total number of steps, the mean loss since the start of the epoch (i.e., we will use the `Mean` metric to compute it), & other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72f1ecaa-9d6c-4b40-848b-d08f050ec924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics = None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                          for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end = end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282952d9-7f1e-4919-92a7-10f80d62b1cf",
   "metadata": {},
   "source": [
    "This code is self-explanatory, unless you are unfamiliar with Python string formatting: `{:.4f}` will format a float with four digits after the decimal point, & using `\\r` (carriage return) along with `end = \"\"` ensures that the status bar always gets printed on the same line. In the notebook, the `print_status_bar()` function includes a progress bar, but you could use the handy `tqdm` library instead.\n",
    "\n",
    "With that, let's get down to business! First, we need to define more hyperparameters & choose the optimiser, the loss function, & the metrics (just the MAE in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d7c51e9-8734-4739-a77f-0da322a3fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimiser = keras.optimizers.Nadam(lr = 0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bcb50f-1bf7-476c-a313-c263c832fcf1",
   "metadata": {},
   "source": [
    "& now we are ready to build the custom loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7752772-3692-4997-b46c-359c8a5a180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "11584/11610 - mean: 0.5317 - mean_absolute_error: 0.4983\n",
      "Epoch 2/5\n",
      "11584/11610 - mean: 0.6584 - mean_absolute_error: 0.4543\n",
      "Epoch 3/5\n",
      "11584/11610 - mean: 0.5356 - mean_absolute_error: 0.5061\n",
      "Epoch 4/5\n",
      "11584/11610 - mean: 0.4665 - mean_absolute_error: 0.4364\n",
      "Epoch 5/5\n",
      "11584/11610 - mean: 0.6518 - mean_absolute_error: 0.5195"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training = True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4cd72-20b2-479a-9243-8cf96ed38dc9",
   "metadata": {},
   "source": [
    "There's a lot going on in this code, so let's walk through it:\n",
    "\n",
    "* We create two nested loops: one for the epochs, the other for the batches within an epoch.\n",
    "* Then we sample a random batch from the training set.\n",
    "* Inside the `tf.GradientTape()` block, we make a prediction for one batch (using the model as a function), & we compute the loss: it is equal to the main loss plus the other losses (in this model, there is one regularisation loss per layer). Since the `mean_squared_error()` function returns one loss per instance, we compute the mean over the batch using `tf.reduce_mean()` (if you wanted to apply different weights to each instance, this is where you would do it). The regularisation losses are already reduced to a single scalar each, so if we just need to sum them (using `tf.add_n()`, which sums multiple tensors of the same shape & data type).\n",
    "* Next, we ask the tape to compute the gradient of the loss with regard to each trainable variable (*not* all variables!), & we apply them to the optimiser to perform a gradient descent step.\n",
    "* Then we update the mean loss & the metrics (over the current epoch), & we display the status bar.\n",
    "* At the end of each epoch, we display the status bar again to make it look complete & to print a line feed, & we reset the states of the mean loss & the metrics.\n",
    "\n",
    "If you set the optimiser's `clipnorm` or `clipvalue` hyperparameter, it will take care of this for you. If you want to apply any other transformation to the gradients, simply do so before calling the `apply_gradients()` method.\n",
    "\n",
    "If you add weight constraints to your model (e.g., by setting `kernel_constraint` or `bias_constraint` when creating a layer), you should update the training loop to apply these constraints just after `apply_gradients()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "217831dc-e97a-4bb8-9e75-eb665c408071",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in model.variables:\n",
    "    if variable.constraint is not None:\n",
    "        variable.assign(variable.constraint(variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac717f-e7fa-4ed9-8b61-02e3320d9860",
   "metadata": {},
   "source": [
    "Most importantly, this training loop does not handle layers that behave differently during training & testing (e.g., `BatchNormalisation` or `Dropout`). To handle these, you need to call the model with `training = True` & make sure it propagates this to every layer that needs it.\n",
    "\n",
    "As you can see, there are quite a lot of things you need to get write, & it's easy to make a mistake. But on the bright side, you can get full control, so it's your call.\n",
    "\n",
    "Now that you know how to customise any poart of your models & training algorithms, let's see how you can use TensorFlow's automatic graph generation feature: it can speed up your custom code considerably, & it will also make it portable to any platform supported by TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac6d39-c825-4010-b15e-35b11db28ef3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b1a69-eea9-47b8-99b2-c0d2cc746c7d",
   "metadata": {},
   "source": [
    "# TensorFlow Functions & Graphs\n",
    "\n",
    "In TensorFlow 2, graphs are simple to use. To show just how simple,let's start with a trivial function that computes the cube of its input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "99a6d7eb-2ea2-4647-a1ac-693ad90fbc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa974f72-6535-4977-b5c1-215158ce1595",
   "metadata": {},
   "source": [
    "We can obviously call this function with a python value, such as an int or a float, or we can call it with a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "953ef1f3-e4b5-4878-b69a-1946651a12e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c72bba0f-9f35-4e64-a8b5-f4ea18629616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b120e78-d8c2-4afe-85eb-a4131db1a129",
   "metadata": {},
   "source": [
    "Now, let's use `tf.function()` to convert this python function to a *TensorFlow Function*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc6cf5f6-aecf-4b87-8032-c2d361e77bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x7fb3f81d75e0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a3b6e-f012-46c0-8303-7d2beb820749",
   "metadata": {},
   "source": [
    "This TF Function can then be used exactly like the original python function, & it will return the same result (but as tensors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e53862d8-e66b-4498-8e6a-f52a8cba9ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ae8b6bb-c13a-40a1-84b9-92191dde83f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149ccac-46c0-441f-b326-ad7f5d728269",
   "metadata": {},
   "source": [
    "Under the hood, `tf.function()` analysed the computations performed by the `cube()` function & generated an equivalent computation graph! As you can see, it was rather painless (we'll see how this works shortly. Alternatively, we could have used `tf.function` as a decorator; this is actually more common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "84cfc8b0-e52c-414a-9181-b2e807d1df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288baa4-a9d2-4f76-b36f-ce1dc0387eca",
   "metadata": {},
   "source": [
    "The original python function is still available via the tf function's `python_function` attribute, in case you ever need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "763cc3ad-e7dd-4a90-bfbb-55db754071a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbe48a-b065-4735-95de-b91c08e57ad3",
   "metadata": {},
   "source": [
    "TensorFlow optimises the computation graph, pruning unused nodes, simplifying expressions (e.g., 1 + 2 would get replaced with 3), & more. Once the optimised graph is ready, the tf function efficiently executes the operations in the graph, in the appropriate order (& in parallel when it can). As a result, a tf function will usually run much faster than the original python function, especially if it performs complex computations. Most of the time, you will not really need to know more than that: when you want to boost a python function, just transform it into a tf function. That's all.\n",
    "\n",
    "Moreover, when you write a custom loss function, a custom metrics, a custom layer, or any other custom function & you use it in a keras model (as we did throughout this lesson), keras automatically converts your function into a tf function -- no need to use `tf.function()`. So most of the time, all this magic is 100% transparent.\n",
    "\n",
    "By default, a tf function generates a new graph for every unique set of input shapes & data types & caches it for subsequent calls. For example, if you call `tf_cube(tf.constant(10))`, a graph will be generated for int32 tensors of shape []. Then, if you call `tf_cube(tf.constant(20))`, the same graph will be reused. But if you then call `tf_cube(tf.constant([10, 20]))`, a new graph will be geenrated for int32 tensors of shape [2]. This is how tf functions handle polymorphism (i.e., varying argument types & shapes). However, this is only true for tensor arguments: if you pass numerical python values to a tf function, a new graph will be generated for every distinct value: for example, calling `tf_cube(10)` & `tf_cube(20)` will generate two graphs\n",
    "\n",
    "## AutoGraph & Tracing\n",
    "\n",
    "So how does TensorFlow generate graphs? It starts by analysing the python function's source code to capture all the control flow statements, such as `for` loops, `while` loops, & `if` statements, as well as `break`, `continue`, & `return` statements. This first step is called *autograph*. The reason TensorFlow has to analyse the source code is that python does not provide any other way to capture control flow statements: it offers magic methods like `__add__()` & `__mul__()` to capture operators like `+` & `*`, but there are no `__while__()` or `__if__()` magic methods. After analysing the function's code, autograph outputs an upgraded version of that function in which all the control flow statements are replaced by the appropriate TensorFlow operations, such as `tf.while_loop()` for loops & `tf.cond()` for `if` statements. For example, in the below figure, autograph analyses the source code of the `sum_squares()` python function, & it generates the `tf_sum_squares()` function. In this function, the `for` loop is replaced by the definition of the `loop_body()` function (containing the body of teh originnal `for` loop), followed by a call to the `for_stmt()` function. This call will build the appropriate `tf.while_loop()` operation in the computation graph.\n",
    "\n",
    "<img src = \"Images/Autograph.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Next, TensorFlow calls this \"upgraded\" function, but instead of passing the argument, it passes a *symbolic tensor* -- a tensor without any actual value, only a name, a data type, & a shape. For example, if you call `sum_square(tf.constant(10))`, then the `tf__sum_squares()` function will be called with a symbolic tensor of type int32 & shape []. The function will run in *graph mode*, meaning that each TensorFlow operation will add a node in the graph to represent itself & its output tensor(s) (as opposed to the regular mode, called *eager execution* or *eager mode*). In graph mode, tf operations do not perform any computations. In the above figure, you can see the `tf__sum_squares()` function being called with a symbolic tensor as its argument (in this case, an int32 tensor of shape []) & the final graph being generated during tracing. The nodes represent operations, & the arrows represent tensors (both the generated function & the graph are simplified).\n",
    "\n",
    "## TF Function Rules\n",
    "\n",
    "Most of the time, converting a python function that performs TensorFlow operations into a tf function is trivial: decorate it with `@tf.function` or let keras take care of it for you. However, there are a few rules to respect:\n",
    "\n",
    "* If you call any external library, including NumPy or even the standard library, this call will run only during tracing; it will not be part of the graph. Indeed a TensorFlow graph can only include TensorFlow constructs (tensors, operations, variables, datasets, & so on). So, make sure you use `tf.reduce_sum()` instead of `np.sum()`, `tf.sort()` instead of the built-in `sorted()` function, & so on (unless you really want the code to run only during tracing). This has a few additional implications:\n",
    "   - If you define a tf function `f(x)` that just returns `np.random.rand()`, a random number will be generated when the function is traced, so `f(tf.constant(2.0))` & `f(tf.constant(3.0))` will return the same random number, but `f(tf.constant([2.0, 3.0])` will return a different one. If you replace `np.random.rand()` with `tf.random.uniform([])`, then a new random number will be generated upon every call, since the operation will be part of the graph.\n",
    "   - If your non-TensorFlow code has side effects (such as logging something or updating a python counter), then you should not expect those side effects to occur every time you call the tf function, as they will only occur when the function is traced.\n",
    "   - You can wrap arbitrary python code in a `tf.py_function()` operation, but doing so will hinder performance, as TensorFlow will not be able to do any graph optimisation on this code. It will also reduce portability, as the graph will only run on platforms where python is available (& where the right libraries are installed).\n",
    "* You can call other python functions or tf functions, but they should follow the same rules, as tensorflow will capture their operations in the computation graph. Note that these other functions do not need to be decorated with `@tf.function`\n",
    "* If the function creates a TensorFlow variable (or any other stateful tensorflow object, such as a dataset or a queue), it must do so upon the very first call, & only then, or else you will get an exception. It is usually preferable to create variables outside of the tf function (e.g., in the `build()` method of a custom layer). If you want to assign a new value to the variable, make sure you call its `assign()` method, instead of using the `=` operator.\n",
    "* The source code of your python function should be available to tensorflow. If the source code is unavailable (for example, if you define your function in the python shell, which does not give access to the source code, or if you deploy only the compiled *.*pyc* python files to production), when the graph generation process will fail or have limited functionality.\n",
    "* TensorFlow will only capture `for` loops that iterate over a tensor or a dataset. So make sure you use `for i in tf.range(x)` rather than `for i in range(x)` or else the loop will not be captured in the graph. Instead, it will run during tracing. (This may be what you want if the `for` loop is meant to build the graph, for example to create each layer in a neural network).\n",
    "* As always, for performance reasons, you should prefer a vectorised implementation whenever you can, rather than using loops.\n",
    "\n",
    "It's time to sum up. In this lesson, we started with a brief overview of TensorFlow, then we looked at TensorFlow's low-level API, including tensors, operations, variables, & special data structures. We then used these tools to customise almost every component of tf.keras. Finally we looked at how tf functions can boost performance, how graphs are generated using autograph & tracing, & what rules to follow when writing tf functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
