{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adebdb4-0bcd-4478-8c33-7efd75b1a488",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Why would you want to use the Data API?\n",
    "2. What are the benefits of splitting a large dataset into multiple files?\n",
    "3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "4. Can you save any binary data to a tfrecord file, or only serialised protocol buffers?\n",
    "5. Why would you go through the hassle of converting all your data to the `Example` protobuf format? Why not use your own protobuf definition?\n",
    "6. When using tf records, when would you want to activate compression? Why not do it systematically?\n",
    "7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using tf transform. Can you list a few pros & cons of each option?\n",
    "8. Name a few common techniques you can use to encode categorical features. What about text?\n",
    "9. Load the fashion MNIST dataset; split it into a training set, validation set, & a test set; shuffle the training set; & save each dataset to multiple tfrecord files. Each record should be a serialised `Example` protobuf with two features: the serialised image (use `tf.io.serialize_tensor()` to serialise each image), & the label. Then use tf.data to create an efficient dataset for each set. Finally, use a keras model to train these datasets, including a preprocessing layer to standardise each input feature. Try to make the input pipeline as efficient as possible, using tensorboard to visualise profiling data.\n",
    "10. In this exercise, you will download a dataset, split it, create a `tf.data.Dataset` to load it & preprocess it efficiently, then build & train a binaary classification model containing an `Embedding` layer.\n",
    "   - Download the Large Movie Review dataset, which contains 50,000 movie reviews from the internet movie database. The data is organised in two directories *train* & *test*, each containing a *pos* subdirectory with 12,500 positive reviews & a *neg* subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files & folders (including pre-processed bag-of-words), but we will ignore them in this exercise.\n",
    "   - Split the test set into a validation set (15,000) & a test set (15,000).\n",
    "   - Use tf.data to create an efficient dataset for each set.\n",
    "   - Create a binary classification model, using `TextVectorization` layer to preprocess each review.\n",
    "   - Add an `Embedding` layer & compute the mean embedding for each review, multiplied by the square root of the number of words. This rescaled mean embedding can then be passed to the rest of your model.\n",
    "   - Train the model & see what accuracy you get. Try to optimise your pipelines to make training as fast as possible.\n",
    "   - Use TFDS to load the same dataset more easily: `tfds.load(\"imdb_reviews\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306afc8-2eea-43cb-8410-f9139817831b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7fe51-ba9d-4d07-b8cf-f40c9412d17c",
   "metadata": {},
   "source": [
    "1. The data API makes loading & preprocessing data with TensorFlow easier. It can ingest large datasets & preprocess them simply: you just create a dataset object & specify where to get the data & how to transfrom it. It can also read from a variety of file types & supports reading from SQL databases.\n",
    "2. Gradient descent works best when the instances of a dataset are independent & identically distributed. This means that we need to shuffle the instances of a dataset. For large datasets that cannot fit into memory, one possible solution is to split the dataset into multiple files. You can then pick multiple files randomly & read them simultaneously, interleaving their records. On top of that, you could also add a shuffling buffer, which will improve shuffling a lot more.\n",
    "3. You want to make sure your GPU is close to 100% utilised. To do this, you employ prefetching & multithreading. Prefetching means that while our training algorithm is working on one batch, the dataset will be working in parallel on getting the next batch ready (load & preprocess). If you also ensure that loading & preprocessing is multi-threaded (set `num_parallel_calls` when calling `interleave()` & `map()`), then you can exploit multiple cores on the CPU so that the time for loading & preprocessing is less than the time for training on the GPU.\n",
    "4. You can save binary data to both a tfrecord file & serialised protocol buffers. To serialise your protocol buffer, use the `SerializetToString()` method.\n",
    "5. We go through the hassle of converting all our data to the `Example` protobuf format instead of using our protobuf definition because the custom protobuf methods are not TensorFlow operations, which mean that they cannot be included in a TensorFlow function (except for wrapping them in a `tf.py_function()` operation, which slows down training & makes the code less portable.\n",
    "6. You want to compress your tfrecord files if they need to be loaded via network connection. This means that if you are going to download the compressed tf record files, then you will want to activate compression. Otherwise, if the files are on the same machine, then it's preferable to leave compression off.\n",
    "7. *(1)* If you preprocess the data when creating the data files, then training will run faster, since it will not have to perform preprocessing. *(2)* If the data is preprocessed with a tf.data pipeline, it can be very efficient (with multithreading & prefetching). However, preprocessing data this way slows down training, because it is done on the fly. This also means that each training instance will be preprocessed multiple times (once per epoch) rather than just once, if the data was preprocessing when creating the data files. *(3)* If you add preprocessing layers to your model, then it will also slow down training & each instance will be preprocessed multiple times (once per epoch). However, you can speed this up by multithreading & prefetching. *(4)* With tf transform, each instance is preprocessed just once, which speeds up training. It also automatically generates preprocessing layers, which is great for maintenance because your preprocessing is all in one place.\n",
    "8. For categorical features, you can use encode them as one-hot vectors. For large datasets, where learning the full vocabulary of the categorical features is a hassle, you can use out-of-vocabulary buckets. You can also use embeddings for categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eba353-93a7-418e-b332-f36f098390c6",
   "metadata": {},
   "source": [
    "# 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c437f12-4fb9-4750-9e53-ddb6fc7a7867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train, y_train = X_train[5000:], y_train[5000:]\n",
    "X_val, y_val = X_train[:5000], y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81322993-2e49-41e9-864a-e403b8717722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size = len(X_train))\n",
    "val_set = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942fe33-fc55-4403-9ce3-2703d78e257c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BytesList = tf.train.BytesList\n",
    "FloatList = tf.train.FloatList\n",
    "Int64List = tf.train.Int64List\n",
    "Feature = tf.train.Feature\n",
    "Features = tf.train.Features\n",
    "Example = tf.train.Example\n",
    "\n",
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    return Example(features = Features(feature = {\"image\": Feature(bytes_list = BytesList(value = [image_data.numpy()])),\n",
    "                                                  \"label\": Feature(int64_list = Int64List(value = [label]))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73158328-8d14-4875-a16a-d269ac203f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for image, label in val_set.take(1):\n",
    "    print(create_example(image, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9dcac-b4a4-4c39-9583-a5088e2564c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "def write_tfrecords(name, dataset, n_shards = 10):\n",
    "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
    "             for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
    "                   for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db363851-4f9e-436f-8d00-2a677a6f4748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\n",
    "val_filepaths = write_tfrecords(\"my_fashion_mnist.val\", val_set)\n",
    "test_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df5a91-ba23-42db-a993-bf353aec65b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type = tf.uint8)\n",
    "    #image = tf.io.decode_jpeg(example[\"image\"])\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]\n",
    "\n",
    "def mnist_dataset(filepaths, n_read_threads = 5, shuffle_buffer_size = None,\n",
    "                  n_parse_threads = 5, batch_size = 32, cache = True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths,\n",
    "                                      num_parallel_reads = n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ace7d6-3f5b-431d-a2de-3ef48bf18ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size = 60000)\n",
    "val_set = mnist_dataset(val_filepaths)\n",
    "test_set = mnist_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf6bea-3928-4448-a1cd-de099096f048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "\n",
    "standardization = Standardization(input_shape=[28, 28])\n",
    "\n",
    "sample_image_batches = train_set.take(100).map(lambda image, label: image)\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n",
    "                               axis = 0).astype(np.float32)\n",
    "standardization.adapt(sample_images)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    standardization,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "])\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = \"nadam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fb849-9349-40ec-9cd5-a8577b42a5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "logs = os.path.join(os.curdir, \"my_logs\",\n",
    "                    \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir = logs, histogram_freq = 1, profile_batch = 10)\n",
    "\n",
    "model.fit(train_set, epochs = 5, validation_data = val_set,\n",
    "          callbacks = [tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfff734-082c-49b4-8cff-d42680a6e7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"C:\\Users\\yuj22\\Desktop\\ML_Python\\Chapter 13\\my_logs\" --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d2008-7c1c-49a0-add0-6242625c682e",
   "metadata": {},
   "source": [
    "# 10a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88164d65-b59a-4dee-af61-f6aac354373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "\n",
    "download_root = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "filename = \"aclImdb_v1.tar.gz\"\n",
    "filepath = keras.utils.get_file(filename, download_root + filename, extract = True)\n",
    "path = Path(filepath).parent / \"aclImdb\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db41125-3ceb-4e3e-9f84-34e02301f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for name, subdirs, files in os.walk(path):\n",
    "    indent = len(Path(name).parts) - len(path.parts)\n",
    "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
    "    for index, filename in enumerate(sorted(files)):\n",
    "        if index == 3:\n",
    "            print(\"    \" * (indent + 1) + \"...\")\n",
    "            break\n",
    "        print(\"    \" * (indent + 1) + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cce96b-f6ff-4682-af4a-7b7a82bef124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
    "\n",
    "train_pos = review_paths(path / \"train\" / \"pos\")\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_val_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_val_neg = review_paths(path / \"test\" / \"neg\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_val_pos), len(test_val_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b8f4f-254b-4ff5-8d6e-2668740d4d47",
   "metadata": {},
   "source": [
    "# 10b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca3146-7f12-4aab-be1d-6365ed9f5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.shuffle(test_val_pos)\n",
    "\n",
    "test_pos = test_val_pos[:5000]\n",
    "test_neg = test_val_neg[:5000]\n",
    "val_pos = test_val_pos[5000:]\n",
    "val_neg = test_val_neg[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a11c20-6524-4375-b45f-efef644216ca",
   "metadata": {},
   "source": [
    "# 10c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0998b-209c-4fd9-a80c-e68ea0bc224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "        return tf.data.Dataset.from_tensor_slices((tf.constant(reviews), tf.constant(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf71b197-5ec0-487e-8b28-309bec018536",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
    "    print(X)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19187cc-32d5-42d6-913d-4ca51bbc08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads = 5):\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
    "                                          num_parallel_reads = n_read_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
    "                                          num_parallel_reads = n_read_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38216bc8-0777-41a7-a340-1a83d2ba3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
    "val_set = imdb_dataset(val_pos, val_neg).batch(batch_size).prefetch(1)\n",
    "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a0e40-f7a3-4204-88a9-56e48f95f872",
   "metadata": {},
   "source": [
    "# 10d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba8f08-29ea-4615-9702-6989e93489f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, n_words = 50):\n",
    "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
    "    Z = tf.strings.substr(X_batch, 0, 300)\n",
    "    Z = tf.strings.lower(Z)\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    Z = tf.strings.split(Z)\n",
    "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
    "\n",
    "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
    "preprocess(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3f180-1bc4-4574-a3d0-38a830e15103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocabulary(data_sample, max_size = 1000):\n",
    "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
    "    counter = Counter()\n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word != b\"<pad>\":\n",
    "                counter[word] += 1\n",
    "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n",
    "\n",
    "get_vocabulary(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310b84e-2be8-47d2-929a-0cd92a2c7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorisation(keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size = 1000, n_oov_buckets = 100, dtype = tf.string, **kwargs):\n",
    "        super().__init__(dtype = dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.n_oov_buckets = n_oov_buckets\n",
    "\n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
    "        words = tf.constant(self.vocab)\n",
    "        word_ids = tf.range(len(self.vocab), dtype = tf.int64)\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs = preprocess(inputs)\n",
    "        return self.table.lookup(preprocessed_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a7500-6106-44e0-a5df-6d6768acfe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorisation = TextVectorisation()\n",
    "text_vectorisation.adapt(X_example)\n",
    "text_vectorisation(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5ae84-49d9-421a-b9fd-c9cb7b6cd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocabulary_size = 1000\n",
    "n_oov_buckets = 100\n",
    "\n",
    "sample_review_batches = train_set.map(lambda review, label: review)\n",
    "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()), axis = 0)\n",
    "\n",
    "text_vectorisation = TextVectorisation(max_vocabulary_size, n_oov_buckets, input_shape = [])\n",
    "text_vectorisation.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6a6de-60cd-4335-97dc-fd485c55042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorisation(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95348047-4511-45d6-80bd-08110d9df271",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n",
    "tf.reduce_sum(tf.one_hot(simple_example, 4), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485dd70-6ce7-42f0-b6c0-4d87f0dfba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype = tf.int32, **kwargs):\n",
    "        super().__init__(dtype = dtype, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis = 1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a8e3a3-1a9c-4382-a8a1-6751a4021a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = BagOfWords(n_tokens = 4)\n",
    "bag_of_words(simple_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592ceb0-dc06-499d-8bea-11901549f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = max_vocabulary_size + n_oov_buckets + 1\n",
    "bag_of_words = BagOfWords(n_tokens)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    text_vectorisation,\n",
    "    bag_of_words,\n",
    "    keras.layers.Dense(1000, activation = \"relu\"),\n",
    "    keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"nadam\", metrics = [\"accuracy\"])\n",
    "model.fit(train_set, epochs = 5, validation_data = val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e12dae-aeaa-4512-83f0-2959fe9d42f0",
   "metadata": {},
   "source": [
    "# 10e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52731114-9a61-4576-9227-d779f0f147ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
