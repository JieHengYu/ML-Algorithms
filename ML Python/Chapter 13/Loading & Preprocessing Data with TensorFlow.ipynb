{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2006727e-b8ad-4311-bb35-4a6afbd5d56e",
   "metadata": {},
   "source": [
    "# Loading & Preprocessing Data with TensorFlow\n",
    "\n",
    "So far, we've used only datasets taht fint in memory, but deep learning systems are often trained on very large dataset that will not fit in RAM. Ingesting a large dataset & preprocessing it efficiently can be tricky to implement with other deep learning libraries, but TensorFlow makes it easy thanks to the *Data API*: you just create a dataset object, & tell it where to get the data & how to transform it. TensorFlow takes care of all the implementation details, such as multithreading, queuing, batching, & prefetching. Moreover, the Data API works seamlessly with tf.keras.\n",
    "\n",
    "Off the shelf, the data API can read from text files (such as CSV files), binary files with fixed-size records, & binaary files that use TensorFlows's tfrecord format, which supports records of varying sizes. TFRecord is a felxible & efficient format usually containing protocol buffers (an open source binary format). The data API also supports for reading from SQL databases. Moreover, many open source extensions are available to read from all sorts of data sources, such as Google's BigQuery service.\n",
    "\n",
    "Reading huge datasets efficiently is not the only difficulty: the data also needs to be preprocessed, usually normalised. Moreover, it is not always composed strictly of convenient numerical fields: there may be text features, categorical features, & so on. These need to be encoded, for example using one-hot encoding, bag-of-words encoding, or *embeddings* (as we will see, an embedding is a trainable dense vector that represents a category or token). One option is to handle all this preprocessing is to write your own custom preprocessing layers. Another is to use the standard preprocessing layers provided by keras.\n",
    "\n",
    "In this lesson, we will learn about the data API, the tfrecord format, & how to create customer preprocessing layers & use the standard keras ones. We will also take a quick look at a few related projects from TensorFlow's ecosystem:\n",
    "\n",
    "* *TF Transform* (`tf.Transform`)\n",
    "   - Makes it possible to write a single preprocessing function that can be run in batch mode on your full training set, before training (to speed it up), & then exported to a tf function & incorporated into your trained model so that once it is deployed on production, it can take care of preprocessing new instances on the fly.\n",
    "* *TF Datasets* (TFDS)\n",
    "   - Provides a convenient function to download many common datasets of all kinds, including large ones like imagenet, as well as convenient dataset objects to manipulate them using the data API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a8621-b6ae-4c4f-90dc-2361627c08bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf862fa3-6572-4d0a-9fe5-b2f0d831f4ab",
   "metadata": {},
   "source": [
    "# The Data API\n",
    "\n",
    "The whole data API revolves around the concept of a *dataset*: as you might suspect, this represents a sequence of data items. Usually, you will use datasets that gradually read data from disk, but for simplicity let's create a dataset entirely in RAM using `tf.data.Dataset.from_tensor_slices()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bce10e9-72e4-4579-9713-3165636840d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 12:48:37.657953: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88c241-52da-484e-b152-239fa30fcfef",
   "metadata": {},
   "source": [
    "The `from_tensor_slices()` function takes a tensor & creates a `tf.data.Dataset` whose elements are all the slices of `X` (along the first dimension), so this dataset contains 10 items: tensors 0, 1, 2, ..., 9. In this case, we would have obtained the same dataset if we had used `tf.data.Dataset.range(10)`.\n",
    "\n",
    "You can simply iterate over a dataset's items like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4454df65-40a3-4b8b-98df-a0ae48a9c02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 12:48:47.810791: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891758f-0574-40aa-b31c-9455be6794ee",
   "metadata": {},
   "source": [
    "## Chaining Transformations\n",
    "\n",
    "Once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain transformations like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed28397-1eda-4e8e-a1f6-95853888cfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 12:48:50.314890: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77443fc-5886-4a80-923f-791d6b3d0b25",
   "metadata": {},
   "source": [
    "<img src = \"Images/Chaining Transformations.png\" width = \"600\" style = \"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3069e-3f2c-4d95-900b-56f83d86374d",
   "metadata": {},
   "source": [
    "In this example, we first call the `repeat()` method on the original dataset, & it returns a new dataset that will repeat the items of the original dataset three times. Of course, this will not copy all the data in memory three times! (If you call this method with no arguments, the new dataset will repeat the source dataset forever, so the code that iterates over the dataset will have to decide when to stop). Then we call the `batch()` method on this new dataset, & again this creates a new dataset. This one will group the items of the previous dataset in batches of seven items. Finally, we iterate over the items of this final dataset. As you can see, the `batch()` method had to output a final batch of size two instead of seven, but you can call it with `drop_remainder = True` if you want it to drop this final batch so that all batches have the exact same size.\n",
    "\n",
    "You can also transform the items by calling the `map()` method. For example, this creates a new dataset with all items doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ffd5e0-e269-4fae-b12b-2102ac3ddaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec557773-badd-4ef5-818b-64e6abd5d880",
   "metadata": {},
   "source": [
    "This function is the one you will call to apply any preprocessing you want to your data. Sometimes this will include computations that can be quite intensive, such as reshaping or rotating an image, so you will usually want to spawn multiple threads to speed things up: it's as simple as setting the `num_parallel_calls` argument. Note that the function you pass to the `map()` method must be convertible to a tf function.\n",
    "\n",
    "While the `map()` method applies a transformation to each item, the `apply()` method applies a transformation to the dataset as a whole. For example, the following code applies the `unbatch()` function to the dataset (this function is currently experimental, but it will most likely move to the core API in ta future release). Each item in the new dataset will be a single-integer tensor instead of a batch of seven integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6273d8aa-ddc9-496f-9916-d42cb79bbd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_62593/643490874.py:1: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.apply(tf.data.experimental.unbatch())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b782d-5b49-4459-bb3a-3c935d7f9d16",
   "metadata": {},
   "source": [
    "It's is also possible to simply filter the dataset using the `filter()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0d848aa-2f3c-4333-bfd4-06dfd4280bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c0417-d272-477e-95b7-d047c9d61d0c",
   "metadata": {},
   "source": [
    "You will often want to look at just a few items from a dataset. You can use the `take()` method for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1499bea1-21b6-4a40-bf91-85024178de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 12:48:59.581319: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10132de-238a-408c-9fe8-350f1d90755f",
   "metadata": {},
   "source": [
    "## Shuffling the Data\n",
    "\n",
    "As you know, gradient descent works best when the instances in the training set are independent & identically distributed. A simple way to ensure this is to shuffle the instances, using the `shuffle()` method. It will create a new dataset htat will start by filling up a buffer with the first items of the source dataset. Then, whenever it is asked for an item, it will pull one out randomly from the buffer & replace it with a fresh one from the source dataset, until it has iterated entirely throughout the source dataset. At this point, it continues to pull out items randomly from the buffer until it is empty. You must specify buffer size, & it is important to make it large enough, or else shuffling will not be very effective. Just don't exceed the amount of RAM you have, & even if you have plenty of it, there's no need to go beyond the dataset's size. You can provide a random see if you want the same random order every time you run your program. For example, the following code creates & displays a dataset containing the integers 0 to 9, repeated 3 times, shuffled using a buffer size of 5 & a random seed of 42, & batched with a batch size = 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de72f19d-88cd-4d61-9151-82977c0bfc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 12:49:05.126336: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size = 5, seed = 42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e2ab4-efcb-454f-a962-064d45f2e115",
   "metadata": {},
   "source": [
    "For a large dataset that does not fit in memory, this simple shuffling-buffer approach may not be sufficient, since the buffer will be small compared to the dataset. One solution is to shuffle the source data itself. This will definitely improve shuffling a lot. Even if the source data is shuffled, you will usually want to shuffle it some more, or else the same order will be repeated at each epoch, & the modelmay end up being biased (e.g., due to some spurious patterns present by change in the source data's order). To shuffle the instances some more, a common approach is to split the source data into multiple files, then read them in a random order during training. However, instances located in the same file will still end up close to each other. To avoid this, you can pick multiple files randomly & read them simultaneously, interleaving their records. Then on top of that, you can add a shuffling buffer using the `shuffle()` method. If all this sounds like a lot of work, don't worry: the data API makes all this possible in just a few lines of code. Let's see how to do this.\n",
    "\n",
    "### Interleaving Lines from Multiple Files\n",
    "\n",
    "First, let's suppose that you've loaded the California housing dataset, shuffled it, & split it into a training set, validation set, & a test set. Then you split each set into many csv files that each look like this (each row contains eigth input features plus the target median house value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6f82b13-b0fc-453c-809a-071ee4bd484a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'METRIC_MAPPING64' from 'sklearn.metrics._dist_metrics' (/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_dist_metrics.cpython-39-darwin.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_62593/3793231504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_california_housing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhousing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_california_housing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetaestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scorer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_MultimetricScorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFitFailedWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_dist_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madjusted_mutual_info_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madjusted_rand_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_supervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfowlkes_mallows_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_supervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msilhouette_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalinski_harabasz_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_distances_chunked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msp_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pairwise_distances_reduction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPairwiseDistancesArgKmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pairwise_fast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_chi2_kernel_fast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sparse_manhattan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_pairwise_distances_reduction/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m from ._dispatcher import (\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mArgKmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mArgKminClassMode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dist_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBOOL_METRICS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMETRIC_MAPPING64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from ._argkmin import (\n\u001b[1;32m     10\u001b[0m     \u001b[0mArgKmin32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'METRIC_MAPPING64' from 'sklearn.metrics._dist_metrics' (/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_dist_metrics.cpython-39-darwin.so)"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, \n",
    "                                                    housing.target.reshape(-1, 1))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66077670-0a95-4b62-9807-f9462211d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb338a-c0b2-4a55-a68c-875a3bd0a057",
   "metadata": {},
   "source": [
    "Let's also suppose `train_filepaths` contains the list of training file paths (& you also have `val_filepaths` & `test_filepaths`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1760d-308b-4109-b3c6-6d81ca1add2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "val_data = np.c_[X_val, y_val]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts = 20)\n",
    "val_filepaths = save_to_multiple_csv_files(val_data, \"valid\", header, n_parts = 10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb5ba9-ef4c-4ed6-ac2e-0e3a0e16557f",
   "metadata": {},
   "source": [
    "Alternatively, you could use file patterns; for example, `train_filepaths = \"datasets/housing/my_train_*.csv\"`. Now let's create a dataset containing only these file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6d9a5-d128-4024-aee2-d15d6d36c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe9ed6-700e-4b9d-b903-7eebef6305e8",
   "metadata": {},
   "source": [
    "By default, the `list_files()` function returns a dataset that shuffles the file paths. In general this is a good thing, but you can set `shuffle = False` if you do not want that for some reason.\n",
    "\n",
    "Next, you can call the `interleave()` method to read from five files at a time & interleave their lines (skipping the first line of each file, which is the header row, using the `skil()` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab323b-09c9-472a-8a6a-f34ca2467c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length = n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9ac18-3898-4a66-a061-108492dafa34",
   "metadata": {},
   "source": [
    "The `interleave()` method will create a dataset that will pull five file paths from the `filepath_dataset`, & for each one it will call the function you gave it (a lambda in this example) to create a new dataset (in this case a `TextLineDataset`). To be clear, at this stage there will be seven datasets in all: the filepath dataset, the interleave dataset, & the five `TextLineDatasets` created internally by the interleave dataset. When we iterate over the interleave dataset, it will cycle through those five `TextLineDatasets`, reading one line at a time from each until all datasets are out of items. Then it will get the next five file paths from the `filepath_dataset` & interleave them the same way, & so on until it runs out of file paths.\n",
    "\n",
    "By default, `interleave()` does not use parallelism; it just reads one line at a time from each file, sequentially. If you want it to actually read files in parallel, you can set the `num_parallel_cals` argument to the number of threads you want (note that the `map()` method also has this argument). You can even set it to `tf.data.experimental.AUTOTUNE` to make TensorFlow choose the right number of threads dynamically based on the available CPU. Let's look at what the dataset contains now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde2e03-fd53-4b7d-9372-4a6fc9e8a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0ecd5-ccdd-4bb1-aabb-3ea301d88218",
   "metadata": {},
   "source": [
    "These are the first rows (ignoring the header row) of five CSV files, chosen randomly. Looks good so far, but as you can see, these are just byte strings; we need to parse them & scale the data.\n",
    "\n",
    "## Preprocessing the Data\n",
    "\n",
    "Let's implement a small function that will perform this preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ebb7f-353a-4b8e-83d0-802bf5819dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.0] * n_inputs + [tf.contant([], dtype = tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults = defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f514a02-46a7-473e-97db-7aa068647cfa",
   "metadata": {},
   "source": [
    "Let's walk through this code:\n",
    "\n",
    "* First, the code assumes that we have precomputed the mean & standard deviation of each feature in the training set. `X_mean` & `X_std` are just 1D tensors (or numpy arrays) containing eight floats, one per input feature.\n",
    "* The `preprocess()` function takes one CSV line & starts by parsing it. For this it uses the `tf.io.decode_csv()` function, which takes two arguments: the first is the line to parse, & the second is an array containing the default value for each column in the CSV file. This array tells TensorFlow not only the default value for each column, but also the number of columns & their types. In this example, we tell it that all feature columns are floats & that missing values should default to 0, but we provide an empty array of type `tf.float32` as the default value for the last column (the target): the array tells TensorFlow that this column contains floats, but that there is no default value, so it will raise an exception if it encounters a missing value.\n",
    "* The `decode_csv()` function returns a list of scaler tensors (one per column), but we need to return 1D tensor arrays. So we call `tf.stack()` on all tensors except for the last one (the target): this will stack these tensors into a 1D array. We then do the same for the target value (this makes it a 1D tensor array with a single value, rather than a scalar tensor).\n",
    "* Finally, we scale the input features by subtracting the feature means & then dividing by the feature standard deviations, & we return a tuple containing the scaled features & the target.\n",
    "\n",
    "Let's test this preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df933caf-3071-4eeb-8fe0-103622e9ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(b\"4.2083,44.0,5.3232,0.9181,846,0.2,3370,37.47,-122.2,2.782\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4f8e3-2270-48db-a0e3-d6e3b07e8071",
   "metadata": {},
   "source": [
    "Looks good! We can now apply the function to the dataset.\n",
    "\n",
    "## Putting Everything Together\n",
    "\n",
    "To make the code reusable, let's put together everything we have discussed so far into a small helper function: it will create & return a dataset that will efficiently load California housing data from multiple CSV files, preprocess it, shuffle it, optionally repeat it, & batch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98008b-f999-45e3-99c5-93e80c2c04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat = 1, n_readers = 5, n_read_threads = None,\n",
    "                       shuffle_buffer_size = 10000, n_parse_threads = 5, batch_size = 32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length = n_readers, \n",
    "        num_parallel_calls = n_read_threads\n",
    "    )\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041c789-5722-4264-ad6a-e04c30273fd9",
   "metadata": {},
   "source": [
    "Everything should make sense in this code, except the very last line (`prefetch(1)`), which is important for performance.\n",
    "\n",
    "<img src = \"Images/Load & Preprocess Data From Multiple CSV Files.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "## Prefetching\n",
    "\n",
    "By calling `prefetch(1)` at the end, we are creating a dataset that will do its best to always be one batch ahead. In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready (e.g., reading the data from disk & preprocessing it). This can improve performance dramatically, illustrated below.\n",
    "\n",
    "<img src = \"Images/Prefetching.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "If we also ensure that loading & preprocessing are multithreaded (by setting `num_parallel_calls` when calling `interleave()` & `map()`), we can exploid multiple cores on the CPU & hopefully make preparing one batch of data shorter than running a training step on the GPU: this way the GPU will be also 100% utilised (except for the data transfer time from the CPU to the GPU), & training will run much faster.\n",
    "\n",
    "If the dataset is small enough to fit in memory, you can significantly speed up training by using the dataset's `cache()` method to cache its content to RAM. You should generally do this after loading & preprocessing the data, but before shuffling, repeating, batching, & prefetching. This way, each instance will only be read & preprocessed once (instead of once per epoch), but the data will still be shuffled differently at each epoch, & the next batch will still be prepared in advance.\n",
    "\n",
    "You now know how to build efficient input pipelines to load & preprocess data from multiple text file. We have discussed the most common dataset methods, but there are a few more you may want to look at: `concatenate()`, `zip()`, `window()`, `reduce()`, `shard()`, `flat_map()`, & `padded_batch()`. There are also a couple more class methods: `from_generator()` & `from_tensors()`, which create a new dataset from a Python generator or a list of tensors, respectively. Check out the API documentation for more details.\n",
    "\n",
    "## Using the Dataset with tf.keras\n",
    "\n",
    "Now we can use the `csv_reader_dataset()` function to create a dataset for the training set. Note that we do not need to repeat it, as this will be taken care of by tf.keras. We also create datasets for the validation set & the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600dbc1c-05d8-4d85-aade-2fc472a70a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "val_set = csv_reader_dataset(val_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc0379-fdf0-46c8-bfdf-16e04f5c283d",
   "metadata": {},
   "source": [
    "Now we can simply build & train a keras model using these datasets. All we need to do is pass the training & validation datasets to the `fit()` method, instead of `X_train`, `y_train`, `X_val`, & `y_val`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45ebfa-cea0-4ed7-94bc-76b4d6ec13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequentail([\n",
    "    keras.layers.Dense(30, activation = \"relu\", input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mse\", optimizer = keras.optimizers.SGD(learning_rate = 1e-3))\n",
    "batch_size = 32\n",
    "model.fit(train_set, steps_per_epoch = len(X_train) // batch_size,\n",
    "          epochs = 10, validation_set = val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f4e2b-5891-42f9-a957-622a78bdea0f",
   "metadata": {},
   "source": [
    "Similarly, we can pass a dataset to the `evaluate()` & `predict()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419d127-55f0-46a7-a742-fae88e9f930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_set)\n",
    "new_set = test_set.take(3).map(lambda X, y: X)\n",
    "model.predict(new_set, steps = len(X_new) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb40a0-3537-4d75-8bdc-70f0c4653e41",
   "metadata": {},
   "source": [
    "Unlike the other sets, the `new_set` will usually not contain labels (if it does, keras will ignore them). Note that in all these cases, you can still use numpy arrays instead of datasets if you want (but of course they need to have been loaded & preprocessed first).\n",
    "\n",
    "If you want to build your own custom training loop, you can just iterate over the training set, very naturally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be70b93-0f85-4b5f-9f89-f5b9de82d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = keras.optimizers.Nadam(learning_rate = 0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "total_steps = n_epochs * n_steps_per_epoch\n",
    "global_step = 0\n",
    "\n",
    "for X_batch, y_batch in train_set.take(total_steps):\n",
    "    global_step += 1\n",
    "    print(\"\\rGlobal step {}/{}\".format(global_step, total_steps), end = \"\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X_batch)\n",
    "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "        loss = tf.add_n([main_loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e081119-6083-4cac-9db2-2056ff556f3f",
   "metadata": {},
   "source": [
    "In fact, it is even possible to create a tf function that performs the whole training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08088510-58cd-402c-b303-de1f3a1f7142",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (3653596466.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_62593/3653596466.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    def train(model, n_epochs, batch_size = 32, n_reader = 5, n_read_threads = 5,\u001b[0m\n\u001b[0m                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "optimiser = keras.optimizers.Nadam(learning_rate = 0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "@tf.function\n",
    "def train(model, n_epochs, batch_size = 32, n_readers = 5, n_read_threads = 5,\n",
    "          shuffle_buffer_size = 10000, n_parse_threads = 5):\n",
    "    train_set = csv_reader_dataset(train_filepaths, repeat = n_epochs, n_readers = n_readers,\n",
    "                                   n_read_threads = n_read_threads, \n",
    "                                   shuffle_buffer_size = shuffle_buffer_size,\n",
    "                                   n_parse_threads = n_parse_threads, batch_size = batch_size)\n",
    "    for X_batch, y_batch in train-set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "train(model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da5b53-0044-4887-934d-ce778ffde1a1",
   "metadata": {},
   "source": [
    "Congratulations, you now know how to build powerful input pipelines using the data API. However, so far, we have used CSV files, which are common, simple & convenient but not really efficient, & do not support large or complex data structures (such as images or audio) very well. Let's see how to use tfrecords instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002bb710-439c-461e-bf3c-25b47fabae1e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6970ef1-b914-4487-b65b-c6493bf7c1d5",
   "metadata": {},
   "source": [
    "# The TFRecord Format\n",
    "\n",
    "The tfrecord format in TensorFlow's preferred format for storing large amounts of data & reading it efficiently. It is a very simple binary format that just contains a sequence of binary records of varying sizes (each record is comprised of a length, a sequence of binary records of varying sizes (each record is comprised of a length, a CRC checksum to check that the length was not corrupted, then the actual data, & finally a CRC checksum for the data). You can easily create a tfrecord file using the `tf.io.TFRecordWriter` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9091c04c-02f9-483a-acd7-88a9bf8cbf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is my first record\")\n",
    "    f.write(b\"& this is the second record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e2700-9e8a-452b-bda1-fa648ee30049",
   "metadata": {},
   "source": [
    "You can then use a `tf.data.TFRecordDataset` to read one or more tfrecord files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbd30c5a-9809-40fb-a114-ccefcb76e744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is my first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'& this is the second record', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 15:38:23.426238: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8a2ce-f383-4b75-b0e0-aac77fc3571c",
   "metadata": {},
   "source": [
    "## Compressed TFRecord Files\n",
    "\n",
    "It can sometimes be useful to compress your tfrecord files, especially if they need to be loaded via a network connection. You can create a compressed tfrecord file by setting the `options` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "045714bb-c181-4e37-940d-da81f6d6a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type = \"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"& this is the second record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8798d5-5331-4110-8ba0-dca41a30157f",
   "metadata": {},
   "source": [
    "When reading a compressed tfrecord file, you need to specify the compression type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee36e9eb-f049-4238-8146-c5fc2c17d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
    "                                  compression_type = \"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63948dc5-3eca-4923-b714-d6f605df5550",
   "metadata": {},
   "source": [
    "## A Brief Introduction to Protocol Buffers\n",
    "\n",
    "Even though each record can use any binary format you want, tfrecord files usually contain serialised protocol buffers (also called *protobufs*). This is a portable, extensible, & efficient binary format developed at Google back in 2001 & made open source in 2008; protobugs are now widely used, in particular in gRPC, Google's remote procedure call system. They a defined using simple language that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa60850-3eeb-4081-9a15-7237717e987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax = \"proto3\";\n",
    "message Person {\n",
    "    string name = 1;\n",
    "    int32 id = 2;\n",
    "    repeated string email = 3;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbc91d-bf41-4640-97d9-9903f30600ad",
   "metadata": {},
   "source": [
    "This definition says we are using verions 3 of the protobuf format, & it specifies that each `Person` object may (optionally) have a `name` of type `string`, an `id` of type `int32`, & zero or more `email` fields, each of type `string`. The numbers 1, 2, & 3 are the field identifiers: they will be used in each record's binary representation. Once you have a definition in a *.proto* file, you can compile it. This requires `protoc`, the protobuf compiler, to generate access classes in Python (or some other language). Note that the protobuf definitions we will use have already been compiled for you, & their python classes are part of TensorFlow, so you will not need to use `protoc`. All you need to know is how to use protobuf access classes in python. To illustrate the basics, let's look at a simple example that uses the access classes generated for the `Person` protobuf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2dcfec1-ef0e-4f1c-8061-dce2958ada8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing person.proto\n"
     ]
    }
   ],
   "source": [
    "%%writefile person.proto\n",
    "syntax = \"proto3\";\n",
    "message Person {\n",
    "  string name = 1;\n",
    "  int32 id = 2;\n",
    "  repeated string email = 3;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f3d3b3e-1544-4c0e-8403-6c7cf1212229",
   "metadata": {},
   "outputs": [],
   "source": [
    "!protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "179f6364-dcfd-4d06-9077-3c53a5e6d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person.desc   person.proto  person_pb2.py\n"
     ]
    }
   ],
   "source": [
    "!ls person*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7dcfad33-0ecc-4242-abd6-834a744cfd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Al\"\n",
      "id: 123\n",
      "email: \"a@b.com\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from person_pb2 import Person # Import the generated access class\n",
    "\n",
    "person = Person(name = \"Al\", id = 123, email = [\"a@b.com\"]) # Create a person\n",
    "print(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f19ad3bc-055c-497b-b8b2-db82936c2b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Al'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.name # Read a field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c57dbd8a-56a3-4f7f-95be-c80e2b978d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.name = \"Alice\" # Modify a field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c27447b9-b29c-46e7-b046-26045c4dede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a@b.com'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.email[0] # Repeated fields can be accessed like arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c9fb820-16ce-4279-8291-2d9661edb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "person.email.append(\"c@d.com\") # Add an email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fbffdbd9-7723-47a7-a099-d2a0f758d433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = person.SerializeToString() # Serialize the object to a byte string\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d8b68add-b4ce-48f8-a322-6a06f29967ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "person2 = Person() # Create a new person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "66c10144-048b-485b-84bd-a9b7e482a915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person2.ParseFromString(s) # Parse the byte string (27 bytes long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "073ca410-d525-4dec-a721-80b7d2e530ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person == person2 # Now they are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c392a-a0c3-45e3-b374-a34dc8721004",
   "metadata": {},
   "source": [
    "In short, we import the `Person` class generated by `protoc`, we create an instance & play with it, visualising it & reading & writing some fields, then we serialise it using the `SerializeToString()` method. This is the binary data that is ready to be saved or transmitted over the network. When reading or receiving this binary data, we can parse it using the `ParseFromString()` method, & we get a copy of the object that was serialised.\n",
    "\n",
    "We could save the serialised `Person` object to a tfrecord file, then we could load & parse it: everything would work fine. However, `SerializeToString()` & `ParseFromString()` are not TensorFlow operations (& neither are the other operations in this code), so they cannot be included in a TensorFlow function (except by wrapping them in a `tf.py_function()` operation, which would make the code slower & less portable). Fortunately, TensorFlow does include specila protobuf definitions for which it provides parsing operations.\n",
    "\n",
    "## TensorFlow Protobufs\n",
    "\n",
    "The main protobuf typically used in a tfrecord file is the `Example` protobuf, which represents one instance in a dataset. It contains a list of named features, where each feature can either be a list of byte strings, a list of floats, or a list of integers. Here is the protobuf definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44984bab-f960-42ba-be18-6c92853c9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax = \"proto3\";\n",
    "message BytesList {repeated bytes value = 1}\n",
    "message FloatList {repeated float value = 1 [packed = true]}\n",
    "message Int64List {repeated int64 value = 1 [packed = true]}\n",
    "message Feature {\n",
    "    oneof kind {\n",
    "        BytesList bytes_list = 1;\n",
    "        FloatList float_list = 2;\n",
    "        Int64List int64_list = 3\n",
    "    }\n",
    "}\n",
    "message Features {map<string, Feature> feature = 1}\n",
    "message Example {Features features = 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296df762-22ea-4b47-9450-0927b02efd44",
   "metadata": {},
   "source": [
    "The definitions of `ByteList`, `FloatList`, & `Int64List` are straightforward enough. Note that [`packed = true`] is used for repeated numerical fields, for a more efficient encoding. A `Feature` contains either a `BytesList` a `FloatList` or an `Int64List`. A `Features` (with an `s`) contains a dictionary that maps a feature name to the corresponding feature value. Finally, an `Example` contains only a `Features` object. Here is how you could create a `tf.train.Example` representing the same person as earlier & write it to a tfrecord file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d092408a-7ba0-4918-a2c3-d1117084d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "person_example = Example(features = Features(\n",
    "    feature = {\"name\": Feature(bytes_list = BytesList(value = [b\"Alice\"])),\n",
    "               \"id\": Feature(int64_list = Int64List(value = [123])),\n",
    "               \"emails\": Feature(bytes_list = BytesList(value = [b\"a@b.com\",\n",
    "                                                                 b\"c@d.com\"]))}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f86940-1270-4f03-8d16-a869f13fd1f6",
   "metadata": {},
   "source": [
    "The code is a bit verbose & repetitive, but it's rather straightforward (& you could easily wrap it inside a small helper function). Now that we have an `Example` protobuf, we can serialize it by calling its `SerializeToString()` method, then write the resulting data to a tfrecord file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "640d1385-c1e3-4d3e-9f51-202849a113b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f786d56-94d9-4182-adb8-3e1785c0e22f",
   "metadata": {},
   "source": [
    "Normally you would write much more than one `Example`! Typically, you would create a conversion script that reads from your current format (say, CSV files), creates an `Example` protobuf for each instance, serialises them, & saves them to several tfrecord files, ideally shuffling them in the process. This requires a bit of work, so once again, make sure it is really necessary (perhaps your pipeline works fine with CSV files). \n",
    "\n",
    "Now that we have a nice tfrecord file containing a serialised `Example` let's try to load it.\n",
    "\n",
    "## Loading & Parsing Examples\n",
    "\n",
    "To load the serialised `Example` protobufs, we will use a `tf.data.TFRecordDataset` once again, & we will parse each `Example` using `tf.io.parse_single_example()`. This is a TensorFlow operation, so it can be included in a tf function. It requires at least two arguments: a string scalar tensor containing the serialised data, & a description of each feature. The description is a dictionary that maps each feature name to either a `tf.io.FixedLenFeature` descriptor indicating the feature's shape, type, & default value, or a `tf.io.VarLenFeature` descriptor indicating only the type (if the length of the feature's list may vary, such as the `\"emails\"` feature).\n",
    "\n",
    "The following code defines a description dictionary, then it iterates over the `TFRecordDataset` & parses the serialised `Example` protobuf this dataset contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3927200a-03e0-46aa-b228-321a34777a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 17:45:17.351546: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value = \"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value = 0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string)\n",
    "}\n",
    "\n",
    "for serialised_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
    "    parsed_example = tf.io.parse_single_example(serialised_example, feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0498e-0a35-4a2e-bb35-162e410d01ef",
   "metadata": {},
   "source": [
    "The fixed-length features are parsed as regular tensors, but the variable-length features are parsed as sparse tensors. You can convert a sparse tensor to a dense tensor using `tf.sparse.to_dense()`, but in this case it is simpler to just access its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a3740605-c50f-4726-bf3e-f813266c624d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(parsed_example[\"emails\"], default_value = b\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fd195f8c-adf6-44c9-94db-acc7693b492c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example[\"emails\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1317069-2457-43cf-93eb-f481bdc72dbf",
   "metadata": {},
   "source": [
    "A `BytesList` can contain any binary data you want, including any serialized object. For example, you can use `tf.io.encode_jpeg()` to encode an image using the JPEG format & put this binary data in a `BytesList`. Later, when your code reads the tfrecord, it will start by parsing the `Example` then it will need to call `tf.io.decode_jpeg()` to parse the data & get the original image (or you can use `tf.io.decode_image()`, which can decode any BMP, GIF, JPEG, or PNG image). You can also store any tensor you want in a `BytesList` by serialising the tensor using `tf.io.serialize_tensor()` then putting the resulting byte string in a `BytesList` feature. Later, when you parse the tfrecord, you can parse this data using `tf.io.parse_tensor()`.\n",
    "\n",
    "Instead of parsing examples one by one using `tf.io.parse_single_example()`, you may want to parse them batch by batch using `tf.io.parse_example()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a0a6e824-4e54-41bd-85ab-3c913ceded09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 18:02:25.096787: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\n",
    "for serialised_examples in dataset:\n",
    "    parsed_examples = tf.io.parse_example(serialised_examples,\n",
    "                                          feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b5cfc5-df3b-4419-85f0-05f40a0d1eb1",
   "metadata": {},
   "source": [
    "As you can see, the `Example` protobuf will probably be sufficient for most use cases. However, it may be a bit cumbersome to use when you are dealing with lists of lists. For example, suppose you want to classify text documents. Each document may be represented as a list of sentences, where each sentence is represented as a list of words. Perhaps each document also has a list of comments, where each comment is represented as a list of words. There may be some contextual data too, such as the document's author, title, & publication data. TensorFlow's `SequenceExample` protobuf is designed for such use cases.\n",
    "\n",
    "## Handling Lists of Lists Using the SequenceExample Protobuf\n",
    "\n",
    "Here is the definition of the `SequenceExample` protobuf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4ffde-5265-48de-b32a-dca9bd252eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message FeatureList {repeated Feature feature = 1; };\n",
    "message FeatureLists {map<string, FeatureList> feature_list = 1; };\n",
    "message SequenceExample {\n",
    "    Features context = 1;\n",
    "    FeatureLists feature_lists = 2;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1c952-942a-446c-8338-af572be11113",
   "metadata": {},
   "source": [
    "A `SequenceExample` contains a `Features` object for the contextual data & a `FeatureLists` object that contains one or more named `FeatureList` objects (e.g., a `FeatureList` named `\"content\"` & another named `\"comments\"`). Each `FeatureList` contains a list of `Feature` objects, each of which may be a list of byte strings, a list of 64-bit integers, or a list of floats (in this example, each `Feature` would rpresent a sentence or comment, perhaps in the form of a list of words identifiers). Building a `SequenceExample`, serializing it, & parsing it is similar to building, serializing, & parsing an `Example`, but you must use `tf.io.parse_single_sequence_example()` to parse a single `SequenceExample` or `tf.io.parse_sequence_example()` to parse a batch. Both functions return a tuple containing the context features (as a dictionary) & the feature lists (also a dictionary). If the feature lists contain sequences of varying sizes (as in the preceding example), you may want to convert them to ragged tensors, using `tf.RaggedTensor.from_sparse()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "63a3d57f-dc64-44f2-becf-3a684849bc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context {\n",
       "  feature {\n",
       "    key: \"author_id\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 123\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"pub_date\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1623\n",
       "        value: 12\n",
       "        value: 25\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"title\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"A\"\n",
       "        value: \"desert\"\n",
       "        value: \"place\"\n",
       "        value: \".\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "feature_lists {\n",
       "  feature_list {\n",
       "    key: \"comments\"\n",
       "    value {\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"When\"\n",
       "          value: \"the\"\n",
       "          value: \"hurlyburly\"\n",
       "          value: \"\\'s\"\n",
       "          value: \"done\"\n",
       "          value: \".\"\n",
       "        }\n",
       "      }\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"When\"\n",
       "          value: \"the\"\n",
       "          value: \"battle\"\n",
       "          value: \"\\'s\"\n",
       "          value: \"lost\"\n",
       "          value: \"and\"\n",
       "          value: \"won\"\n",
       "          value: \".\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature_list {\n",
       "    key: \"content\"\n",
       "    value {\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"When\"\n",
       "          value: \"shall\"\n",
       "          value: \"we\"\n",
       "          value: \"three\"\n",
       "          value: \"meet\"\n",
       "          value: \"again\"\n",
       "          value: \"?\"\n",
       "        }\n",
       "      }\n",
       "      feature {\n",
       "        bytes_list {\n",
       "          value: \"In\"\n",
       "          value: \"thunder\"\n",
       "          value: \",\"\n",
       "          value: \"lightning\"\n",
       "          value: \",\"\n",
       "          value: \"or\"\n",
       "          value: \"in\"\n",
       "          value: \"rain\"\n",
       "          value: \"?\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeatureList = tf.train.FeatureList\n",
    "FeatureLists = tf.train.FeatureLists\n",
    "SequenceExample = tf.train.SequenceExample\n",
    "\n",
    "context = Features(feature = {\"author_id\": Feature(int64_list = Int64List(value = [123])),\n",
    "                              \"title\": Feature(bytes_list = BytesList(value = [b\"A\", b\"desert\", b\"place\", b\".\"])),\n",
    "                              \"pub_date\": Feature(int64_list = Int64List(value = [1623, 12, 25]))})\n",
    "\n",
    "content = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"], \n",
    "           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\n",
    "comments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n",
    "            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n",
    "\n",
    "def words_to_feature(words):\n",
    "    return Feature(bytes_list = BytesList(value = [word.encode(\"utf-8\")\n",
    "                                                   for word in words]))\n",
    "\n",
    "content_features = [words_to_feature(sentence) for sentence in content]\n",
    "comments_features = [words_to_feature(comment) for comment in comments]\n",
    "\n",
    "sequence_example = SequenceExample(context = context,\n",
    "                                   feature_lists = FeatureLists(feature_list = {\n",
    "                                       \"content\": FeatureList(feature = content_features),\n",
    "                                       \"comments\": FeatureList(feature = comments_features)\n",
    "                                   }))\n",
    "sequence_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7debd283-dd64-4710-b0aa-4d2249336d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'When', b'shall', b'we', b'three', b'meet', b'again', b'?'],\n",
       " [b'In', b'thunder', b',', b'lightning', b',', b'or', b'in', b'rain', b'?']]>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialised_sequence_example = sequence_example.SerializeToString()\n",
    "\n",
    "context_feature_descriptions = {\n",
    "    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value = 0),\n",
    "    \"title\": tf.io.VarLenFeature(tf.string),\n",
    "    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value = [0, 0, 0])\n",
    "}\n",
    "\n",
    "sequence_feature_descriptions = {\n",
    "    \"content\": tf.io.VarLenFeature(tf.string),\n",
    "    \"comments\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
    "    serialised_sequence_example, context_feature_descriptions, sequence_feature_descriptions\n",
    ")\n",
    "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\n",
    "parsed_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f630b-cf1f-4100-8a2f-a7a50734489a",
   "metadata": {},
   "source": [
    "Now that you know how to efficient store, load, & parse data, the next step is to prepare it so that it can be fed to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147985b-a9a9-487a-81bc-ccdfe0cefa2a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee2d25-2416-41ee-a46e-a25bbef8e1c0",
   "metadata": {},
   "source": [
    "# Preprocessing the Input Features\n",
    "\n",
    "Preparing your data for a neural network requires converting all features into numerical features, generally normalising them, & more. In particular, if your data contains categorical features or text features, they need to be converted to numbers. This can be done ahead of time when preparing your data files, using any tool you like (e.g., numpy, pandas, scikit-learn). Alternatively, you can preprocess your data on the fly when loading it with the data API (e.g., using the dataset's `map()` method, as we saw earlier), or you can include a preprocessing layer directly in your model. Let's look at this last option now. \n",
    "\n",
    "For example, here is how you can implement a standardisation layer using a `Lambda` layer. For each feature, it subtracts the mean & divides by its standard deviation (plus a tiny smoothing term to avoid divisions by zero):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c800540-40b2-4335-80c4-f67c8462c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(X_train, axis = 0, keepdims = True)\n",
    "stds = np.std(X_train, axis = 0, keepdims = True)\n",
    "eps = keras.backend.epsilon()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),\n",
    "    [...] # other layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c149ee4-09b3-486d-a58e-c4a63259fef7",
   "metadata": {},
   "source": [
    "That's not too hard! However, you may prefer to use a nice self-contained custom layer (much like scikit-learn's `StandardScaler`), rather than having global variables like `means` & `stds` dangling around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e051ee5-7041-4fe2-9077-fe328eab85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardisation(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis = 0, keepdims = True)\n",
    "        self.stds_ = np.std(data_sample, axis = 0, keepdims = True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48481fd7-68dc-4af5-850b-51aa82c9b6a7",
   "metadata": {},
   "source": [
    "Before you can use this standardisation layer, you will need to adapt it to your dataset by calling the `adapt()` method & passing it a data sample. This will allow it to use the appropriate mean & standard deviation for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487bb13-f2b6-4aca-a523-1eead00c0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_layer = Standardization()\n",
    "std_layer.adapt(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b63b9-fe7c-4bb2-a7eb-a35ce41347c9",
   "metadata": {},
   "source": [
    "This sample must be large enough to be representative of your dataset, but it does not have to be the full training set: in general, a few hundred randomly selected instances will suffice (however, this depends on your task). Next, you can use this preprocessing layer like a normal layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e3750-321f-4642-9871-387c72afb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(std_layer)\n",
    "[...] # Create the rest of the model\n",
    "model.compile([...])\n",
    "model.fit([...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc94bc-0183-419a-86b4-589c5372b852",
   "metadata": {},
   "source": [
    "If you are thinking that keras should contain a standardisation layer like this one, here's some good news for you: by the time you read this, the `keras.layers.Normalization` layer will probably be available. It will work very much like our custom `Standardization` layer: first, create the layer, then adapt it to your dataset by passing a data sample to the `adapt()` method, & finally use the layer normally.\n",
    "\n",
    "Now, let's look at categorical features. We will start by encoding them as one-hot vectors.\n",
    "\n",
    "## Encoding Categorical Features Using One-Hot Vectors\n",
    "\n",
    "Consider the `ocean_proximity` feature in the California housing dataset we explored before: it is a categorical feature with five possible values: `\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", ISLAND\"`. We need to encode this feature before we feed it to a neural network. Since there are very few categories, we can use one-hot encoding. For this, we first need to map each category to its index (0 to 4), which can be done using a lookup table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "20acdf00-d5bc-40b8-bd38-02018d9b54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910e244-54e8-4487-a439-ee5fd0f08973",
   "metadata": {},
   "source": [
    "Let's go through this code:\n",
    "\n",
    "* We first define the *vocabulary*: this is the list of all possible categories.\n",
    "* Then we create a tensor with the corresponding indices (0 to 4).\n",
    "* Next, we create an initializer for the lookup table, passing it the list of categories & their corresponding indices. In this example, we already have this data, so we use a `KeyValueTensorInitializer`; but if the categories were listed in a text file (with on category per line), we would use a `TextFileInitializer` instead.\n",
    "* In the last two lines, we create the lookup table, giving it the initializer & specifying the number of *out-of-vocabulary* (oov) buckets. If we look up a category that does not exist in the vocabulary, the lookup table will compute a hash of this cateoogry & use it to assign the unknwon category to one of the oov buckets. Their indices start after the known categories, so in this example, the indices of the two oov buckets are 5 & 6.\n",
    "\n",
    "Why use oov buckets? Well, if the number of categories is large (e.g., zip codes, cities, words, products, orusers) & the dataset is large as well, or it keeps changing, then getting the full list of categories may be inconvenient. One solution is to define the vocabulary based on a data sample rather than the whole training set & add some oov buckets for the other categories that were not in the data sample. The more unknown categories you expect to find during training, the more oov buckets you should use. Indeed, if there are not enough oov buckets, there will be collisions: different categories will end up in the same bucket, so the neuralnetwork will not be able to distinguish them (at least not based on this feature).\n",
    "\n",
    "Now let's use the lookup table to encode a small batch of categorical features into one-hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "105a7097-2c47-49ef-a114-1a398dabd167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5e11dde8-14d4-4db0-80a9-02c89e7e172b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_one_hot = tf.one_hot(cat_indices, depth = len(vocab) + num_oov_buckets)\n",
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d4cf7-f247-43a9-afbf-b47854acbec3",
   "metadata": {},
   "source": [
    "As you can see, `\"NEAR BAY\"` was mapped to index 3, the unknown category `\"DESERT\"` was mapped to one of the two oov buckets (at index 5), & `\"INLAND\"` was mapped to index 1, twice. Then we used `tf.one_hot()` to one-hot encode these indices. Notice that we have to tell this function the total number of indices, which is equal to the vocabulary size plus the number of oov buckets. Now you know how to encode categorical features to one-hot vectors using TensorFlow!\n",
    "\n",
    "Just like earlier, it wouldn't be too difficult to bundle all of this logic into a nice self-contained class. Its `adapt()` method would take a data sample & extract all the distinct categories it contains. It would create a lookup table to map each category to its index (including unknown categories using oov buckets). Then its `call()` method would use the lookup table to map the input categories to their indices. \n",
    "\n",
    "Keras has a `keras.layers.TextVectorization` layer, which is capable of doing exactly that: its `adapt()` method will extract the vocabulary from a data sample, & its `call()` method will convert each category to its index in the vocabulary. You could add this layer at the beginning of your model, followed by a `Lambda` layer that would apply the `tf.one_hot()` function, if you want to convert these indices to one-hot vectors.\n",
    "\n",
    "This may not be the best solution, though. The size of each one-hot vector is the vocabulary length plus the number of oov buckets, this is fine when there are just a few possible categories, but if the vocabulary is large, it is much more efficient to encode them using *embeddings* instead.\n",
    "\n",
    "## Encoding Categorical Features Using Embeddings\n",
    "\n",
    "An embedding is a trainable dense vector that represents a category. By default, embeddings are initialised randomly, so for example, the `\"NEAR BAY\"` category could be represented initially by a random vector such as `[0.131, 0.890]`, while the `\"NEAR OCEAN\"` category might be represented by another random vector such as `[0.631, 0.791]`. In this example, we use 2D embeddings, but the number of dimensions is a hyperparameter you can tweak. Since these embeddings are trainable, they will gradually improve during training; & as they represent fairly similar categories, gradient descent will certainly end up pushing them closer together, while it will tend to move them away from the `\"INLAND\"` category's embedding. Indeed, the better the representation, the easier it will be for the neural network to make accurate predictions, so training tends to make embeddings useful representations of the categories. This is called *representation learning*.\n",
    "\n",
    "<img src = \"Images/Embeddings.png\" width = \"450\" style = \"margin:auto\"/>\n",
    "\n",
    "Let's look at how we could implement embeddings manually, to understand how they work (then we will use a simple keras layer instead). First, we need to create an *embedding matrix* containing each category's embedding, initialised randomly; it will ahve one row per category & per oov bucket, & one column per embedding dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ad6ac-24c9-433a-8416-441f120fb034",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78a056-352e-4774-baa5-30330e1a9789",
   "metadata": {},
   "source": [
    "In this example, we are using 2D embeddings, but as a rule of thumb embeddings typically have 10 to 300 dimensions, depending on the task & the vocabulary size (you will have to tune this hyperparameter).\n",
    "\n",
    "This embedding matrix is a random 6 x 2 matrix, stored in a variable (so it can be tweaked by gradient descent during training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1543a-0905-4b48-acf4-1c201aa11fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a93a01a-08b7-4671-a997-73a6b3fc8983",
   "metadata": {},
   "source": [
    "Now, let's encode the same batch of categorical features as earlier, but this time using those embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fbe2b41-b2f7-4c51-8bc5-b5b9414efedb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_65490/3687337597.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NEAR BAY\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DESERT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"INLAND\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"INLAND\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcat_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c556ee-f569-4db2-8688-43e17564effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1ffd8-b0be-438f-be25-3c8f555422a2",
   "metadata": {},
   "source": [
    "The `tf.nn.embedding_lookup()` function looks up the rows in the embedding matrix, at the given indices -- that's all it does. For example, the lookup table says that the `\"INLAND\"` category is at index 1, so the `tf.nn.embedding_lookup()` function returns the embedding at row 1 in the embedding matrix (twice): `[0.3528825, 0.46448255]`.\n",
    "\n",
    "Keras provides a `keras.layer.Embedding` layer that handles the embedding matrix (trainable, by default); when the layer is created, it initialises the embedding matrix randomly, & then when it is called with some category indices it returns the rows at those indices in the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4f009-a4bb-4860-aacc-92beee5a3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = keras.layers.Embedding(input_dim = len(vocab) + num_oov_buckets, \n",
    "                                   output_dim = embedding_dim)\n",
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a073b-986d-4a87-b3ea-4aa440ace70b",
   "metadata": {},
   "source": [
    "Putting everything together, we can now create a keras model that can process categorical features (along with regular numerical features) & learn an embedding for each category (as well as for each oov bucket):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6f723-5d30-4332-9ca1-4b65f0c4cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_inputs = keras.layers.Input(shape = [8])\n",
    "categories = keras.layers.Input(shape = [], dtype = tf.string)\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
    "cat_embed = keras.layers.Embedding(input_dim = 6, output_dim = 2)(cat_indices)\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
    "model = keras.models.Model(inputs = [regular_inputs, categories],\n",
    "                           outputs = [outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d659a-0e36-4f93-9fb3-ba1017f17ccd",
   "metadata": {},
   "source": [
    "This model takes two inputs: a regular input containing eight numerical features per instance, plus a categorical input (containing one categorical feature per instance). It uses a `Lambda` layer to look up each category's index, then it looks up the embeddings for these indices. Next, it concatenates the embeddings & the regular inputs in order to give the encoded inputs, which are ready to be fed to a neural network. We could add any kind of neural network at this point, but we just add a dense output layer, & we create the keras model.\n",
    "\n",
    "When the `keras.layers.TextVectorization` layer is available, you can call its `adapt` method to make it extract the vocabulary from a data sample (it will take care of creating the lookup table for you). Then you can add it to your model, & it will perform the index lookup (replacing the `Lambda` layer in the previous code example).\n",
    "\n",
    "## Keras Preprocessing Layers\n",
    "\n",
    "The TensorFlow team is working on providing a set of standard keras preprocessing layers. They will probably be available by now; however, the API may change slightly, so refer to the documentation if anything behaves unexpectedly. This API will likely supersed the existing feature columns API, which is harder to use & less intuitive.\n",
    "\n",
    "We already discussed two of these layers: the `keras.layers.Normalization` layer that will perform feature standardisation (it will be equivalent to the `Standardization` layer we defined earlier), & the `TextVectorization` layer that will be capable of encoding each word in the inputs into its index in the vocabulary. In both cases, you create the layer, you call its `adapt()` method with a data sample, & then you use the layer normally in your model. The other preprocessing layers will follow the same pattern.\n",
    "\n",
    "The API will also include a `keras.layers.Discretization` layer that will chop continuous data into different bins & encode each bin as a one-hot vector. For example, you could use it to discretize prices into three categories (low, medium, high), which would be encoded as [1, 0, 0], [0, 1, 0], & [0, 0, 1], respectively. Of course, this loses a lot of information, but in some cases, it can help the model detect patterns that would otherwise not be obvious when just looking at the continuous values.\n",
    "\n",
    "It will also be possible to chain multiple preprocessing layers using the `PreprocessingStage` class. For example, the following code will create a preprocessing pipeline that will first normalize the inputs, then discretize them (this may remind you of scikit-learn pipelines). After you adapt this piepline to a data sample, you can use it like a regular layer in your models (but again, only at the start of the model, since it contains a nondifferentiable preprocessing layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eab262-9c5f-49c0-af51-2b3149bad84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation = keras.layers.Normalization()\n",
    "discretisation = keras.layers.Discretization([...])\n",
    "pipeline = keras.layers.PreprocessingStage([normalisation, discretisation])\n",
    "pipeline.adapt(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4cf4df-e8de-47e7-9c26-17349ea73c0c",
   "metadata": {},
   "source": [
    "The `TextVectorization` layer will also have an option to output word-count vectors instead of word indices. For example, if the vocabulary contains three words, say `[\"and\", \"basketball\", \"more\"]`, then the text `\"more and more\"` will be mapped to the vector `[1, 0, 2]`: the word `\"and\"` appears once, the word `\"basketball\"` does not appear at all, & the word `\"more\"` appears twice. This text representation is called a *bag of words*, since it completely loses the order of the words. Common words like `\"and\"` will ahve a large value in most text, even though they are usually the least interesting (e.g., in the text `\"more and more basketball\"`, the word `\"basketball\"` is clearly the most important, precisely because it is not a very frequent word). So, the word counts should be normalised in a way that reduces the important of frequent words. A common way to do this is to divide each word count by the log of the total number of training instances in which the word appears. This technique is called *Term-Frequency x Inverse-Document-Frequency* (TF-IDF). For example, let's imagine that the words `\"and\"`, `\"basketball\"`, & `\"more\"` appear respectively in 200, 10, & 10 text instances in the training set: in thei case, the final vector will be `[1/log(200), 0/log(10), 2/log(100)]`, which is approximately equal to `[0.19, 0.0, 0.43]`. The `TextVectorization` layer will (likely) have an option to perform TF-IDF.\n",
    "\n",
    "As youc an see, these keras preprocessing layers will make preprocessing much easier! Now, whether you choose to write your own preprocessing layers or use Kera's (or even the feature columns API), all the preprocessing will be done on the fly. During traiing, however, it maybe preferable to perform preprocessing ahead of time. let's see why we'd want to do that & how we'd go about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8bece-f044-4fd9-a61a-2e56a967f17a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102df30-3559-48be-997d-1d637b4ea4dc",
   "metadata": {},
   "source": [
    "# TF Transform\n",
    "\n",
    "If preprocessing is computationally expensive, then handling it before trianing rather than on the fly may five you a significant speedup: the data will be preprocessed just once per instance *before* training, rather than once per instance & per epoch *during* training. As mentioned earlier, if the dataset is small enouh to fit in RAM, you can use its `cache()` method. But if it is too large, then tools like apache beam or spark will help. They let you run efficient data processing pipelines over large amounts of data, even distributed across multiple servers, so youc an use them to preprocess all teh training data before training.\n",
    "\n",
    "This works great & indeed can speed up training, but there is one problem: once your model is trained, suppose you want to deploy it to a mobile app. In that case, you will need to write some code in your app to take care of preprocessing the data before it is fed tothe model. Suppose you also want to deploy the model to TensorFlow.js so that it runs in a web browser? Oce again, you will need to write some preprocessing code. This can become a maintenance nightmare: whever you want to change the preprocessing logic, you will need to update yoru apache beam code, your mobile app code, & your javascript code. This is not only time-consuming, but also error-prone: you may end up with subtle differences between the preprocessing operations performed before training & the ones performed in your app or in the browser. This *training/serving skew* will lead to bugs or degraded performance.\n",
    "\n",
    "One improvement would be to take the trained model (trained on data that was preprocessed by your apache beam or spark code) & before deploying it to your app or the browser, add extra preprocessing layers to take care of preprocessing on the fly. That's definitely better, since now you just have two versions of your preprocessing code: the apache beam or spark code, & the preprocessing layers' code.\n",
    "\n",
    "But what if you could define your preprocessing operations just once? This is what tf transform was designed for. It is part of TensorFlow Extended (TFX), an end-to-end platform for productionising TensorFlow models. First, to use a TFX component such as tf transform, you must install it; it does not come bundled with TensorFlow. You then define your preprocessing function just once (in Python), by using tf transform functions for scaling, bucketising, & more. You can also use any TensorFlow oepration you need. Here is what this preprocessing function might look like if we just had two features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200237a6-166e-45b6-8628-d702a7a5d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft\n",
    "\n",
    "def preprocess(inputs):  # inputs = a batch of input features\n",
    "    median_age = inputs[\"housing_median_age\"]\n",
    "    ocean_proximity = inputs[\"ocean_proximity\"]\n",
    "    standardised_age = tft.scale_to_z_score(median_age)\n",
    "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "    return {\"standardised_median_age\": standardised_age,\n",
    "            \"ocrean_proximity_id\": ocean_proximity_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5a421-6033-47a5-ad92-5c42004dd67b",
   "metadata": {},
   "source": [
    "Next, tf transform lets you apply this `preprocess()` function to the whole training set using apache beam (it provides an `AnalyzeandTransformDataset` class that you can use for this purpose in your apache beam pipeline). In the process, it will also compute all the necessary statistics over the whole training set: in this example, the mean & standard deviation of the `housing_median_age` feature, & the vocabulary for the `ocean_proximity` feature. The components that compute these statistics are called *analyzers*.\n",
    "\n",
    "Importantly, tf transform will also generate an equivalent TensorFlow function that you can plug into the model you deploy. This tf function includes some constants that correspond to all the all the necessary statistics computed by apache beam (the mean, standard deviation, & vocabulary).\n",
    "\n",
    "With the data API, tfrecords, the keras preprocessing layers, & tf transform, you can build highly scalable input pipelines for training & benefit from fast & portable data preprocessing in production.\n",
    "\n",
    "But what if you just wanted to use a standard dataset? Well in that case, things are much simpler: just use TFDS!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ca9d3-bb12-4eac-88d9-4a9775f3013a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d196ce66-4b53-4f6f-9501-64da2b92514c",
   "metadata": {},
   "source": [
    "# The TensorFlow Datasets (TFDS) Project\n",
    "\n",
    "The TensorFlow Datasets project makes it very easy to download common datasets, from small ones like MNIST or Fashion MNIST to huge datasets like imagenet (you will need quite a bit of disk space). The list includes image datasets, text datasets (including translation datasets), & audio & video datasets.\n",
    "\n",
    "TFDS is not bundled with TensorFlow, so you need to install the `tensorflow_datasets` library (e.g., using pip). Then call the `tfds.load()` function, & it will download the data you want (unless it was already downloaded earlier) & return the data as a dictionary of datasets (typically one for training & one for testing, but this depends on the dataset you choose). For example, let's download MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7765a1c8-a2fd-493c-bfe0-6aa3763f7d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 14:35:30.294588: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-09-08 14:35:33.864379: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/jiehengyu/tensorflow_datasets/mnist/3.0.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32af5d36cebd49c594dbf336ac31eaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mnist downloaded and prepared to /Users/jiehengyu/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset = tfds.load(name = \"mnist\")\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbe1b0-ea42-4196-8ca3-b019f4332625",
   "metadata": {},
   "source": [
    "You can then apply any transformation you want (typically shuffling, batching, & prefetching), & you're ready to train your model. Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b8bd4-7b28-4763-a287-937f872c9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)\n",
    "for item in mnist_train:\n",
    "    images = item[\"image\"]\n",
    "    labels = item[\"label\"]\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ea90a-060a-4e85-9267-2c653ff435ef",
   "metadata": {},
   "source": [
    "Note that each item in the dataset is a dictionary containing both the features & the labels. But keras expects each item to be a tuple containing two elements (again, the features & the labels). You could transform the dataset using the `map()` method, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd593b7-3bdc-4fb2-8359-64e788132126",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(10000).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a60d57-0e1b-43d0-a720-23d9ea929ce0",
   "metadata": {},
   "source": [
    "But it's simpler to ask the `load()` function to do this for you by setting `as_supervised = True` (obviously this works only for labeled datasets). You can also specify the batch size if you want. Then you can pass the dataset directly to your tf.keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a1061-6353-4a6b-af58-c58030b38808",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(name = \"mnist\", batch_size = 32, as_supervised = True)\n",
    "mnist_train = dataset[\"train\"].prefetch(1)\n",
    "model = keras.models.Sequential([...])\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"sgd\")\n",
    "model.fit(mnist_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566296f-1b19-4597-bc7c-80a0dad7e4f9",
   "metadata": {},
   "source": [
    "This was quite a technical lesson, & you may feel that it is a bit far form the abstract beauty of neural networks, but the fact is deep learning often involves large amounts of data, & knowing how to load, parse, & preprocess it efficiently is a crucial skill to have."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
