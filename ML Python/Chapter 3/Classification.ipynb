{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9938417b-b2bd-480c-9634-9eecbe38d879",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In Chapter 1, we mentioned that most common supervised learning tasks are regression (predicting values) & classification (predicting classes). In Chapter 2, we explored a regression task, predicting housing values, using various algorithms such as linear regression task, predicting housing values, using various algorithms such as linear regression, decision tree, & random forests. Now we will turn our attention to classification systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6a658-2106-4c81-9549-fdd11d51b0c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47015497-23e3-4cef-a50e-3fb3e7110ba1",
   "metadata": {},
   "source": [
    "# MNIST \n",
    "\n",
    "In this chapter, we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students & employees of the US Census Bureau. Each image is labedl with the digit it represents. This set has ben studied so much that it is often called the \"Hello World\" of machine learning: whenever people come up with a new classification algorithm, they are curious to see how it will perform on MNIST. Whenever someone learns machine learning, sooner or later, they tackle MNIST.\n",
    "\n",
    "Scikit-Learn provides many helper functions to download popular datasets. MNIST is one of them. The following code fetches the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea783d6-8e95-44e7-bc77-b12a220e934e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version = 1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3896274-3d55-4628-8fb8-8d83e24acccf",
   "metadata": {},
   "source": [
    "Datasets loaded by Scikit-Learn generally have a similar dictionary structure including:\n",
    "\n",
    "* a `DESCR` key describing the dataset\n",
    "* a `data` key containing an array with one row per instance & one column per feature\n",
    "* a `target` key containing an array with the labels\n",
    "\n",
    "Let's look at these arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6001e82-e441-4422-a447-30d334504b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (70000, 784)\n",
      "y.shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(\"X.shape: {}\".format(X.shape))\n",
    "print(\"y.shape: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1556e-d64c-4d4d-9666-2ddf61c9e02d",
   "metadata": {},
   "source": [
    "There are 70,000 images, & each image has 784 features. This is because each image is 28x28 pixels, & each feature simply represents one pixel's intensity, from 0 (white) & 255 (black). Let's take a peek at one digit from the dataset. All you need to do is grab an instance's feature vector, reshape it to a 28x28 array, & display it using matplotlib's `imshow()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60178fa-96ab-467c-b0ae-3bdc26f1a031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb451c8a5e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOUElEQVR4nO3dX4xUdZrG8ecFwT8MKiyt2zJEZtGYIRqBlLAJG0Qni38SBS5mAzGIxogXIDMJxEW5gAsvjO7MZBQzplEDbEYmhJEIiRkHCcYQE0OhTAuLLGpapkeEIkTH0QsU373ow6bFrl81VafqlP1+P0mnquup0+dNhYdTXae6fubuAjD0DSt6AACtQdmBICg7EARlB4Kg7EAQF7RyZ+PGjfOJEye2cpdAKD09PTp58qQNlDVUdjO7XdJvJQ2X9Ly7P5G6/8SJE1UulxvZJYCEUqlUNav7abyZDZf0rKQ7JE2WtNDMJtf78wA0VyO/s0+X9IG7f+TupyX9QdLcfMYCkLdGyj5e0l/7fd+b3fYdZrbEzMpmVq5UKg3sDkAjGin7QC8CfO+9t+7e5e4ldy91dHQ0sDsAjWik7L2SJvT7/seSPmlsHADN0kjZ90q61sx+YmYjJS2QtD2fsQDkre5Tb+7+jZktk/Sa+k69vejuB3ObDECuGjrP7u6vSno1p1kANBFvlwWCoOxAEJQdCIKyA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiCsgNBUHYgCMoOBEHZgSAoOxAEZQeCoOxAEJQdCIKyA0FQdiCIhlZxRfs7c+ZMMv/888+buv9169ZVzb766qvktocPH07mzz77bDJfuXJl1Wzz5s3JbS+66KJkvmrVqmS+Zs2aZF6EhspuZj2SvpB0RtI37l7KYygA+cvjyH6Lu5/M4ecAaCJ+ZweCaLTsLunPZrbPzJYMdAczW2JmZTMrVyqVBncHoF6Nln2mu0+TdIekpWY269w7uHuXu5fcvdTR0dHg7gDUq6Gyu/sn2eUJSdskTc9jKAD5q7vsZjbKzEafvS5pjqQDeQ0GIF+NvBp/paRtZnb257zk7n/KZaoh5ujRo8n89OnTyfytt95K5nv27KmaffbZZ8ltt27dmsyLNGHChGT+8MMPJ/Nt27ZVzUaPHp3c9sYbb0zmN998czJvR3WX3d0/kpR+RAC0DU69AUFQdiAIyg4EQdmBICg7EAR/4pqDd999N5nfeuutybzZf2baroYPH57MH3/88WQ+atSoZH7PPfdUza666qrktmPGjEnm1113XTJvRxzZgSAoOxAEZQeCoOxAEJQdCIKyA0FQdiAIzrPn4Oqrr07m48aNS+btfJ59xowZybzW+ejdu3dXzUaOHJncdtGiRckc54cjOxAEZQeCoOxAEJQdCIKyA0FQdiAIyg4EwXn2HIwdOzaZP/XUU8l8x44dyXzq1KnJfPny5ck8ZcqUKcn89ddfT+a1/qb8wIHqSwk8/fTTyW2RL47sQBCUHQiCsgNBUHYgCMoOBEHZgSAoOxAE59lbYN68ecm81ufK11peuLu7u2r2/PPPJ7dduXJlMq91Hr2W66+/vmrW1dXV0M/G+al5ZDezF83shJkd6HfbWDPbaWZHssv0JxgAKNxgnsZvkHT7ObetkrTL3a+VtCv7HkAbq1l2d39T0qlzbp4raWN2faOkefmOBSBv9b5Ad6W7H5Ok7PKKanc0syVmVjazcqVSqXN3ABrV9Ffj3b3L3UvuXuro6Gj27gBUUW/Zj5tZpyRllyfyGwlAM9Rb9u2SFmfXF0t6JZ9xADRLzfPsZrZZ0mxJ48ysV9IaSU9I2mJmD0g6KunnzRxyqLv00ksb2v6yyy6re9ta5+EXLFiQzIcN431ZPxQ1y+7uC6tEP8t5FgBNxH/LQBCUHQiCsgNBUHYgCMoOBMGfuA4Ba9eurZrt27cvue0bb7yRzGt9lPScOXOSOdoHR3YgCMoOBEHZgSAoOxAEZQeCoOxAEJQdCILz7ENA6uOe169fn9x22rRpyfzBBx9M5rfccksyL5VKVbOlS5cmtzWzZI7zw5EdCIKyA0FQdiAIyg4EQdmBICg7EARlB4LgPPsQN2nSpGS+YcOGZH7//fcn802bNtWdf/nll8lt77333mTe2dmZzPFdHNmBICg7EARlB4Kg7EAQlB0IgrIDQVB2IAjOswc3f/78ZH7NNdck8xUrViTz1OfOP/roo8ltP/7442S+evXqZD5+/PhkHk3NI7uZvWhmJ8zsQL/b1prZ38xsf/Z1Z3PHBNCowTyN3yDp9gFu/427T8m+Xs13LAB5q1l2d39T0qkWzAKgiRp5gW6ZmXVnT/PHVLuTmS0xs7KZlSuVSgO7A9CIesv+O0mTJE2RdEzSr6rd0d273L3k7qWOjo46dwegUXWV3d2Pu/sZd/9W0npJ0/MdC0De6iq7mfX/28L5kg5Uuy+A9lDzPLuZbZY0W9I4M+uVtEbSbDObIskl9Uh6qHkjokg33HBDMt+yZUsy37FjR9XsvvvuS2773HPPJfMjR44k8507dybzaGqW3d0XDnDzC02YBUAT8XZZIAjKDgRB2YEgKDsQBGUHgjB3b9nOSqWSl8vllu0P7e3CCy9M5l9//XUyHzFiRDJ/7bXXqmazZ89ObvtDVSqVVC6XB1zrmiM7EARlB4Kg7EAQlB0IgrIDQVB2IAjKDgTBR0kjqbu7O5lv3bo1me/du7dqVus8ei2TJ09O5rNmzWro5w81HNmBICg7EARlB4Kg7EAQlB0IgrIDQVB2IAjOsw9xhw8fTubPPPNMMn/55ZeT+aeffnreMw3WBRek/3l2dnYm82HDOJb1x6MBBEHZgSAoOxAEZQeCoOxAEJQdCIKyA0Fwnv0HoNa57Jdeeqlqtm7duuS2PT099YyUi5tuuimZr169OpnffffdeY4z5NU8spvZBDPbbWaHzOygmf0iu32sme00syPZ5ZjmjwugXoN5Gv+NpBXu/lNJ/yppqZlNlrRK0i53v1bSrux7AG2qZtnd/Zi7v5Nd/0LSIUnjJc2VtDG720ZJ85o0I4AcnNcLdGY2UdJUSW9LutLdj0l9/yFIuqLKNkvMrGxm5Uql0uC4AOo16LKb2Y8k/VHSL93974Pdzt273L3k7qWOjo56ZgSQg0GV3cxGqK/ov3f3s38GddzMOrO8U9KJ5owIIA81T72ZmUl6QdIhd/91v2i7pMWSnsguX2nKhEPA8ePHk/nBgweT+bJly5L5+++/f94z5WXGjBnJ/JFHHqmazZ07N7ktf6Kar8GcZ58paZGk98xsf3bbY+or+RYze0DSUUk/b8qEAHJRs+zuvkfSgIu7S/pZvuMAaBaeJwFBUHYgCMoOBEHZgSAoOxAEf+I6SKdOnaqaPfTQQ8lt9+/fn8w//PDDekbKxcyZM5P5ihUrkvltt92WzC+++OLzngnNwZEdCIKyA0FQdiAIyg4EQdmBICg7EARlB4IIc5797bffTuZPPvlkMt+7d2/VrLe3t66Z8nLJJZdUzZYvX57cttbHNY8aNaqumdB+OLIDQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBBhzrNv27atobwRkydPTuZ33XVXMh8+fHgyX7lyZdXs8ssvT26LODiyA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQ5u7pO5hNkLRJ0j9L+lZSl7v/1szWSnpQUiW762Pu/mrqZ5VKJS+Xyw0PDWBgpVJJ5XJ5wFWXB/Ommm8krXD3d8xstKR9ZrYzy37j7v+V16AAmmcw67Mfk3Qsu/6FmR2SNL7ZgwHI13n9zm5mEyVNlXT2M56WmVm3mb1oZmOqbLPEzMpmVq5UKgPdBUALDLrsZvYjSX+U9Et3/7uk30maJGmK+o78vxpoO3fvcveSu5c6OjoanxhAXQZVdjMbob6i/97dX5Ykdz/u7mfc/VtJ6yVNb96YABpVs+xmZpJekHTI3X/d7/bOfnebL+lA/uMByMtgXo2fKWmRpPfMbH9222OSFprZFEkuqUdSet1iAIUazKvxeyQNdN4ueU4dQHvhHXRAEJQdCIKyA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQVB2IAjKDgRB2YEgan6UdK47M6tI+rjfTeMknWzZAOenXWdr17kkZqtXnrNd7e4Dfv5bS8v+vZ2bld29VNgACe06W7vOJTFbvVo1G0/jgSAoOxBE0WXvKnj/Ke06W7vOJTFbvVoyW6G/swNonaKP7ABahLIDQRRSdjO73cwOm9kHZraqiBmqMbMeM3vPzPabWaHrS2dr6J0wswP9bhtrZjvN7Eh2OeAaewXNttbM/pY9dvvN7M6CZptgZrvN7JCZHTSzX2S3F/rYJeZqyePW8t/ZzWy4pP+V9O+SeiXtlbTQ3f+npYNUYWY9kkruXvgbMMxslqR/SNrk7tdntz0p6ZS7P5H9RznG3f+zTWZbK+kfRS/jna1W1Nl/mXFJ8yTdpwIfu8Rc/6EWPG5FHNmnS/rA3T9y99OS/iBpbgFztD13f1PSqXNunitpY3Z9o/r+sbRcldnagrsfc/d3sutfSDq7zHihj11irpYoouzjJf213/e9aq/13l3Sn81sn5ktKXqYAVzp7sekvn88kq4oeJ5z1VzGu5XOWWa8bR67epY/b1QRZR9oKal2Ov83092nSbpD0tLs6SoGZ1DLeLfKAMuMt4V6lz9vVBFl75U0od/3P5b0SQFzDMjdP8kuT0japvZbivr42RV0s8sTBc/z/9ppGe+BlhlXGzx2RS5/XkTZ90q61sx+YmYjJS2QtL2AOb7HzEZlL5zIzEZJmqP2W4p6u6TF2fXFkl4pcJbvaJdlvKstM66CH7vClz9395Z/SbpTfa/IfyhpdREzVJnrXyT9Jfs6WPRskjar72nd1+p7RvSApH+StEvSkexybBvN9t+S3pPUrb5idRY027+p71fDbkn7s687i37sEnO15HHj7bJAELyDDgiCsgNBUHYgCMoOBEHZgSAoOxAEZQeC+D+ypTV9clByEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X.iloc[0].to_numpy()\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation = \"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec08a8-2367-4b6b-aabb-f7f87619c896",
   "metadata": {},
   "source": [
    "This looks like a 5, & indeed, that's what the label tells us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f642a0e-b3f2-4660-9e84-163252d9bb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26491d10-307f-4761-9354-aa53b6c019f3",
   "metadata": {},
   "source": [
    "Note that the label is a string. Since machine learning algorithms prefer numbers, let's cast `y` to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45122b17-c6f6-40db-bdab-876a3871cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674bdd6-7506-4bb9-bcf9-30e30e7f6661",
   "metadata": {},
   "source": [
    "The below figure shows a few more images from the MNIST dataset to give you a feel for the complexity of the classification task.\n",
    "\n",
    "<img src = \"Images/MNIST.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "But wait! You should always create a test set & set it aside before inspecting the data closely. The MNIST dataset is actually already split into a training set (the first 60,000 images) & a test set (the last 10,000 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd50232-41ad-440d-9f8b-d5350ed04bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da71d78-ff12-4170-a63b-b0d752824c5a",
   "metadata": {},
   "source": [
    "The training set is already shuffled for us, which is good as this guarantees that all cross-validation folds will be similar (you don't want one fold to be missing some digits). Moreover, some learning algorithms are sensitive to the order of the training instances, & they perform poorly if they get many similar instances in a row. Shuffling the dataset ensures that this won't happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b43f9-18ea-4791-b290-1eb092c1444c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39533043-f8ff-45d5-9b85-8449c94e5543",
   "metadata": {},
   "source": [
    "# Training a Binary Classifier\n",
    "\n",
    "Let's simplify the problem for now & only try to identify one digit -- for example, the number 5. This \"5-detector\" will be an example of a *binary classifier*, capable of distinguishing between just two classes, 5 & not 5. Let's create the target vectors for this classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa1c893-49c9-4823-b09d-df8f2c3ed098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.9/site-packages (1.3.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Return True if training set label == 5.\n",
    "y_train_5 = (y_train == 5)\n",
    "\n",
    "# Return True if testing set label == 5.\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b093693-70d5-4a48-9364-e8e2859c6523",
   "metadata": {},
   "source": [
    "Okay, now let's pick a classifier & train it. A good place to start is with a *Stochastic Gradient Descent* (SGD) classifier, using Scikit-Learn's `SGDClassifier` class. This classifier has the advantage of being capable of handling very large datasets efficiently. This is in part because SGD deals with training instances independently, one at a time (which also makes SGD well suited for *online learning*). Let's create an `SGDClassifier` & train it on the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099b913d-9e61-4ea6-9618-0e923a76f4ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'METRIC_MAPPING64' from 'sklearn.metrics._dist_metrics' (/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_dist_metrics.cpython-39-darwin.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGDClassifier\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a SGD classifier instance.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sgd_clf \u001b[38;5;241m=\u001b[39m SGDClassifier(random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesianRidge, ARDRegression\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_least_angle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     Lars,\n\u001b[1;32m     13\u001b[0m     LassoLars,\n\u001b[1;32m     14\u001b[0m     lars_path,\n\u001b[1;32m     15\u001b[0m     lars_path_gram,\n\u001b[1;32m     16\u001b[0m     LarsCV,\n\u001b[1;32m     17\u001b[0m     LassoLarsCV,\n\u001b[1;32m     18\u001b[0m     LassoLarsIC,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_coordinate_descent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     Lasso,\n\u001b[1;32m     22\u001b[0m     ElasticNet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     MultiTaskLassoCV,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_glm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PoissonRegressor, GammaRegressor, TweedieRegressor\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arrayfuncs, as_float_array  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_random_state\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvergenceWarning\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_split\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_split\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_predict\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_validate\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetaestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_split\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_scoring\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_scorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _check_multimetric_scoring, _MultimetricScorer\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FitFailedWarning\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/__init__.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multilabel_confusion_matrix\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adjusted_mutual_info_score\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adjusted_rand_score\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/cluster/__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fowlkes_mallows_score\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m silhouette_samples\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m silhouette_score\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calinski_harabasz_score\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/cluster/_unsupervised.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances_chunked\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sp_version, parse_version\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PairwiseDistancesArgKmin\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _chi2_kernel_fast, _sparse_manhattan\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_pairwise_distances_reduction/__init__.py:89\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pairwise Distances Reductions\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#    using Generalized Matrix Multiplication over `float64` data (see the\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#    docstring of :class:`GEMMTermComputer64` for details).\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     90\u001b[0m     ArgKmin,\n\u001b[1;32m     91\u001b[0m     ArgKminClassMode,\n\u001b[1;32m     92\u001b[0m     BaseDistancesReductionDispatcher,\n\u001b[1;32m     93\u001b[0m     RadiusNeighbors,\n\u001b[1;32m     94\u001b[0m     sqeuclidean_row_norms,\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     97\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseDistancesReductionDispatcher\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgKmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean_row_norms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    103\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOOL_METRICS, METRIC_MAPPING64\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArgKmin32,\n\u001b[1;32m     11\u001b[0m     ArgKmin64,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin_classmode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     ArgKminClassMode32,\n\u001b[1;32m     15\u001b[0m     ArgKminClassMode64,\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'METRIC_MAPPING64' from 'sklearn.metrics._dist_metrics' (/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_dist_metrics.cpython-39-darwin.so)"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Create a SGD classifier instance.\n",
    "sgd_clf = SGDClassifier(random_state = 42)\n",
    "\n",
    "# Fit the SGD classifier instance to the training set.\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b50ed-2569-471f-9c90-e8d287aeb448",
   "metadata": {},
   "source": [
    "Now you can use it to detect images of the number 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196bd0f5-83e5-40c0-8d6b-86e1265ed25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SGD classifier to predict the class of some_digit.\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a69b06-1fa4-47ff-89bf-3b2c0558f607",
   "metadata": {},
   "source": [
    "The classifier gueses that this image represents a 5 (`True`). Looks like it guessed right in this particular case! Now, let's evaluate this model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307e6e4-2e37-4988-bcc9-20395cc85a69",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f1b88-89b6-4b8b-9109-04209869b331",
   "metadata": {},
   "source": [
    "# Performance Measures\n",
    "\n",
    "Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large part of this chapter on this topic. There are many performance measures available so buckle down.\n",
    "\n",
    "## Measuring Accuracy Using Cross-Validation\n",
    "\n",
    "A good way to evaluate a model is to use cross-validation. Let's use the `cross_cal_score()` function to evaluate your `SGDClassifier` model using K-fold cross-validation, with 10 folds. Remembers that K-fold cross-validation means splitting the training set into K-folds (in this case, 10), then making predictions & evaluating them on each fold using a model trained on the remaining folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5183193-c1cb-482f-90ba-d2bdb047f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate the accuracy of our SGD classifier by cross-validating the SGD classifier.\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv = 10, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f93cde-c47c-4c87-97aa-cf0a32560532",
   "metadata": {},
   "source": [
    "Wow! We get over 95% *accuracy* (ratio of correct predictions) on all cross-validation folds. This looks amazing! Well, before we get too excited, let's look at a very dumb classifier that just classifies every single image in the \"not 5\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d943fb1e-b851-444c-8e45-fcacad4932e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# Define the object class.\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    # Does nothing.\n",
    "    def fit(self, X, y = None):\n",
    "        pass\n",
    "    # Returns an array the length of the training set, filled with only zeros.\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype = bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc0e79-f0e3-431f-ab0d-e178a4a98e6b",
   "metadata": {},
   "source": [
    "Can you guess this model's accuracy? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92435367-dea6-4080-9cc1-3fb4eca9ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "\n",
    "# Evaluate the accuracy of our newly defined classifier by cross-validating it.\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv = 10, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d60b51-53c8-4a4d-ad16-749f8e03e702",
   "metadata": {},
   "source": [
    "That's right, it has over 90% accuracy! This is simply because only about 10% of the images are 5s, so if you always guess that an image is not a 5, you will be right about 90% of the time.  This demonstrates why accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with *skewed datasets* (i.e., when some classes are much more frequent than others).\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "A much better way to evaluate the performance of a classifier is to look at the *confusion matrix*. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row & 3rd column of the confusion matrix. \n",
    "\n",
    "To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let's keep it untouched for now (remember tht you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the `cross_val_predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119fca26-47a8-4426-9b0e-9e83ed40b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Generate cross-validated predictions of our training set.\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 10)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eadb80c-d690-4349-ac9f-f12194f7b4e3",
   "metadata": {},
   "source": [
    "Just like the `cross_val_score()` function, `cross_val_predict()` performs K-fold cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means that you get a clean prediction for each instance in the training set (\"clean\" meaning that the prediction is made by a model that never saw the data during training).\n",
    "\n",
    "Now you are ready to get the confusion matrix using the `confusion_matrix()` function. Just pass it the target classes (`y_train_5`) & the predicted classes (`y_train_pred`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58753726-cf3e-40c4-8966-322e524f56ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Creates a confusion matrix for the training set.\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139eded5-5524-4ff3-bbca-d14e00845882",
   "metadata": {},
   "source": [
    "Each row in a confusion matrix represents an *actual class*, while each column represents a *predicted class*. The first row of this matrix considers non 5 images (the *negative class*): 53,763 of them were correctly classified as non 5s (they are called *true negatives*), while the remaining 816 were wrongly classified as 5s (*false positives*). The second row considers the images of 5s (the *positive class*): 1,372 were wrongly classified as non 5s (*false negatives*), while the remaining 4,049 were correctly classified as 5s (*true positives*). A perfect classifier would have only true positives & true negatives, so its confusion matrix would have nonzero values only on its main diagonal (top left & bottom right).\n",
    "\n",
    "The confusion matrix gives you a lot of information, but sometimes, you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive predictions; this is called the *precision* of the classifier.\n",
    "\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "TP is the number of true positives, & FP is the number of false positives. A trivial way to have perfect precision is to make one single positive prediction & ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one positive instance. So precision is typically used along with another metric named *recall*, also called *sensitivity* or *true positive rate* (TPR): this is the ratio of positive instances that are correctly detected by the classifier.\n",
    "\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "FN is the number of false negatives. \n",
    "\n",
    "If you are confused about confusion matrices, the below figure may help.\n",
    "\n",
    "<img src = \"Images/Confusion Matrix.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "## Precision & Recall\n",
    "\n",
    "Scikit-Learn provides several functions to compute classifier metrics, including precision & recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a109d5-3c9a-4375-b83c-c9ec431d051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Calculate precision & recall scores.\n",
    "ps = precision_score(y_train_5, y_train_pred)\n",
    "rs = recall_score(y_train_5, y_train_pred)\n",
    "print(\"Precision Score: {}, Recall Score: {}\".format(ps, rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c31c37-6672-452a-bfda-7ececacc2a51",
   "metadata": {},
   "source": [
    "Now, our 5-detector does not look as good as it did when we looked at its accuracy. When it claims an image represents a 5, it is correct only 74.7% of the time. Moreover, it only detects 83.3% of the 5s.\n",
    "\n",
    "It is often convenient to combine precision & recall into a single metric called the $F_1$ *score*, in particular if you need a simple way to compare two classifiers. The $F_1$ score is the *harmonic mean* of precision & recall. Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high $F_1$ score if both recall & precision are high.\n",
    "\n",
    "$$F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = 2 * \\frac{precision * recall}{precision + recall} = \\frac{TP}{TP + \\frac{FN + FP}{2}}$$\n",
    "\n",
    "To compute the $F_1$ score, simply call the `f1_score()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee642ad8-e7d7-4b51-831e-6e7e6d0c802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score.\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c12a5-eb73-4132-b489-c4a0829c367f",
   "metadata": {},
   "source": [
    "The $F_1$ score favors classifiers that have similar precision & recall. This is not always what you want: in some context you mostly care about precision,& in other context you really care about recall. For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer aclassifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier that has a much higher recall but lets a few really bad videos show up in your product (in such cases,you may even want to add a human pipeline to check the classifier's video selection). On the other head, suppose you train a classifier to detect shoplifters on surveillance images: it is probably fine if your classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught).\n",
    "\n",
    "Unfortunately, you can't have it both ways: increasing precision reduces recall, & vice versa. This is called the *precision/recall tradeoff*.\n",
    "\n",
    "## Precision/Recall Tradeoff\n",
    "\n",
    "To understand this tradeoff, let's look at how the `SGDClassifier` makes it classification decision. For each instance, it computes a score based on a *decision function*, & if that score is greater than a threshold, it assigns the instance to the positive class, or else it assigns it to the negative class. The following figure shows a few digits positioned fromthe lowest score on the left to the highest score on the right. Suppose the *decision threshold* is positioned at the central arrow (between the two 5s): you will find 4 true positives (actual 5s), on the right of tha threshold, & one false positive (actually a 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you raise the threhold (move it to the arrow on the right), the false positive (the 6) becomes a true negative, thereby increasing precision (up to 100% in this case), but one true positive becomes a false negative, decreasing recall down to 50%. Conversely, lowering the threhold increases recall & reduces precision.\n",
    "\n",
    "<img src = \"Images/Precision Recall Tradeoff.png\" alt = \"Alternative text\" width = \"500\" style = \"margine:auto\"/>\n",
    "\n",
    "Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions. Instead of calling the classifier's `predict()` method, you can call its `decision_function()` method, which returns a score for each instance, & then makes predictions based on those scores using any threshold you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53b3cc-7c9c-4af6-805b-2d676437dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict confidence score for some_digit. The confidence score for any sample is proportional \n",
    "# to the signed distance of that sample to the hyperplane.\n",
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "print(\"y_scores: {}, y_some_digit_pred: {}\".format(y_scores, y_some_digit_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877afe9-a373-4cfd-a667-3942580c9545",
   "metadata": {},
   "source": [
    "The `SGDClassifier` uses a threhold equal to 0, so the previous code returns the same results as the `predict()` method (i.e., `True`). Let's raise the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a7e9d-a284-472c-8b4e-df495644000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 8000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264185b4-f3b7-4236-be39-f2a64777ee8a",
   "metadata": {},
   "source": [
    "This confirms that raising the threshold decreases recall. The image actually represents a 5, & the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 8,000.\n",
    "\n",
    "Now how do you decide which threshold to use? For this, you will first need to get the scores of all instances in the training set using the `cross_val_predict()` function again, but this time specifying that you want it to return decision scores instead of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3dbabc-724e-486d-b5c2-1a43944852ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the cross-validated confidence scores for each sample in the training set.\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 10, method = \"decision_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9471b493-e62f-4fbb-9f60-0f1c25d2aa45",
   "metadata": {},
   "source": [
    "Now with these scores, you can compute precision & recall for all possible thresholds using the `precision_recall_curve()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe462dd9-0882-4058-9a0c-35ad009b67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Generate precision-recall pairs for different probability thresholds.\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf25661-f1be-4678-9cf1-da9fdde53aa3",
   "metadata": {},
   "source": [
    "Finally, you can plot precision & recall as functions of the threshold value using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7ce9a-697b-4aa1-bfe0-09e5b267ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function.\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \n",
    "    # Plot the precision scores across thresholds with a blue dashed line, labeled \"Precision\".\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label = \"Precision\", linewidth = 2)\n",
    "    \n",
    "    # Plot the recall scores across thresholds with a green solid line, labeled \"Recall\".\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label = \"Recall\", linewidth = 2)\n",
    "    \n",
    "    # Fix the legend to the center right of the visualisation.\n",
    "    plt.legend(loc = \"center right\", fontsize = 14)\n",
    "    \n",
    "    # Label the x-axis \"Threshold\".\n",
    "    plt.xlabel(\"Threshold\", fontsize = 14)\n",
    "    \n",
    "    # Add a grid background.\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Limit the x-axis from -50000 to 50000 & the y-axis from 0 to 1.\n",
    "    plt.axis([-50000, 50000, 0, 1])\n",
    "    \n",
    "# Suppose we want our precision score to be >= 90%, get recall & threshold for precision = 90%.\n",
    "recall_90_precision = recalls[np.argmax(precisions >= 0.9)]\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.9)]\n",
    "\n",
    "# Call the newly defined function.\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "\n",
    "# Plot red dotted line from (threshold_90_precision, 0) to (threshold_90_precision, 0.9).\n",
    "plt.plot([threshold_90_precision, threshold_90_precision], [0, 0.9], \"r:\")\n",
    "\n",
    "# Plot red dotted line from (-50000, 0.9) to (threshold_90_precision, 0.9).\n",
    "plt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")\n",
    "\n",
    "# Plot red dotted line from (-50000, recall_90_precision) to (threshold_90_precision, recall_90_precision).\n",
    "plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")\n",
    "\n",
    "# Plot a point at (threshold_90_precision, 0.9).\n",
    "plt.plot([threshold_90_precision], [0.9], \"ro\")\n",
    "\n",
    "# Plot a red point at (threshold_90_precision, recall_90_precision).\n",
    "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582fb26-8e15-4fd2-947f-48699947b552",
   "metadata": {},
   "source": [
    "Another way to select a good precision/recall tradeoff is to plot precision directly against recall, as shown in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3948e-d3eb-44aa-8e08-acbc15d92198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function.\n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    \n",
    "    # Plot the precision scores against recall scores with a blue solid line.\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth = 2)\n",
    "    \n",
    "    # Label the x-axis \"Recall\".\n",
    "    plt.xlabel(\"Recall\", fontsize = 14)\n",
    "    \n",
    "    # Label the y-axis \"Precision\".\n",
    "    plt.ylabel(\"Precision\", fontsize = 14)\n",
    "    \n",
    "    # Limit both the x & y-axes to 0 & 1.\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    \n",
    "    # Add a grid background.\n",
    "    plt.grid(True)\n",
    "\n",
    "# Call the newly defined function.\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "\n",
    "# Plot a red dotted line from (recall_90_precision, 0) to (recall_90_precision, 0.9).\n",
    "plt.plot([recall_90_precision, recall_90_precision], [0, 0.9], \"r:\")\n",
    "\n",
    "# Plot a red dotted line from (0, 0.9) to (recall_90_precision, 0.9).\n",
    "plt.plot([0, recall_90_precision], [0.9, 0.9], \"r:\")\n",
    "\n",
    "# Plot a red point at (recall_90_precision, 0.9).\n",
    "plt.plot([recall_90_precision], [0.9], \"ro\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b731a-a826-4c2f-9a04-b189826fd574",
   "metadata": {},
   "source": [
    "You can see that precision starts to fall sharpy around 80% recall. You will probably want to select a precision/recall tradeoff just before that drop -- for example, at around 60% recall. But of course, the choice depends on your project.\n",
    "\n",
    "So let's suppose you decide to aim for 90% precision. You look up the first plot & find that you need to use a threshold of about 8,000. To be more precise you can search for the lowest threshold that gives you at least 90% precision (`np.argmax()` will give us the first index of the maximum value, which in this case means the first `True` value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553c6f8-8f9a-4afe-9f76-5d07b1cc0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1cc94a-d1b6-4e6e-9601-c961a4c721d1",
   "metadata": {},
   "source": [
    "To make predictions (on the training set for now), instead of calling the classifier's `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d186bac-cd56-4f6b-b260-1f503183d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns True if training set labels' confidence scores are greater than the 90% precision threshold.\n",
    "y_train_pred_90 = (y_scores >= threshold_90_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff2112-43c1-4fe2-b1c8-a208bfad3053",
   "metadata": {},
   "source": [
    "Let's check these predictions' precision & recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0c7cd-c700-42e2-8040-d8fe29d4b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the precision & recall scores.\n",
    "ps = precision_score(y_train_5, y_train_pred_90)\n",
    "rs = recall_score(y_train_5, y_train_pred_90)\n",
    "print(\"Precision Score: {}, Recall Score: {}\".format(ps, rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba631b-2792-4054-a2e3-3eea5a1c0085",
   "metadata": {},
   "source": [
    "Great, you now have a 90% precision classifier! As you can see, it is fairly easy to create a classifier with virtually any precision you want: just set a high enough threshold, & you're done. Hmm, not so fast. A high-precision classifier is not very useful if its recall is too low.\n",
    "\n",
    "## The ROC Curve\n",
    "\n",
    "The *receiver operating characteristic* (ROC) curve is another common tool used with binary classifiers. It is very similar to the prevision/recall curve, but instead of plotting precision versus recall, the ROC plots the *true positive rate* (another name for recall) against the *false positive rate*. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the *true negative rate*, which is the ratio of negative instances that are correctly classified as negative. The TNR is also called *specificity*. Hence, the ROC curve plots *sensitivity* (recall) versus 1 - *specificity*.\n",
    "\n",
    "To plot the ROC curve, you first need to compute the TPR & FPR for various threshold values, using the `roc_curve()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918df323-7256-4be0-989e-58550dfef3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Computes the true positive rate & false positive rate receiver operating characteristic.\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a755d1-ffab-40db-9a8b-5e9a6d99edf6",
   "metadata": {},
   "source": [
    "Then you can plot the FPR against the TPR using matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa6384-836c-42f2-82f7-0ad99d666179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function.\n",
    "def plot_roc_curve(fpr, tpr, label = None):\n",
    "    \n",
    "    # Plot true positive rate across false positive rates.\n",
    "    plt.plot(fpr, tpr, linewidth = 2, label = label)\n",
    "    \n",
    "    # Plot a black dashed line from (0, 0) to (1, 1).\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    \n",
    "    # Limit both x & y-axes to between 0 & 1.\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    \n",
    "    # Label the x-axis \"False Positive Rate (Fall-Out)\".\n",
    "    plt.xlabel(\"False Positive Rate (Fall-Out)\", fontsize = 14)\n",
    "    \n",
    "    # Label the y-axis \"True Positive Rate (Recall)\".\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\", fontsize = 14)\n",
    "    \n",
    "    # Add a grid background.\n",
    "    plt.grid(True)\n",
    "\n",
    "# Call the newly defined function.\n",
    "plot_roc_curve(fpr, tpr)\n",
    "\n",
    "# Find the false positive rate when the true positive rate (recall) is greater than recall_90_precision.\n",
    "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]\n",
    "\n",
    "# Plot a red dotted line from (fpr_90, 0) to (fpr_90, recall_90_precision).\n",
    "plt.plot([fpr_90, fpr_90], [0, recall_90_precision], \"r:\")\n",
    "\n",
    "# Plot a red dotted line from (0, recall_90_precision) to (fpr_90, recall_90_precision).\n",
    "plt.plot([0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")\n",
    "\n",
    "# Plot a red point at (fpr_90, recall_90_precision).\n",
    "plt.plot([fpr_90], [recall_90_precision], \"ro\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfb618-5cfd-4818-9ecd-07af450a6b75",
   "metadata": {},
   "source": [
    "Once again, there is a tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good clasisifer stays as far awya from thatline as possible (toward the top-left corner).\n",
    "\n",
    "One way to compare classifiers is to measure the *area under the curve* (AUC). A perfect classifier will have a *ROC AUC* equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ffad4-7e09-474d-965b-c21dc54e9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate the area under the ROC curve.\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caef409-f02e-4a8a-99a7-c3198825caf5",
   "metadata": {},
   "source": [
    "Let's train a `RandomForestClassifer` & compare its ROC & ROC AUC score to the `SGDClassifier`. First, you need to get scores for each instance in the training set. But due to the way it works, the `RandomForestClassifier` class does not have a `decision_function()` method. Instead it has a `predict_proba()` method. Scikit-Learn classifiers generally have on or the other. The `predict_proba()` method returns an array containing a row per instance & a column per class, each containing the probability that the given instance belongs to the given class (e.g., 70% chance that the image represents a 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636233e-6e38-419c-bd92-5519893654fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state = 42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv = 10, method = \"predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058a143-cb62-4835-b572-21e8bcfbcad0",
   "metadata": {},
   "source": [
    "But to plot a ROC curve, you need scores, not probabilities. A simple solution is to use the positive class's probability as the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51751e9a-bbff-4b6d-8f80-56e5a5158320",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1]\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696de521-f985-4dfe-b97c-ce666485e0ae",
   "metadata": {},
   "source": [
    "Now you are ready to plot the ROC curve. It is useful to plot the first ROC curve as well as how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e19af5-fe81-402a-9fe9-7ff84de6f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_for_forest = tpr_forest[np.argmax(fpr_forest >= fpr_90)]\n",
    "\n",
    "plt.plot(fpr, tpr, \"b:\", linewidth = 2, label = \"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.plot([fpr_90, fpr_90], [0, recall_90_precision], \"r:\")\n",
    "plt.plot([0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")\n",
    "plt.plot([fpr_90], [recall_90_precision], \"ro\")\n",
    "plt.plot([fpr_90, fpr_90], [0, recall_for_forest], \"r:\")\n",
    "plt.plot([fpr_90], [recall_for_forest], \"ro\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc = \"lower right\", fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152e479-6b7b-4e58-8e0d-cb7ada781abb",
   "metadata": {},
   "source": [
    "As you can see, the `RandomForestClassifier's` ROC curve looks much better than the `SGDClassifier`'s: it comes much closer tot he top-left corner. As a result its ROC AUC score is also significantly better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15d49d-31cc-48d5-9f4f-b56be20e8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e08e4-52ac-4a7d-9de5-71b0534d377c",
   "metadata": {},
   "source": [
    "Try measuring the precision & recall scores: you should find 99% precision & ~ 87% recall. Not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5215e-1f78-4a73-a61d-7a117049eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv = 10)\n",
    "ps = precision_score(y_train_5, y_train_pred_forest)\n",
    "rs = recall_score(y_train_5, y_train_pred_forest)\n",
    "print(\"Precision Score: {}, Recall Score {}\".format(ps, rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbcf9e-a7a9-4957-84fa-d058be1ee9c9",
   "metadata": {},
   "source": [
    "Hopefully you now know how to train binary classifiers, choose the appropriate metric for your task, evaluate your classifiers using cross-validation, select the precision/recall tradeoff that fits your needs, & compare various models using ROC curves & ROC AUC scores. Now let's try to detect more than just the 5s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6012b-e649-4bf0-bbd1-7a2cec7c136e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02721e39-0912-491f-831b-9b57961e1156",
   "metadata": {},
   "source": [
    "# Multiclass Classification\n",
    "\n",
    "Whereas binary classifiers distinguish between two classes, *multiclass classifiers* (aka. *multinomial classifiers*) can distinguish between more than two class.\n",
    "\n",
    "Some algorithms (such as random forest classifiers or naive bayes classifiers) are capable of handling multiple classes directly. Others (such as support vector machine classifiers or linear classifiers) are strictly binary classifiers. However, there are various strategies that you can use to perform multiclass classification using multiple binary classifiers.\n",
    "\n",
    "For example, one way to create a system that can classify the digit images into 10 class (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, & so on). Then when you want to classify an image, you get the decision score from each classifier for that image & you select the class whose classifier outputs the highest score. This is called the *one-versus-all* (OvA) strategy.\n",
    "\n",
    "Another strategy is to train a binary classifier for every pair of digits: one to distinguish 0s & 1s another to distinguish 0s & 2s, another for 1s & 2s, & so on. This is called the *one-versus-one* (OvO) strategy. If there are *N* classes, you need to train $\\frac{N(N - 1)}{2}$ classifiers. For the MNIST problem, this means training 45 binary classifiers! When you want to classify an image, you have to run the image through all 45 classifiers & see which class wins the most duels. The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.\n",
    "\n",
    "Some algorithms (such as support vector machine classifiers) scale poorly with the size of the training set, so for these algorithms, OvO is preferred since it is faster to train many classifiers on small training sets than training few classifiers on large training sets. For most binary classification algorithms, however, OvA is preferred.\n",
    "\n",
    "Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, & it automatically runs OvA (except for SVM classifiers for which it uses OvO). Let's try this with the `SGDClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6902ba-78bc-4c58-b7ae-4263d42e1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58906488-6f71-4056-99c8-3cd34e40fc9d",
   "metadata": {},
   "source": [
    "That was easy! We trained the `SGDClassifier` on the training set using the original target classes from 0 to 9 (`y_train`), instead of the 5-versus-all target classes (`y_train_5`). Then it makes a prediction (a correct one in this case). Under the hood, scikit-learn actually trained 10 binary classifiers, got their decision scores for the image, & selected the class with the highest score.\n",
    "\n",
    "To see that this is indeed the case, you can call the `decision_function()` method. Instead of returning just one score per instance, it now returns 10 scores, one per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3eeea4-b985-46a2-a8e1-dc5010aab7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "some_digit_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da69f2-4cca-4ac0-beb2-2c39e4cc6da7",
   "metadata": {},
   "source": [
    "The highest score is indeed the one corresponding to class 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edb6fa-99c1-42a9-9619-1dbcfd94b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(some_digit_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91047f0e-3573-4e18-8eaf-5875445f3249",
   "metadata": {},
   "source": [
    "If you want to force scikit-learn to use one-versus-one or one-versus-all, you can use the `OneVsOneClassifier` or `OneVsRestClassifier` classes. Simply create an instance & pass a binary classifier to its constructor. For example, this code creates a multiclass classifier usingthe OvO strategy, based on a `SGDClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899d66b-1c38-44d6-a55a-25f07b0c2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "ovo_clf = OneVsOneClassifier(SGDClassifier(random_state = 42))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "ovo_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6be61-2981-4c73-aa0d-dea869ac6aeb",
   "metadata": {},
   "source": [
    "Training a `RandomForestClassifier` is just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f14b18-f45e-4979-ad98-13415c96a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4983e-3679-4958-8c0b-0aa59696f50c",
   "metadata": {},
   "source": [
    "This time, scikit-learn did not have to run OvA or OvO because random forest classifiers can directly classify instances into multiple classes. You can call `predict_proba()` to get the list of probabilities that the classifier assigned to each instance for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be31bdf-2bf9-4cf6-85bd-e09381aae3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.predict_proba([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8de1b-0cef-4fe9-a38c-83401f5fe5e2",
   "metadata": {},
   "source": [
    "You can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th index in the array means that the model estimates a 90% probability that the image represents a 5. It also thinks that the image could instead be a 2, a 3, or a 9, respectively with 1%, 8%, & 1% probability.\n",
    "\n",
    "Now, of course, you want to evaluate these classifiers. As usual, you want to use cross-validation. Let's evaluate the `SGDClassifier's` accuracy using the `cross_val_score()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6044af-c004-4658-a77c-dfc0f8791894",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcross_val_score\u001b[49m(sgd_clf, X_train, y_train, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, scoring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv = 10, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e1722-d5e6-40f9-9d0f-4e7b8b03f601",
   "metadata": {},
   "source": [
    "It gets over 84% on all test folds. If you used a random classifier, you would get 10% accuracy, so this is not such a bad score, but you cando much better. For example, simply scaling the inputs increases accuracy above 89%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4f0d80-6533-4d4a-8ff9-9d0e595bd391",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m      4\u001b[0m X_train_sclaed \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64))\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcross_val_score\u001b[49m(sgd_clf, X_train_scaled, y_train, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, scoring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sclaed = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv = 10, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273872c-b9f7-4159-89ff-2babf31f0873",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71799e8d-8f98-4236-852d-dfe5a6ba2e19",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
