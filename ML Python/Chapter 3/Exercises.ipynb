{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338027b0-16ac-466a-ac48-419324a80eb3",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set. Hint: the `KNeighborsClassifier` works quite well for this task; you just need to find good hyperparameter values (try a grid search on the `weights` & `n_neighbors` hyperparameters).\n",
    "2. Write a function that can shift an MNIST image in any direction (left, right, up, down) by one pixel. Then, for each image in the training set, create four shifted copies (one per direction) & add them to the training set. Finally, train your best model on this expanded training set & measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing your training set is called *data augmentation* or *training set expansion*.\n",
    "3. Tackle the *Titanic* dataset.\n",
    "4. Build a spam classifier:\n",
    "   * Download examples of spam & ham from [Apache SpamAssassin's Public Datasets](https://spamassassin.apache.org/old/publiccorpus/).\n",
    "   * Unzip the datasets & familiarise yourself with the data format.\n",
    "   * Split the datasets into a training set & a test set.\n",
    "   * Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector indication the presence or absence of each possible word. For example, if all emails only ever contain four words, \"Hello\", \"how\", \"are\", \"you\", then the email \"Hello you Hello Hello you\" would be converted into a vector [1, 0, 0, 1] (meaning [\"Hello\" is present, \"how\" is absent, \"are\" is absent, \"you\" is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word.\n",
    "   * You may want to add hyperparameters to your preparation pipeline to control whether or not to strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with \"URL\", replace all numbers with \"NUMBER\", or even performing *stemming* (i.e, trim off word endings; there are python libraries available to do this).\n",
    "   * Try several classifiers & see if you can build a great spam classifier, with both high recall & high precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d11040-bb87-407a-a73b-d6ca0b1e32ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47d30a-3d46-468c-a78a-4380f12ab20e",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd30a25e-b55b-4065-b7b2-b47094d53ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version = 1, as_frame = False, parser = \"auto\")\n",
    "mnist.keys()\n",
    "X, y = mnist[\"data\"].astype(np.intc), mnist[\"target\"].astype(np.intc)\n",
    "\n",
    "strat_split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 32)\n",
    "for train_index, test_index in strat_split.split(X, y):\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1193d1-5a4a-473a-82f9-fe348465d5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 6, 'weights': 'distance'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "kNN = KNeighborsClassifier()\n",
    "param_search_space = [{\"n_neighbors\":[5, 6, 7], \"weights\":[\"uniform\", \"distance\"]}]\n",
    "grid_search = GridSearchCV(kNN, param_search_space, cv = 3,\n",
    "                           scoring = \"accuracy\", return_train_score = True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccdffe4-d331-40ea-8e0d-1754bafa2898",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720714285714286"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kNN_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, kNN_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3a2b0-41d4-4cc3-9303-cc9bf3ca8044",
   "metadata": {},
   "source": [
    "# 2. \n",
    "\n",
    "Shift image 1 pixel in four directions (left, right, up, down) for each image in the training set, run it through the function & add the four new images to the training set. Then get the model accuracy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b7e49d-8bab-4ae0-a476-d73302a5057f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming the \"outer edge\" pixels are alway intensity = 0...\n",
    "\n",
    "X_train_left = []\n",
    "X_train_right = []\n",
    "X_train_up = []\n",
    "X_train_down = []\n",
    "\n",
    "for instance in range(len(X_train)):\n",
    "    sample = X_train[instance].reshape(28, 28)\n",
    "    sample_left = sample.tolist().copy()\n",
    "    sample_right = sample.tolist().copy()\n",
    "    sample_up = sample.tolist().copy()\n",
    "    sample_down = sample.tolist().copy()\n",
    "\n",
    "    sample_up = sample_up[1:] + [[0] * len(sample)]\n",
    "    sample_down = [[0] * len(sample)] + sample_down[:-1]\n",
    "    for index in range(len(sample)):\n",
    "        sample_left[index] = sample_left[index][1:] + [0]\n",
    "        sample_right[index] = [0] + sample_right[index][:-1]\n",
    "\n",
    "    X_train_left.append(np.array(sample_left).reshape(1, 784)[0].tolist())\n",
    "    X_train_right.append(np.array(sample_right).reshape(1, 784)[0].tolist())\n",
    "    X_train_up.append(np.array(sample_up).reshape(1, 784)[0].tolist())\n",
    "    X_train_down.append(np.array(sample_down).reshape(1, 784)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa9182a-1503-4f24-8002-2119040cc644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_left = np.array(X_train_left)\n",
    "X_train_right = np.array(X_train_right)\n",
    "X_train_up = np.array(X_train_up)\n",
    "X_train_down = np.array(X_train_down)\n",
    "X_train_combined = np.concatenate((X_train, X_train_left, X_train_right, X_train_up, X_train_down))\n",
    "y_train_combined = np.tile(y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0bfb1f-5f87-462c-8be1-f2150d87b0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9788571428571429"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kNN = KNeighborsClassifier(**grid_search.best_params_)\n",
    "new_kNN.fit(X_train_combined, y_train_combined)\n",
    "expanded_pred = new_kNN.predict(X_test)\n",
    "accuracy_score(y_test, expanded_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cc923-8efc-4c3a-aa50-6e0ce6a1e629",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01389d66-a301-443f-8f0a-a9ef1839ffdd",
   "metadata": {},
   "source": [
    "# 3. \n",
    "Practice with Kaggle's Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a0632a6-f270-452a-8a42-285ae696c69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                               Name  \\\n",
       "0              1       3                            Braund, Mr. Owen Harris   \n",
       "1              2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2              3       3                             Heikkinen, Miss. Laina   \n",
       "3              4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4              5       3                           Allen, Mr. William Henry   \n",
       "..           ...     ...                                                ...   \n",
       "886          887       2                              Montvila, Rev. Juozas   \n",
       "887          888       1                       Graham, Miss. Margaret Edith   \n",
       "888          889       3           Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890       1                              Behr, Mr. Karl Howell   \n",
       "890          891       3                                Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0      male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1    female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2    female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3    female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4      male  35.0      0      0            373450   8.0500   NaN        S  \n",
       "..      ...   ...    ...    ...               ...      ...   ...      ...  \n",
       "886    male  27.0      0      0            211536  13.0000   NaN        S  \n",
       "887  female  19.0      0      0            112053  30.0000   B42        S  \n",
       "888  female   NaN      1      2        W./C. 6607  23.4500   NaN        S  \n",
       "889    male  26.0      0      0            111369  30.0000  C148        C  \n",
       "890    male  32.0      0      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[889 rows x 11 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"titanic/train.csv\")\n",
    "train = train[train[\"Embarked\"].notnull()]\n",
    "X_test = pd.read_csv(\"titanic/test.csv\")\n",
    "X_train = train.drop([\"Survived\"], axis = 1)\n",
    "y_train = train[\"Survived\"]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae9ca214-a859-4d77-accc-959e0ec1dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 889 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  889 non-null    int64  \n",
      " 1   Survived     889 non-null    int64  \n",
      " 2   Pclass       889 non-null    int64  \n",
      " 3   Name         889 non-null    object \n",
      " 4   Sex          889 non-null    object \n",
      " 5   Age          712 non-null    float64\n",
      " 6   SibSp        889 non-null    int64  \n",
      " 7   Parch        889 non-null    int64  \n",
      " 8   Ticket       889 non-null    object \n",
      " 9   Fare         889 non-null    float64\n",
      " 10  Cabin        202 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 90.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c6def-5ea4-463e-a906-e5cc2cbc4878",
   "metadata": {},
   "source": [
    "I'll do what I can.\n",
    "\n",
    "I will need a pipeline that:\n",
    "1. Removes features: `PassengerId`, `Cabin`\n",
    "2. Transform the `Ticket` feature, by reducing the feature to its ticket number (remove the letters & symbols), recode the \"LINE\" tickets, & convert the feature to a numeric value.\n",
    "3. Transform the `Name` feature into `Name_Length`, where we'll measure the length of the value for the feature. I'm aware that there are some samples where there are two names, but I believe this is related to the `SibSp` feature, which lists the number of siblings/spouses the passenger has on board with them. Also, `Name_Length` would be able to capture the longer \"double\" names. This will be a numeric value.\n",
    "4. New feature: `Fare_per_Pclass`. No missing values for both features.\n",
    "5. Numeric features: `Pclass`, `Name_Length`, `Age`, `SibSp`, `Parch`, `Ticket`, `Fare`, `Fare_per_Pclass`. Will need an imputer for these features. Then scaler.\n",
    "6. Categorical features: `Sex`, `Embarked`. Will need an encoder for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8672aaf1-8295-41da-9b3c-847be6ad2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Function\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DatasetPreparation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self\n",
    "    def fit(self, X, y = None):\n",
    "        ticket_removed_suffix = X[\"Ticket\"].str.split(\" \").str[-1]\n",
    "        X[\"Ticket\"] = pd.to_numeric(ticket_removed_suffix.replace(\"LINE\", \"0\"))\n",
    "        X[\"Name_Length\"] = X[\"Name\"].apply(lambda x: len(x))\n",
    "        X[\"Fare_per_Pclass\"] = X[\"Fare\"]/X[\"Pclass\"]\n",
    "        return self\n",
    "    def transform(self, X, y = None):\n",
    "        X = X.drop([\"Name\", \"Cabin\", \"PassengerId\"], axis = 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad6f01ab-79c8-494a-ae77-ff207442a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy = \"median\")),\n",
    "                         (\"scaler\", StandardScaler())])\n",
    "\n",
    "# Categorical Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([(\"encoder\", OneHotEncoder(handle_unknown = \"ignore\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f19b91d-e9fe-48f0-9d83-fb51a2db3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine prep function step with numeric & categorical pipelines.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Name_Length\", \"Fare_per_Pclass\"]\n",
    "cat_features = [\"Sex\", \"Embarked\"]\n",
    "\n",
    "type_pipeline = ColumnTransformer([(\"numeric\", num_pipeline, num_features), \n",
    "                                   (\"categoric\", cat_pipeline, cat_features)])\n",
    "new_pipeline = Pipeline([(\"prep\", DatasetPreparation()),\n",
    "                         (\"type\", type_pipeline)])\n",
    "X_train_copy = X_train.copy()\n",
    "new_X_train = new_pipeline.fit_transform(X_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb8eb6e5-817a-4a84-b38c-67c54ede0aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 6, 'n_estimators': 475}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "randomForests = RandomForestClassifier()\n",
    "param_search_space = [{\"n_estimators\": [400, 425, 450, 475, 500, 525, 550, 575, 600], \n",
    "                       \"max_features\":[2, 3, 4, 5, 6, 7, 8]}]\n",
    "grid_search = GridSearchCV(randomForests, param_search_space, cv = 5,\n",
    "                           scoring = \"accuracy\", return_train_score = True)\n",
    "grid_search.fit(new_X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "339f1f33-efd7-4cac-8daf-40d235eaa484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "class TopNFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, scores, n):\n",
    "        self.scores = scores\n",
    "        self.n = n\n",
    "    def fit(self, X, y = None):\n",
    "        self.top_n_features = np.sort(np.argpartition(self.scores, -n)[-n:])\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, list(self.top_n_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c787f224-a58a-48e3-b04c-9a2f768544fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = grid_search.best_params_[\"max_features\"] \n",
    "full_pipeline = Pipeline([(\"new\", new_pipeline),\n",
    "                          (\"feature\", TopNFeatures(feature_importances, n))])\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3e58a288-5577-496a-8ef5-3d6dff4e6338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.74157303 0.82022472 0.78651685 0.85393258 0.86516854 0.82022472\n",
      " 0.86516854 0.79775281 0.88764045 0.80681818]\n",
      "Mean Score:  0.8245020429009194\n",
      "Score (Std. Dev.):  0.04188683517579273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "randomForests = RandomForestClassifier(**grid_search.best_params_)\n",
    "scores = cross_val_score(randomForests, X_train_prepared, y_train,\n",
    "                         scoring = \"accuracy\", cv = 10)\n",
    "print(\"Scores: \", scores)\n",
    "print(\"Mean Score: \", scores.mean())\n",
    "print(\"Score (Std. Dev.): \", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0cfe728-38a9-48f6-8901-6d5a9eda6d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForests.fit(X_train_prepared, y_train)\n",
    "X_test_prepared = full_pipeline.fit_transform(X_test)\n",
    "randomForests.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2892e7-43fb-4406-bade-74b8fcae10de",
   "metadata": {},
   "source": [
    "Our accuracy is really not that great, & I currently don't know any other methods to improve the prediction accuracy. I'll end it here for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e418d-e61a-4126-be08-1274cd5e2f88",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926f8c2-4622-4dbf-a641-86b1b147108c",
   "metadata": {},
   "source": [
    "# 4.\n",
    "\n",
    "I saw a youtube video about an email spam classifier that uses Term Frequency Inverse Document Frequency (TFIDF) & the problem sounds a lot like it, so that's what I'll be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d38e08b0-9256-4501-b104-5d8e277b9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "ham_files = os.listdir(\"Spam Classifier Data/easy_ham\")\n",
    "spam_files = os.listdir(\"Spam Classifier Data/spam\")\n",
    "\n",
    "def load_files(spam = False, filename = \"ok man\"):\n",
    "    if spam: path = \"Spam Classifier Data/spam\"\n",
    "    else: path = \"Spam Classifier Data/easy_ham\"\n",
    "    with open(os.path.join(path, filename), \"rb\") as file:\n",
    "        return email.parser.BytesParser(policy = email.policy.default).parse(file)\n",
    "    \n",
    "ham_emails = [load_files(spam = False, filename = name) for name in ham_files]\n",
    "spam_emails = [load_files(spam = True, filename = name) for name in spam_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ab4f92f-8af9-43e1-b551-016a9bd5523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a message dated 9/24/2002 11:24:58 AM, jamesr@best.com writes:\n",
      "\n",
      ">This situation wouldn't have happened in the first place if California\n",
      ">didn't have economically insane regulations.  They created a regulatory\n",
      ">climate that facilitated this.  So yes, it is the product of\n",
      ">over-regulation.\n",
      ">\n",
      "\n",
      "Which is to say, if you reduce the argument to absurdity, that law causes \n",
      "crime. \n",
      "\n",
      "(Yes, I agree that badly written law can make life so frustrating that people \n",
      "have little choice but to subvery it if they want to get anything done. This \n",
      "is also true of corporate policies, and all other attempts to regulate \n",
      "conduct by rules. Rules just don't work well when situations are fluid or \n",
      "ambiguous. But I don't think that the misbehavior of energy companies in \n",
      "California can properly be called well-intentioned lawbreaking by parties who \n",
      "were trying to do the right thing but could do so only by falling afoul of \n",
      "some technicality.)\n",
      "\n",
      "If you want to get to root causes, we should probably go to the slaying of \n",
      "Abel by Cain. Perhaps we can figure out what went wrong then, and roll our \n",
      "learning forward through history and create a FoRKtopia.\n",
      "\n",
      "Nonpartisanly, which is to say casting stones on all houses, whether \n",
      "bicameral or unicameral, built on sand or on rock, to the left of them or to \n",
      "the right of them, of glass or brick or twig or straw, \n",
      "\n",
      "Tom\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[0].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2e01e3b-9ad8-41cb-8f73-052496347453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Homeowner,\n",
      " \n",
      "Interest Rates are at their lowest point in 40 years!\n",
      "\n",
      "We help you find the best rate for your situation by\n",
      "matching your needs with hundreds of lenders!\n",
      "\n",
      "Home Improvement, Refinance, Second Mortgage,\n",
      "Home Equity Loans, and More! Even with less than\n",
      "perfect credit!\n",
      "\n",
      "This service is 100% FREE to home owners and new\n",
      "home buyers without any obligation. \n",
      "\n",
      "Just fill out a quick, simple form and jump-start\n",
      "your future plans today!\n",
      "\n",
      "\n",
      "Visit http://61.145.116.186/user0201/index.asp?Afft=QM10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To unsubscribe, please visit:\n",
      "\n",
      "http://61.145.116.186/light/watch.asp\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[0].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77cd5ba5-9420-4936-90a2-8b0c2b4267f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<email.message.EmailMessage object at 0x7fd7504a2580>,\n",
       "       <email.message.EmailMessage object at 0x7fd74d82ed90>,\n",
       "       <email.message.EmailMessage object at 0x7fd74f304100>, ...,\n",
       "       <email.message.EmailMessage object at 0x7fd74fe324f0>,\n",
       "       <email.message.EmailMessage object at 0x7fd74f311280>,\n",
       "       <email.message.EmailMessage object at 0x7fd7505549a0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype = object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 18)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82ee989a-8bc7-4a04-9cc9-0ca462dd4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "from urlextract import URLExtract\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "urlextractor = URLExtract()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "class CleanEmails(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y = None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            std_text = email.get_content().strip().lower() \n",
    "            urls = list(set(urlextractor.find_urls(std_text)))\n",
    "            for url in urls:\n",
    "                std_text = std_text.replace(url, \" URL \")\n",
    "            std_text = re.sub(r\"\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?\", \" NUMBER \", std_text)\n",
    "            std_text = re.sub(r\"\\W+\", \" \", std_text, flags = re.M)\n",
    "            split_text = std_text.split()\n",
    "            stemmed_text = \" \".join([ps.stem(word) for word in split_text])\n",
    "            X_transformed.append(stemmed_text)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0a2d367-9ce7-4a58-9016-e4fc312d89dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character '\\ufffd' in position 910: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_15190/2263177357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbruh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCleanEmails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbruh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_15190/2321411894.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0memail\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mstd_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml2text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/email/message.py\u001b[0m in \u001b[0;36mas_string\u001b[0;34m(self, unixfrom, maxheaderlen, policy)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxheaderlen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0mmaxheaderlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_line_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munixfrom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxheaderlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/email/message.py\u001b[0m in \u001b[0;36mas_string\u001b[0;34m(self, unixfrom, maxheaderlen, policy)\u001b[0m\n\u001b[1;32m    156\u001b[0m                       \u001b[0mmaxheaderlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxheaderlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                       policy=policy)\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munixfrom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munixfrom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/email/generator.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(self, msg, unixfrom, linesep)\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mufrom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'From nobody '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mufrom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_gen_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/email/generator.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_munge_cte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moldfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/email/generator.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writeBody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/email/generator.py\u001b[0m in \u001b[0;36m_handle_text\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content-transfer-encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 self._munge_cte = (msg['content-transfer-encoding'],\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/email/message.py\u001b[0m in \u001b[0;36mset_payload\u001b[0;34m(self, payload, charset)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCharset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mcharset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_charset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'decode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_payload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogateescape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character '\\ufffd' in position 910: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "bruh = CleanEmails()\n",
    "bruh.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cd68a-e6b3-4c4f-bfe9-d562d7c16f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
