{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338027b0-16ac-466a-ac48-419324a80eb3",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set. Hint: the `KNeighborsClassifier` works quite well for this task; you just need to find good hyperparameter values (try a grid search on the `weights` & `n_neighbors` hyperparameters).\n",
    "2. Write a function that can shift an MNIST image in any direction (left, right, up, down) by one pixel. Then, for each image in the training set, create four shifted copies (one per direction) & add them to the training set. Finally, train your best model on this expanded training set & measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing your training set is called *data augmentation* or *training set expansion*.\n",
    "3. Tackle the *Titanic* dataset.\n",
    "4. Build a spam classifier:\n",
    "   * Download examples of spam & ham from [Apache SpamAssassin's Public Datasets](https://spamassassin.apache.org/old/publiccorpus/).\n",
    "   * Unzip the datasets & familiarise yourself with the data format.\n",
    "   * Split the datasets into a training set & a test set.\n",
    "   * Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector indication the presence or absence of each possible word. For example, if all emails only ever contain four words, \"Hello\", \"how\", \"are\", \"you\", then the email \"Hello you Hello Hello you\" would be converted into a vector [1, 0, 0, 1] (meaning [\"Hello\" is present, \"how\" is absent, \"are\" is absent, \"you\" is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word.\n",
    "   * You may want to add hyperparameters to your preparation pipeline to control whether or not to strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with \"URL\", replace all numbers with \"NUMBER\", or even performing *stemming* (i.e, trim off word endings; there are python libraries available to do this).\n",
    "   * Try several classifiers & see if you can build a great spam classifier, with both high recall & high precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d11040-bb87-407a-a73b-d6ca0b1e32ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47d30a-3d46-468c-a78a-4380f12ab20e",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd30a25e-b55b-4065-b7b2-b47094d53ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version = 1, as_frame = False, parser = \"auto\")\n",
    "mnist.keys()\n",
    "X, y = mnist[\"data\"].astype(np.intc), mnist[\"target\"].astype(np.intc)\n",
    "\n",
    "strat_split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 32)\n",
    "for train_index, test_index in strat_split.split(X, y):\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1193d1-5a4a-473a-82f9-fe348465d5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 6, 'weights': 'distance'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "kNN = KNeighborsClassifier()\n",
    "param_search_space = [{\"n_neighbors\":[5, 6, 7], \"weights\":[\"uniform\", \"distance\"]}]\n",
    "grid_search = GridSearchCV(kNN, param_search_space, cv = 3,\n",
    "                           scoring = \"accuracy\", return_train_score = True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccdffe4-d331-40ea-8e0d-1754bafa2898",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720714285714286"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kNN_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, kNN_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3a2b0-41d4-4cc3-9303-cc9bf3ca8044",
   "metadata": {},
   "source": [
    "# 2. \n",
    "\n",
    "Shift image 1 pixel in four directions (left, right, up, down) for each image in the training set, run it through the function & add the four new images to the training set. Then get the model accuracy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b7e49d-8bab-4ae0-a476-d73302a5057f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming the \"outer edge\" pixels are alway intensity = 0...\n",
    "\n",
    "X_train_left = []\n",
    "X_train_right = []\n",
    "X_train_up = []\n",
    "X_train_down = []\n",
    "\n",
    "for instance in range(len(X_train)):\n",
    "    sample = X_train[instance].reshape(28, 28)\n",
    "    sample_left = sample.tolist().copy()\n",
    "    sample_right = sample.tolist().copy()\n",
    "    sample_up = sample.tolist().copy()\n",
    "    sample_down = sample.tolist().copy()\n",
    "\n",
    "    sample_up = sample_up[1:] + [[0] * len(sample)]\n",
    "    sample_down = [[0] * len(sample)] + sample_down[:-1]\n",
    "    for index in range(len(sample)):\n",
    "        sample_left[index] = sample_left[index][1:] + [0]\n",
    "        sample_right[index] = [0] + sample_right[index][:-1]\n",
    "\n",
    "    X_train_left.append(np.array(sample_left).reshape(1, 784)[0].tolist())\n",
    "    X_train_right.append(np.array(sample_right).reshape(1, 784)[0].tolist())\n",
    "    X_train_up.append(np.array(sample_up).reshape(1, 784)[0].tolist())\n",
    "    X_train_down.append(np.array(sample_down).reshape(1, 784)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa9182a-1503-4f24-8002-2119040cc644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_left = np.array(X_train_left)\n",
    "X_train_right = np.array(X_train_right)\n",
    "X_train_up = np.array(X_train_up)\n",
    "X_train_down = np.array(X_train_down)\n",
    "X_train_combined = np.concatenate((X_train, X_train_left, X_train_right, X_train_up, X_train_down))\n",
    "y_train_combined = np.tile(y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0bfb1f-5f87-462c-8be1-f2150d87b0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9788571428571429"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kNN = KNeighborsClassifier(**grid_search.best_params_)\n",
    "new_kNN.fit(X_train_combined, y_train_combined)\n",
    "expanded_pred = new_kNN.predict(X_test)\n",
    "accuracy_score(y_test, expanded_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cc923-8efc-4c3a-aa50-6e0ce6a1e629",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01389d66-a301-443f-8f0a-a9ef1839ffdd",
   "metadata": {},
   "source": [
    "# 3. \n",
    "Practice with Kaggle's Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6a0632a6-f270-452a-8a42-285ae696c69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"titanic/train.csv\")\n",
    "titanic_test = pd.read_csv(\"titanic/test.csv\")\n",
    "titanic_survival = pd.read_csv(\"titanic/gender_submission.csv\")\n",
    "test = titanic_test.merge(titanic_survival, on = \"PassengerId\", how = \"left\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae9ca214-a859-4d77-accc-959e0ec1dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c6def-5ea4-463e-a906-e5cc2cbc4878",
   "metadata": {},
   "source": [
    "I'll do what I can.\n",
    "\n",
    "I will need a pipeline that:\n",
    "1. Removes features: `PassengerId`, `Cabin`\n",
    "2. Transform the `Ticket` feature, by reducing the feature to its ticket number (remove the letters & symbols), recode the \"LINE\" tickets, & convert the feature to a numeric value.\n",
    "3. Transform the `Name` feature into `Name_Length`, where we'll measure the length of the value for the feature. I'm aware that there are some samples where there are two names, but I believe this is related to the `SibSp` feature, which lists the number of siblings/spouses the passenger has on board with them. Also, `Name_Length` would be able to capture the longer \"double\" names. This will be a numeric value.\n",
    "4. New feature: `Fare_per_Pclass`. No missing values for both features.\n",
    "5. Numeric features: `Pclass`, `Name_Length`, `Age`, `SibSp`, `Parch`, `Ticket`, `Fare`, `Fare_per_Pclass`. Will need an imputer for these features. Then scaler.\n",
    "6. Categorical features: `Sex`, `Embarked`. Will need an encoder for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8672aaf1-8295-41da-9b3c-847be6ad2641",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 'Ticket')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), 'Ticket')' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_5727/3133285754.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTicketAndNameTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_5727/3133285754.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mticket_removed_suffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Ticket\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtranformed_ticket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_removed_suffix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LINE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mname_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3815\u001b[0m             \u001b[0;31m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3816\u001b[0m             \u001b[0;31m#  the TypeError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3817\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3818\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   6057\u001b[0m             \u001b[0;31m# if key is not a scalar, directly raise an error (the code below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6058\u001b[0m             \u001b[0;31m# would convert to numpy arrays and raise later any way) - GH29926\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6059\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6061\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 'Ticket')"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TicketAndNameTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self\n",
    "    def fit(self, X, y = None):\n",
    "        ticket_removed_suffix = X[:, \"Ticket\"].str.split(\" \").str[-1]\n",
    "        tranformed_ticket = pd.to_numeric(train_removed_suffix.replace(\"LINE\", \"0\"))\n",
    "        name_length = X[:, \"Name\"].apply(lambda x: len(x))\n",
    "        return self\n",
    "    def transform(self, X, y = None):\n",
    "        transformed_X = X.drop(\"Name\", axis = 1)\n",
    "        transformed_X = X.reset_index(drop = True).join(transformed_ticket)\n",
    "        transformed_X = X.reset_index(drop = True).join(name_length)\n",
    "        return transformed_X\n",
    "\n",
    "tant = TicketAndNameTransformer()\n",
    "tant.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a84e44f-1040-4392-9750-f3293ec33804",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Ticket\"] = train[\"Ticket\"].str.split(\" \").str[-1]\n",
    "train = train.drop(\"Cabin\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca6faf8d-570d-406b-8f2f-2245dfcfdd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "embarked = np.array(train[\"Embarked\"]).reshape(-1, 1)\n",
    "embarked_encoded = encoder.fit_transform(embarked)\n",
    "embarked_encoded_DF = pd.DataFrame(embarked_encoded.toarray())\n",
    "embarked_encoded_DF.columns = [\"Embarked_C\", \"Embarked_Q\", \"Embarked_S\", \"Embarked_nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "844c9694-5ba8-4bbb-ae81-116eeb3c9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(\"Embarked\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c3bbff0-3302-402d-9309-bc46e6d7c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop = True).join(embarked_encoded_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85be4040-5c5f-4967-ba2b-1ddab2c9fd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   891 non-null    int64  \n",
      " 1   Survived      891 non-null    int64  \n",
      " 2   Pclass        891 non-null    int64  \n",
      " 3   Name          891 non-null    object \n",
      " 4   Sex           891 non-null    object \n",
      " 5   Age           714 non-null    float64\n",
      " 6   SibSp         891 non-null    int64  \n",
      " 7   Parch         891 non-null    int64  \n",
      " 8   Ticket        891 non-null    object \n",
      " 9   Fare          891 non-null    float64\n",
      " 10  Embarked_C    891 non-null    float64\n",
      " 11  Embarked_Q    891 non-null    float64\n",
      " 12  Embarked_S    891 non-null    float64\n",
      " 13  Embarked_nan  891 non-null    float64\n",
      "dtypes: float64(6), int64(5), object(3)\n",
      "memory usage: 97.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831dd357-1d81-4ba3-a58d-7ebe92baead8",
   "metadata": {},
   "source": [
    "Wth am I supposed to do with `Ticket == \"LINE\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1acfbee9-152d-4554-a332-2efbd4f52926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   891 non-null    int64  \n",
      " 1   Survived      891 non-null    int64  \n",
      " 2   Pclass        891 non-null    int64  \n",
      " 3   Name          891 non-null    object \n",
      " 4   Sex           891 non-null    object \n",
      " 5   Age           714 non-null    float64\n",
      " 6   SibSp         891 non-null    int64  \n",
      " 7   Parch         891 non-null    int64  \n",
      " 8   Ticket        887 non-null    float64\n",
      " 9   Fare          891 non-null    float64\n",
      " 10  Embarked_C    891 non-null    float64\n",
      " 11  Embarked_Q    891 non-null    float64\n",
      " 12  Embarked_S    891 non-null    float64\n",
      " 13  Embarked_nan  891 non-null    float64\n",
      "dtypes: float64(7), int64(5), object(2)\n",
      "memory usage: 97.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train[\"Ticket\"] = pd.to_numeric(train[\"Ticket\"].replace(\"LINE\", np.NaN))\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8874ed96-efd3-4d08-a48c-00ce13c4bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "needs_to_be_imputed = train.loc[:, [\"Age\", \"Ticket\"]]\n",
    "imputer = SimpleImputer(strategy = \"median\")\n",
    "imputer.fit(needs_to_be_imputed)\n",
    "imputed_age_ticket = pd.DataFrame(imputer.transform(needs_to_be_imputed))\n",
    "imputed_age_ticket.columns = [\"Age\", \"Ticket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17721ef1-1fc5-4131-b5be-055a217b67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop([\"Age\", \"Ticket\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e28baca7-f7a8-4362-85cd-7082f6b7fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop = True).join(imputed_age_ticket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d4e53f2-3efa-4c43-b615-6621ec065666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   891 non-null    int64  \n",
      " 1   Survived      891 non-null    int64  \n",
      " 2   Pclass        891 non-null    int64  \n",
      " 3   Name          891 non-null    object \n",
      " 4   Sex           891 non-null    object \n",
      " 5   SibSp         891 non-null    int64  \n",
      " 6   Parch         891 non-null    int64  \n",
      " 7   Fare          891 non-null    float64\n",
      " 8   Embarked_C    891 non-null    float64\n",
      " 9   Embarked_Q    891 non-null    float64\n",
      " 10  Embarked_S    891 non-null    float64\n",
      " 11  Embarked_nan  891 non-null    float64\n",
      " 12  Age           891 non-null    float64\n",
      " 13  Ticket        891 non-null    float64\n",
      "dtypes: float64(7), int64(5), object(2)\n",
      "memory usage: 97.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2aa439c2-1cfd-4c9f-a9d4-5fd35ee64d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = np.array(train[\"Sex\"]).reshape(-1, 1)\n",
    "sex_encoded = encoder.fit_transform(sex)\n",
    "sex_encoded_DF = pd.DataFrame(sex_encoded.toarray())\n",
    "sex_encoded_DF.columns = [\"Female\", \"Male\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d65ba36c-668a-4877-8234-ea9982b18fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(\"Sex\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "af9873f4-62a3-4e8a-b948-d50da0a9280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop = True).join(sex_encoded_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0c4563ea-af8f-43d0-a8df-5320a7f9083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Name_Length\"] = train[\"Name\"].apply(lambda x: len(x))\n",
    "train = train.drop(\"Name\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "badf3d33-d800-44a9-8d9c-c47c89818a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdScaler = StandardScaler()\n",
    "bruh = pd.DataFrame(stdScaler.fit_transform(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b135c-0b09-41d1-a819-b272682c395b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
