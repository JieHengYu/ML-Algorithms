{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338027b0-16ac-466a-ac48-419324a80eb3",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set. Hint: the `KNeighborsClassifier` works quite well for this task; you just need to find good hyperparameter values (try a grid search on the `weights` & `n_neighbors` hyperparameters).\n",
    "2. Write a function that can shift an MNIST image in any direction (left, right, up, down) by one pixel. Then, for each image in the training set, create four shifted copies (one per direction) & add them to the training set. Finally, train your best model on this expanded training set & measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing your training set is called *data augmentation* or *training set expansion*.\n",
    "3. Tackle the *Titanic* dataset.\n",
    "4. Build a spam classifier:\n",
    "   * Download examples of spam & ham from [Apache SpamAssassin's Public Datasets](https://spamassassin.apache.org/old/publiccorpus/).\n",
    "   * Unzip the datasets & familiarise yourself with the data format.\n",
    "   * Split the datasets into a training set & a test set.\n",
    "   * Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector indication the presence or absence of each possible word. For example, if all emails only ever contain four words, \"Hello\", \"how\", \"are\", \"you\", then the email \"Hello you Hello Hello you\" would be converted into a vector [1, 0, 0, 1] (meaning [\"Hello\" is present, \"how\" is absent, \"are\" is absent, \"you\" is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word.\n",
    "   * You may want to add hyperparameters to your preparation pipeline to control whether or not to strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with \"URL\", replace all numbers with \"NUMBER\", or even performing *stemming* (i.e, trim off word endings; there are python libraries available to do this).\n",
    "   * Try several classifiers & see if you can build a great spam classifier, with both high recall & high precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d11040-bb87-407a-a73b-d6ca0b1e32ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47d30a-3d46-468c-a78a-4380f12ab20e",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd30a25e-b55b-4065-b7b2-b47094d53ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version = 1, as_frame = False, parser = \"auto\")\n",
    "mnist.keys()\n",
    "X, y = mnist[\"data\"].astype(np.intc), mnist[\"target\"].astype(np.intc)\n",
    "\n",
    "strat_split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 32)\n",
    "for train_index, test_index in strat_split.split(X, y):\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1193d1-5a4a-473a-82f9-fe348465d5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 6, 'weights': 'distance'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "kNN = KNeighborsClassifier()\n",
    "param_search_space = [{\"n_neighbors\":[5, 6, 7], \"weights\":[\"uniform\", \"distance\"]}]\n",
    "grid_search = GridSearchCV(kNN, param_search_space, cv = 3,\n",
    "                           scoring = \"accuracy\", return_train_score = True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccdffe4-d331-40ea-8e0d-1754bafa2898",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720714285714286"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kNN_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, kNN_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3a2b0-41d4-4cc3-9303-cc9bf3ca8044",
   "metadata": {},
   "source": [
    "# 2. \n",
    "\n",
    "Shift image 1 pixel in four directions (left, right, up, down) for each image in the training set, run it through the function & add the four new images to the training set. Then get the model accuracy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b7e49d-8bab-4ae0-a476-d73302a5057f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming the \"outer edge\" pixels are alway intensity = 0...\n",
    "\n",
    "X_train_left = []\n",
    "X_train_right = []\n",
    "X_train_up = []\n",
    "X_train_down = []\n",
    "\n",
    "for instance in range(len(X_train)):\n",
    "    sample = X_train[instance].reshape(28, 28)\n",
    "    sample_left = sample.tolist().copy()\n",
    "    sample_right = sample.tolist().copy()\n",
    "    sample_up = sample.tolist().copy()\n",
    "    sample_down = sample.tolist().copy()\n",
    "\n",
    "    sample_up = sample_up[1:] + [[0] * len(sample)]\n",
    "    sample_down = [[0] * len(sample)] + sample_down[:-1]\n",
    "    for index in range(len(sample)):\n",
    "        sample_left[index] = sample_left[index][1:] + [0]\n",
    "        sample_right[index] = [0] + sample_right[index][:-1]\n",
    "\n",
    "    X_train_left.append(np.array(sample_left).reshape(1, 784)[0].tolist())\n",
    "    X_train_right.append(np.array(sample_right).reshape(1, 784)[0].tolist())\n",
    "    X_train_up.append(np.array(sample_up).reshape(1, 784)[0].tolist())\n",
    "    X_train_down.append(np.array(sample_down).reshape(1, 784)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa9182a-1503-4f24-8002-2119040cc644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_left = np.array(X_train_left)\n",
    "X_train_right = np.array(X_train_right)\n",
    "X_train_up = np.array(X_train_up)\n",
    "X_train_down = np.array(X_train_down)\n",
    "X_train_combined = np.concatenate((X_train, X_train_left, X_train_right, X_train_up, X_train_down))\n",
    "y_train_combined = np.tile(y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0bfb1f-5f87-462c-8be1-f2150d87b0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9788571428571429"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kNN = KNeighborsClassifier(**grid_search.best_params_)\n",
    "new_kNN.fit(X_train_combined, y_train_combined)\n",
    "expanded_pred = new_kNN.predict(X_test)\n",
    "accuracy_score(y_test, expanded_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cc923-8efc-4c3a-aa50-6e0ce6a1e629",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01389d66-a301-443f-8f0a-a9ef1839ffdd",
   "metadata": {},
   "source": [
    "# 3. \n",
    "Practice with Kaggle's Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a0632a6-f270-452a-8a42-285ae696c69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                               Name  \\\n",
       "0              1       3                            Braund, Mr. Owen Harris   \n",
       "1              2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2              3       3                             Heikkinen, Miss. Laina   \n",
       "3              4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4              5       3                           Allen, Mr. William Henry   \n",
       "..           ...     ...                                                ...   \n",
       "886          887       2                              Montvila, Rev. Juozas   \n",
       "887          888       1                       Graham, Miss. Margaret Edith   \n",
       "888          889       3           Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890       1                              Behr, Mr. Karl Howell   \n",
       "890          891       3                                Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0      male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1    female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2    female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3    female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4      male  35.0      0      0            373450   8.0500   NaN        S  \n",
       "..      ...   ...    ...    ...               ...      ...   ...      ...  \n",
       "886    male  27.0      0      0            211536  13.0000   NaN        S  \n",
       "887  female  19.0      0      0            112053  30.0000   B42        S  \n",
       "888  female   NaN      1      2        W./C. 6607  23.4500   NaN        S  \n",
       "889    male  26.0      0      0            111369  30.0000  C148        C  \n",
       "890    male  32.0      0      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[889 rows x 11 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"titanic/train.csv\")\n",
    "train = train[train[\"Embarked\"].notnull()]\n",
    "X_test = pd.read_csv(\"titanic/test.csv\")\n",
    "X_train = train.drop([\"Survived\"], axis = 1)\n",
    "y_train = train[\"Survived\"]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae9ca214-a859-4d77-accc-959e0ec1dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 889 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  889 non-null    int64  \n",
      " 1   Survived     889 non-null    int64  \n",
      " 2   Pclass       889 non-null    int64  \n",
      " 3   Name         889 non-null    object \n",
      " 4   Sex          889 non-null    object \n",
      " 5   Age          712 non-null    float64\n",
      " 6   SibSp        889 non-null    int64  \n",
      " 7   Parch        889 non-null    int64  \n",
      " 8   Ticket       889 non-null    object \n",
      " 9   Fare         889 non-null    float64\n",
      " 10  Cabin        202 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 90.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c6def-5ea4-463e-a906-e5cc2cbc4878",
   "metadata": {},
   "source": [
    "I'll do what I can.\n",
    "\n",
    "I will need a pipeline that:\n",
    "1. Removes features: `PassengerId`, `Cabin`\n",
    "2. Transform the `Ticket` feature, by reducing the feature to its ticket number (remove the letters & symbols), recode the \"LINE\" tickets, & convert the feature to a numeric value.\n",
    "3. Transform the `Name` feature into `Name_Length`, where we'll measure the length of the value for the feature. I'm aware that there are some samples where there are two names, but I believe this is related to the `SibSp` feature, which lists the number of siblings/spouses the passenger has on board with them. Also, `Name_Length` would be able to capture the longer \"double\" names. This will be a numeric value.\n",
    "4. New feature: `Fare_per_Pclass`. No missing values for both features.\n",
    "5. Numeric features: `Pclass`, `Name_Length`, `Age`, `SibSp`, `Parch`, `Ticket`, `Fare`, `Fare_per_Pclass`. Will need an imputer for these features. Then scaler.\n",
    "6. Categorical features: `Sex`, `Embarked`. Will need an encoder for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8672aaf1-8295-41da-9b3c-847be6ad2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Function\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DatasetPreparation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self\n",
    "    def fit(self, X, y = None):\n",
    "        ticket_removed_suffix = X[\"Ticket\"].str.split(\" \").str[-1]\n",
    "        X[\"Ticket\"] = pd.to_numeric(ticket_removed_suffix.replace(\"LINE\", \"0\"))\n",
    "        X[\"Name_Length\"] = X[\"Name\"].apply(lambda x: len(x))\n",
    "        X[\"Fare_per_Pclass\"] = X[\"Fare\"]/X[\"Pclass\"]\n",
    "        return self\n",
    "    def transform(self, X, y = None):\n",
    "        X = X.drop([\"Name\", \"Cabin\", \"PassengerId\"], axis = 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad6f01ab-79c8-494a-ae77-ff207442a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy = \"median\")),\n",
    "                         (\"scaler\", StandardScaler())])\n",
    "\n",
    "# Categorical Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([(\"encoder\", OneHotEncoder(handle_unknown = \"ignore\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f19b91d-e9fe-48f0-9d83-fb51a2db3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine prep function step with numeric & categorical pipelines.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Name_Length\", \"Fare_per_Pclass\"]\n",
    "cat_features = [\"Sex\", \"Embarked\"]\n",
    "\n",
    "type_pipeline = ColumnTransformer([(\"numeric\", num_pipeline, num_features), \n",
    "                                   (\"categoric\", cat_pipeline, cat_features)])\n",
    "new_pipeline = Pipeline([(\"prep\", DatasetPreparation()),\n",
    "                         (\"type\", type_pipeline)])\n",
    "X_train_copy = X_train.copy()\n",
    "new_X_train = new_pipeline.fit_transform(X_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb8eb6e5-817a-4a84-b38c-67c54ede0aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 6, 'n_estimators': 475}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "randomForests = RandomForestClassifier()\n",
    "param_search_space = [{\"n_estimators\": [400, 425, 450, 475, 500, 525, 550, 575, 600], \n",
    "                       \"max_features\":[2, 3, 4, 5, 6, 7, 8]}]\n",
    "grid_search = GridSearchCV(randomForests, param_search_space, cv = 5,\n",
    "                           scoring = \"accuracy\", return_train_score = True)\n",
    "grid_search.fit(new_X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "339f1f33-efd7-4cac-8daf-40d235eaa484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "class TopNFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, scores, n):\n",
    "        self.scores = scores\n",
    "        self.n = n\n",
    "    def fit(self, X, y = None):\n",
    "        self.top_n_features = np.sort(np.argpartition(self.scores, -n)[-n:])\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, list(self.top_n_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c787f224-a58a-48e3-b04c-9a2f768544fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = grid_search.best_params_[\"max_features\"] \n",
    "full_pipeline = Pipeline([(\"new\", new_pipeline),\n",
    "                          (\"feature\", TopNFeatures(feature_importances, n))])\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3e58a288-5577-496a-8ef5-3d6dff4e6338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.74157303 0.82022472 0.78651685 0.85393258 0.86516854 0.82022472\n",
      " 0.86516854 0.79775281 0.88764045 0.80681818]\n",
      "Mean Score:  0.8245020429009194\n",
      "Score (Std. Dev.):  0.04188683517579273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "randomForests = RandomForestClassifier(**grid_search.best_params_)\n",
    "scores = cross_val_score(randomForests, X_train_prepared, y_train,\n",
    "                         scoring = \"accuracy\", cv = 10)\n",
    "print(\"Scores: \", scores)\n",
    "print(\"Mean Score: \", scores.mean())\n",
    "print(\"Score (Std. Dev.): \", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0cfe728-38a9-48f6-8901-6d5a9eda6d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForests.fit(X_train_prepared, y_train)\n",
    "X_test_prepared = full_pipeline.fit_transform(X_test)\n",
    "randomForests.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2892e7-43fb-4406-bade-74b8fcae10de",
   "metadata": {},
   "source": [
    "Our accuracy is really not that great, & I currently don't know any other methods to improve the prediction accuracy. I'll end it here for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e418d-e61a-4126-be08-1274cd5e2f88",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926f8c2-4622-4dbf-a641-86b1b147108c",
   "metadata": {},
   "source": [
    "# 4.\n",
    "\n",
    "I saw a youtube video about an email spam classifier that uses Term Frequency Inverse Document Frequency (TFIDF) & the problem sounds a lot like it, so that's what I'll be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38e08b0-9256-4501-b104-5d8e277b9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "ham_files = os.listdir(\"Spam Classifier Data/easy_ham\")\n",
    "spam_files = os.listdir(\"Spam Classifier Data/spam\")\n",
    "\n",
    "def load_files(spam = False, filename = \"ok man\"):\n",
    "    if spam: path = \"Spam Classifier Data/spam\"\n",
    "    else: path = \"Spam Classifier Data/easy_ham\"\n",
    "    with open(os.path.join(path, filename), \"rb\") as file:\n",
    "        return email.parser.BytesParser().parse(file)\n",
    "    \n",
    "ham_emails = [load_files(spam = False, filename = name) for name in ham_files]\n",
    "spam_emails = [load_files(spam = True, filename = name) for name in spam_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab4f92f-8af9-43e1-b551-016a9bd5523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path: <fork-admin@xent.com>\n",
      "Delivered-To: yyyy@localhost.spamassassin.taint.org\n",
      "Received: from localhost (jalapeno [127.0.0.1])\n",
      "\tby jmason.org (Postfix) with ESMTP id 070DF16F03\n",
      "\tfor <jm@localhost>; Tue, 24 Sep 2002 17:55:30 +0100 (IST)\n",
      "Received: from jalapeno [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor jm@localhost (single-drop); Tue, 24 Sep 2002 17:55:30 +0100 (IST)\n",
      "Received: from xent.com ([64.161.22.236]) by dogma.slashnull.org\n",
      "    (8.11.6/8.11.6) with ESMTP id g8OGAEC11404 for <jm@jmason.org>;\n",
      "    Tue, 24 Sep 2002 17:10:14 +0100\n",
      "Received: from lair.xent.com (localhost [127.0.0.1]) by xent.com (Postfix)\n",
      "    with ESMTP id ACE072940DA; Tue, 24 Sep 2002 09:06:08 -0700 (PDT)\n",
      "Delivered-To: fork@spamassassin.taint.org\n",
      "Received: from imo-r09.mx.aol.com (imo-r09.mx.aol.com [152.163.225.105])\n",
      "    by xent.com (Postfix) with ESMTP id 522F329409A for <fork@xent.com>;\n",
      "    Tue, 24 Sep 2002 09:05:51 -0700 (PDT)\n",
      "Received: from ThosStew@aol.com by imo-r09.mx.aol.com (mail_out_v34.10.)\n",
      "    id 2.1a3.92f0d57 (4418) for <fork@xent.com>; Tue, 24 Sep 2002 12:09:26\n",
      "    -0400 (EDT)\n",
      "From: ThosStew@aol.com\n",
      "Message-Id: <1a3.92f0d57.2ac1e836@aol.com>\n",
      "Subject: Re: liberal defnitions\n",
      "To: fork@spamassassin.taint.org\n",
      "MIME-Version: 1.0\n",
      "Content-Type: text/plain; charset=\"US-ASCII\"\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-Mailer: AOL 5.0 for Mac sub 45\n",
      "Sender: fork-admin@xent.com\n",
      "Errors-To: fork-admin@xent.com\n",
      "X-Beenthere: fork@spamassassin.taint.org\n",
      "X-Mailman-Version: 2.0.11\n",
      "Precedence: bulk\n",
      "List-Help: <mailto:fork-request@xent.com?subject=help>\n",
      "List-Post: <mailto:fork@spamassassin.taint.org>\n",
      "List-Subscribe: <http://xent.com/mailman/listinfo/fork>, <mailto:fork-request@xent.com?subject=subscribe>\n",
      "List-Id: Friends of Rohit Khare <fork.xent.com>\n",
      "List-Unsubscribe: <http://xent.com/mailman/listinfo/fork>,\n",
      "    <mailto:fork-request@xent.com?subject=unsubscribe>\n",
      "List-Archive: <http://xent.com/pipermail/fork/>\n",
      "Date: Tue, 24 Sep 2002 12:09:26 EDT\n",
      "\n",
      "\n",
      "In a message dated 9/24/2002 11:24:58 AM, jamesr@best.com writes:\n",
      "\n",
      ">This situation wouldn't have happened in the first place if California\n",
      ">didn't have economically insane regulations.  They created a regulatory\n",
      ">climate that facilitated this.  So yes, it is the product of\n",
      ">over-regulation.\n",
      ">\n",
      "\n",
      "Which is to say, if you reduce the argument to absurdity, that law causes \n",
      "crime. \n",
      "\n",
      "(Yes, I agree that badly written law can make life so frustrating that people \n",
      "have little choice but to subvery it if they want to get anything done. This \n",
      "is also true of corporate policies, and all other attempts to regulate \n",
      "conduct by rules. Rules just don't work well when situations are fluid or \n",
      "ambiguous. But I don't think that the misbehavior of energy companies in \n",
      "California can properly be called well-intentioned lawbreaking by parties who \n",
      "were trying to do the right thing but could do so only by falling afoul of \n",
      "some technicality.)\n",
      "\n",
      "If you want to get to root causes, we should probably go to the slaying of \n",
      "Abel by Cain. Perhaps we can figure out what went wrong then, and roll our \n",
      "learning forward through history and create a FoRKtopia.\n",
      "\n",
      "Nonpartisanly, which is to say casting stones on all houses, whether \n",
      "bicameral or unicameral, built on sand or on rock, to the left of them or to \n",
      "the right of them, of glass or brick or twig or straw, \n",
      "\n",
      "Tom\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[0].as_string().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e01e3b-9ad8-41cb-8f73-052496347453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path: <pamela4701@eudoramail.com>\n",
      "Delivered-To: zzzz@localhost.spamassassin.taint.org\n",
      "Received: from localhost (jalapeno [127.0.0.1])\n",
      "\tby zzzzason.org (Postfix) with ESMTP id 5D14216F17\n",
      "\tfor <zzzz@localhost>; Mon,  9 Sep 2002 10:49:04 +0100 (IST)\n",
      "Received: from jalapeno [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor zzzz@localhost (single-drop); Mon, 09 Sep 2002 10:49:04 +0100 (IST)\n",
      "Received: from smtp-ft1.fr.colt.net (smtp-ft1.fr.colt.net [213.41.78.25])\n",
      "    by dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g899AfC06863 for\n",
      "    <webmaster@efi.ie>; Mon, 9 Sep 2002 10:10:41 +0100\n",
      "Received: from mailsweeper.abc-arbitrage.com (mailhost2.abc-arbitrage.com\n",
      "    [213.41.18.43]) by smtp-ft1.fr.colt.net with ESMTP id g899AvS20929 for\n",
      "    <webmaster@efi.ie>; Mon, 9 Sep 2002 11:10:57 +0200\n",
      "Received: from 210.214.94.76 (unverified) by mailsweeper.abc-arbitrage.com\n",
      "    (Content Technologies SMTPRS 4.2.10) with ESMTP id\n",
      "    <T5d3abf3ca1c0a8bf0537c@mailsweeper.abc-arbitrage.com>; Mon,\n",
      "    9 Sep 2002 11:06:09 +0200\n",
      "Message-Id: <00005cd5540a$00004a9b$00007fa8@mx1.eudoramail.com>\n",
      "To: <Undisclosed.Recipients@smtp-ft1.fr.colt.net>\n",
      "From: pamela4701@eudoramail.com\n",
      "Subject: Let us find the right mortgage lender for you      AFPE\n",
      "Date: Mon, 09 Sep 2002 14:36:18 -0700\n",
      "MIME-Version: 1.0\n",
      "Content-Type: text/plain; charset=\"Windows-1252\"\n",
      "Content-Transfer-Encoding: 7bit\n",
      "\n",
      "Dear Homeowner,\n",
      " \n",
      "Interest Rates are at their lowest point in 40 years!\n",
      "\n",
      "We help you find the best rate for your situation by\n",
      "matching your needs with hundreds of lenders!\n",
      "\n",
      "Home Improvement, Refinance, Second Mortgage,\n",
      "Home Equity Loans, and More! Even with less than\n",
      "perfect credit!\n",
      "\n",
      "This service is 100% FREE to home owners and new\n",
      "home buyers without any obligation. \n",
      "\n",
      "Just fill out a quick, simple form and jump-start\n",
      "your future plans today!\n",
      "\n",
      "\n",
      "Visit http://61.145.116.186/user0201/index.asp?Afft=QM10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To unsubscribe, please visit:\n",
      "\n",
      "http://61.145.116.186/light/watch.asp\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[0].as_string().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77cd5ba5-9420-4936-90a2-8b0c2b4267f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([<email.message.Message object at 0x7f94e9614700>,\n",
       "       <email.message.Message object at 0x7f94e8198ee0>,\n",
       "       <email.message.Message object at 0x7f94e861af40>, ...,\n",
       "       <email.message.Message object at 0x7f94e8e71130>,\n",
       "       <email.message.Message object at 0x7f94e872b550>,\n",
       "       <email.message.Message object at 0x7f94e974bb50>], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype = object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 18)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae1cd68a-e6b3-4c4f-bfe9-d562d7c16f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def HTMLToText(html):\n",
    "    text = re.sub(\"<head.*?>.*?</head>\", \"\", html, flags = re.M | re.S | re.I)\n",
    "    text = re.sub(\"<a\\s.*?>\", \" HYPERLINK \", text, flags = re.M | re.S | re.I)\n",
    "    text = re.sub(\"<.*?>\", \"\", text, flags = re.M | re.S)\n",
    "    text = re.sub(r\"(\\s*\\n)+\", \"\\n\", text, flags = re.M | re.S)\n",
    "    return unescape(text)\n",
    "\n",
    "def EmailToText(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        if not part.get_content_type() in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except:\n",
    "            content = str(part.get_payload())\n",
    "        if part.get_content_type() == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return HTMLToText(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f6a2f2-e945-4c6d-95f1-a11ee4c258bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import PorterStemmer\n",
    "from urlextract import URLExtract\n",
    "import pandas as pd\n",
    "\n",
    "ps = PorterStemmer()\n",
    "url_extractor = URLExtract()\n",
    "\n",
    "class CleanEmails(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y = None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = EmailToText(email) or \"\"\n",
    "            std_text = text.lower()\n",
    "            std_text = re.sub(\"_\", \"\", std_text)\n",
    "            urls = list(set(url_extractor.find_urls(std_text)))\n",
    "            for url in urls:\n",
    "                std_text = std_text.replace(url, \" URL \")\n",
    "            std_text = re.sub(r\"\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?\", \"NUMBER\", std_text)\n",
    "            std_text = re.sub(r\"\\W+\", \" \", std_text, flags = re.M)\n",
    "            split_text = std_text.split()\n",
    "            stemmed_text = \" \".join([ps.stem(word) for word in split_text])\n",
    "            X_transformed.append(stemmed_text)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff6a6fed-6185-4c21-afbf-75238a3e5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "feature_extraction = TfidfVectorizer(min_df = 1, stop_words = \"english\", lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f779d6-2df8-464d-91af-34143aa16d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2401x25591 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 195102 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "prepro_piplin = Pipeline([(\"clean_email\", CleanEmails()), \n",
    "                          (\"tfidf_vec\", feature_extraction)])\n",
    "X_train_features = prepro_piplin.fit_transform(X_train)\n",
    "X_test_features = prepro_piplin.transform(X_test)\n",
    "X_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17cad0e4-09c5-4f60-91a4-3b1ec151f0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 15000, 'max_iter': 1000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "param_search_space = [{\"C\":[15000, 20000, 25000], \"max_iter\":[1000, 1250, 1500]}]\n",
    "grid_search = GridSearchCV(logreg, param_search_space, cv = 3,\n",
    "                           scoring = \"accuracy\", return_train_score = True)\n",
    "grid_search.fit(X_train_features, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba3844f-2bfd-497e-a047-84a414d185ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.99170124 0.9875     0.9875     0.99166667 0.98333333 0.99583333\n",
      " 0.99166667 0.97916667 0.99166667 0.99166667]\n",
      "Mean Score: 0.9891701244813278\n",
      "Score StdDev: 0.004641677978858909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = LogisticRegression(**grid_search.best_params_)\n",
    "score = cross_val_score(log_reg, X_train_features, y_train, cv = 10, scoring = \"accuracy\")\n",
    "\n",
    "print(f\"Scores: {score}\")\n",
    "print(f\"Mean Score: {score.mean()}\")\n",
    "print(f\"Score StdDev: {score.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db4e2de-c692-40be-a9b7-4410e88573a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.99\n",
      "Recall: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "log_reg.fit(X_train_features, y_train)\n",
    "predictions = log_reg.predict(X_test_features)\n",
    "\n",
    "print(\"Precision: {}\".format(\"{:.2f}\".format(precision_score(y_test, predictions))))\n",
    "print(\"Recall: {}\".format(\"{:.2f}\".format(recall_score(y_test, predictions))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
