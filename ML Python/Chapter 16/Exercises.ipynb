{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4988f5b1-aa7b-4024-a02b-9cffbd4517e4",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. What are the pros & cons of using a stateful RNN versus a stateless RNN?\n",
    "2. Why do people use encoder-decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "3. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
    "4. What is beam search & why would you use it? What tool can you use to implement it?\n",
    "5. What is an attention mechanism? How does it help?\n",
    "6. What is the most important layer in the transformer architecture? What is its purpose?\n",
    "7. When would you need to use sampled softmax?\n",
    "8. *Embedded Reber grammars* were used by Hochreiter & Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as \"BPBTSXXVPSEPE\". Check out Jenny Orr's introduction to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr's page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect grammar, & 50% that don't.\n",
    "9. Train an encoder-decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\").\n",
    "10. Go through TensorFlow's neural machine translation with attention tutorial.\n",
    "11. Use one of the recent language models (e.g., BERT) to generate more convincing Shakespearean text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e291b7-04c7-470f-a15f-0fd57c9950d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698007f1-617f-4769-adc2-5cc7b09ba8c6",
   "metadata": {},
   "source": [
    "1. A stateless RNN will only capture short-term patterns, or at least patterns within the size of the windows the stateless RNN is trained on. Stateful RNNs can capture long-term patterns, but the preparation is more difficult. With stateful RNNs, you have to worry about the issue with consecutive batches -- they are not independent & equally distributed, which is not good for gradient descent.\n",
    "2. If you started translating as you read a sentence one word at a time, your resulting translated sentence may have a ton of grammatical errors or it would just not make sense at all. This is how a sequence-to-sequence RNN would translate your sentences. An encoder-decoder RNN will read the whole sentence before translating it, which leads to more accurate translations.\n",
    "3. For input sequences of different lengths, depending on how long the sequences are, you can bin the sequences based on their length & pad them so that all sequences in the bins are the same length. With all of this padding, you would make sure your model masks the padding tokens. Since, generall, the length of the output sequence is not known ahead of time, you would train the model to output an \\<eos> token at the end of each sequence. But if you did know the length of the output in advance, you would need to configure the loss function to ignore tokens after the \\<eos> token.\n",
    "4. Beam search is used to improve the performance of an encoder-decoder model. It keeps a short like of the *k* best output sequences & at each decoder step, it tries to extend the sequence by one word; then it keeps the *k* most likely sequences. *k* is beam width, & is a tunable hyperparameter.\n",
    "5. Attention mechanisms are used in encoder-decoder models to deal with longer input sequences. At each decoder time step, the current decoder's hidden state & the encoder's output is processed by the alignment model that outputs a score for each input time step. The score determines which part of the input is the most relevant (weighted) to the current decoder time step. This weighted sum of the encoder output (weighted by the alignment score) is fed to the decoder to produce the next decoder time step & the output for this time step. The benefit of doing all of this extra work is that it makes the encoder-decoder model able to process longer input sequences & it can potentially make the model easier to debug as well by pointing out which part of the input the model is paying attention to.\n",
    "6. The most important part of the transformer architecture is the multi-head attention layer. It allows models to identify words that are most aligned with each other (ex: smart & smartest, or grief, grieve, & grieving), & then improve each words representation in the output.\n",
    "7. You use sampled softmax when there are many classes (thousands). It approximates the cross-entropy loss based on the logit predicted from a sample of incorrect words, which speeds up training a ton, because it doesn't need to output a probability for each class if there are many classes, only a sample. But, after training, use a regular softmax function (not sampled softmax) to compute all the class probabilities (because you are trying to predict new words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0af58-fda7-4fde-8ba0-5209667aaa33",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2dc4d-881b-43e1-aeb9-bf138e4e8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "reber_grammar = [[(\"B\", 1)],\n",
    "                 [(\"T\", 2), (\"P\", 3)],\n",
    "                 [(\"S\", 2), (\"X\", 4)],\n",
    "                 [(\"T\", 3), (\"V\", 5)],\n",
    "                 [(\"X\", 3), (\"S\", 6)],\n",
    "                 [(\"P\", 4), (\"V\", 6)],\n",
    "                 [(\"E\", None)]]\n",
    "\n",
    "embedded_reber_grammar = [[(\"B\", 1)],\n",
    "                          [(\"T\", 2), (\"P\", 3)], \n",
    "                          [(reber_grammar, 4)],\n",
    "                          [(reber_grammar, 5)],\n",
    "                          [(\"T\", 6)],\n",
    "                          [(\"P\", 6)],\n",
    "                          [(\"E\", None)]]\n",
    "\n",
    "def generate_string(grammar):\n",
    "    node = 0\n",
    "    output = []\n",
    "    while node != None:\n",
    "        index = np.random.randint(len(grammar[node]))\n",
    "        production, node = grammar[node][index]\n",
    "        if isinstance(production, list):\n",
    "            production = generate_string(grammar = production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43e62a-4e86-4e8d-94e9-2c322f8a73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_string(reber_grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48cbae9-223e-49ca-bb15-d6fd1ba84a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bad_string(grammar, chars = \"BEPSTVX\"):\n",
    "    good_string = generate_string(grammar)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return good_string[:index] + bad_char + good_string[index + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c708e6-6bea-4f9e-89dd-2cdd06077d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_bad_string(embedded_reber_grammar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e23990-88dc-4d29-9aa6-2e7095f4ee70",
   "metadata": {},
   "source": [
    "We can't feed strings directly to an RNN, so we will encode them. We'll perform embedding. One-hot encoding works too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590ca3f-5b5d-4194-94b5-300cc4b9059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_ids(s, chars = \"BEPSTVX\"):\n",
    "    return [chars.index(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd181d3-8577-4a00-a3d7-70130c726f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_ids(\"BPBTXSEPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae2492-c726-497a-b955-e6e0db82d43c",
   "metadata": {},
   "source": [
    "We'll generate a dataset of half reber strings & half not reber strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d46e9-dc10-4771-a903-299721da1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def generate_dataset(size):\n",
    "    good_strings = [str_to_ids(generate_string(embedded_reber_grammar)) \n",
    "                               for _ in range(size // 2)]\n",
    "    bad_strings = [str_to_ids(generate_bad_string(embedded_reber_grammar))\n",
    "                              for _ in range(size // 2)]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank = 1)\n",
    "    y = np.array([[1.0] for _ in range(len(good_strings))] + \n",
    "                 [[0.0] for _ in range(len(bad_strings))])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f0620-d9fa-4b56-b9c5-d237d8e3e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_dataset(10000)\n",
    "X_val, y_val = generate_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf915d-1565-4e34-bdad-f5401755a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 5\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape = [None], dtype = tf.int32, ragged = True),\n",
    "    keras.layers.Embedding(input_dim = len(\"BEPSTVX\"), output_dim = embedding_size),\n",
    "    keras.layers.GRU(30),\n",
    "    keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate = 0.02, momentum = 0.95, nesterov = True)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs = 20, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede2548-64ab-4066-bb0f-ad3ce50d2253",
   "metadata": {},
   "source": [
    "# 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1efeab0a-c17f-4111-9120-878819aff4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 18:00:10.061534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(max_date - min_date, size = n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    x = [months[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edab3c31-e13a-4e4d-9dad-51845a673e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_chars = \"\".join(sorted(set(\"\".join(months) + \"0123456789, \")))\n",
    "input_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497158b3-f1c7-47ff-9443-78678f366b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_chars = \"0123456789-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbd35f2-b342-44dc-818a-e95abed77eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars = input_chars):\n",
    "    return [chars.index(c) for c in date_str]\n",
    "\n",
    "def prepare_date_strs(date_strs, chars = input_chars):\n",
    "    x_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    x = tf.ragged.constant(x_ids, ragged_rank = 1)\n",
    "    return (x + 1).to_tensor()\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, input_chars), prepare_date_strs(y, output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e6061e4-3f0b-4e32-b18d-b03a27ff3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset(10000)\n",
    "X_val, y_val = create_dataset(2000)\n",
    "X_test, y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd34ac98-69ff-4f0d-99e8-e79b40a84025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.3068 - loss: 1.9636 - val_accuracy: 0.6122 - val_loss: 1.1097\n",
      "Epoch 2/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.6060 - loss: 1.1239 - val_accuracy: 0.6851 - val_loss: 0.8552\n",
      "Epoch 3/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.6940 - loss: 0.8447 - val_accuracy: 0.7489 - val_loss: 0.6408\n",
      "Epoch 4/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.7741 - loss: 0.5816 - val_accuracy: 0.8396 - val_loss: 0.4253\n",
      "Epoch 5/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.8609 - loss: 0.3702 - val_accuracy: 0.9047 - val_loss: 0.2694\n",
      "Epoch 6/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9271 - loss: 0.2259 - val_accuracy: 0.9624 - val_loss: 0.1483\n",
      "Epoch 7/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.9709 - loss: 0.1232 - val_accuracy: 0.9835 - val_loss: 0.0798\n",
      "Epoch 8/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.9296 - loss: 0.2810 - val_accuracy: 0.9678 - val_loss: 0.1699\n",
      "Epoch 9/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.9821 - loss: 0.1199 - val_accuracy: 0.9925 - val_loss: 0.0585\n",
      "Epoch 10/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.9953 - loss: 0.0463 - val_accuracy: 0.9965 - val_loss: 0.0334\n",
      "Epoch 11/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9984 - loss: 0.0260 - val_accuracy: 0.9983 - val_loss: 0.0215\n",
      "Epoch 12/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0166 - val_accuracy: 0.9992 - val_loss: 0.0145\n",
      "Epoch 13/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9997 - loss: 0.0110 - val_accuracy: 0.9994 - val_loss: 0.0108\n",
      "Epoch 14/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.9998 - val_loss: 0.0077\n",
      "Epoch 15/15\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.9998 - val_loss: 0.0062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x152429be0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = y_train.shape[1]\n",
    "\n",
    "encoder = keras.models.Sequential([keras.layers.Embedding(input_dim = len(input_chars) + 1,\n",
    "                                                          output_dim = embedding_size,\n",
    "                                                          input_shape = [None]),\n",
    "                                   keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences = True),\n",
    "    keras.layers.Dense(len(output_chars) + 1, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([encoder,\n",
    "                                 keras.layers.RepeatVector(max_output_length),\n",
    "                                 decoder])\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"nadam\",\n",
    "              metrics = [\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs = 15, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab11a0d-f136-4acd-938f-e3917f47dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars = output_chars):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence]) for sequence in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44bf794c-cc9c-4b34-8c69-0baeb71c99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])\n",
    "\n",
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = np.argmax(model.predict(X), axis = -1)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6b22ce1-e32c-4325-a3fe-effd9f62a6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d55a6-c359-4fc4-9ab2-e81f86cee4e6",
   "metadata": {},
   "source": [
    "# 10.\n",
    "\n",
    "[\"NMT with Attention\"](https://www.tensorflow.org/text/tutorials/nmt_with_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c8d0d-26fb-4f71-9456-fc87dcd0eb0d",
   "metadata": {},
   "source": [
    "# 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "084a43ae-725b-48c4-aea1-5795bf8fe941",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.openai.modeling_tf_openai because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/transformers/activations_tf.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1764\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/transformers/models/openai/modeling_tf_openai.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFBaseModelOutput, TFCausalLMOutput, TFSequenceClassifierOutput\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/transformers/activations_tf.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFOpenAIGPTLMHeadModel\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m TFOpenAIGPTLMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-gpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1755\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1754\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1752\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1754\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1755\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1767\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1768\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1769\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.openai.modeling_tf_openai because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import TFOpenAIGPTLMHeadModel\n",
    "\n",
    "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94801525-d0c5-4a98-997c-fdfcf37dd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a1852-0aee-468c-aee0-53887793e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(input_ids = encoded_prompt,\n",
    "                                     do_sample = True,\n",
    "                                     max_length = length + len(encoded_prompt[0]),\n",
    "                                     temperature = 1.0,\n",
    "                                     top_k = 0,\n",
    "                                     top_p = 0.9,\n",
    "                                     repetition_penalty = 1.0,\n",
    "                                     num_return_sequences = num_sequences)\n",
    "generate_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b9a38-37d5-464c-a5c4-bebf5956cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces = True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
