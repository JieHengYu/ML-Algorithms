{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa412ae-8319-4748-8f11-23153ea1c7c3",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Is it OK to initialise all the weights to the same value as long as that value is selected randomly using He initialisation?\n",
    "2. Is it OK to initialise the bias terms to 0?\n",
    "3. Name three advantages of the SELU activation function over ReLU.\n",
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (& its variants), ReLU, tanh, logistic, & softmax?\n",
    "5. What may happen if you set the `momentum` hyperparameter too close to 1 (e.g., 0.99999) when using an `SGD` optimizer?\n",
    "6. Names three ways you can produce a sparse model.\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "   - Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialisation & the ELU activation function.\n",
    "   - Using Nadam optimisation & early stopping, training the network on the CIFAR10 dataset. You can load it with `keras.datasets.cifar10.load_data()`. The data is composed of 60,000 32 x 32-pixel colour images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.\n",
    "   - Now try adding batch normalisation & compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "   - Try replacing batch normalisation with SELU, & make the necessary adjustments to ensure the network self-normalises (i.e., standardise the input features, use LeCun normal initialisation, make sure the DNN contains only a sequence of dense layers, etc).\n",
    "   - Try regularising the model with alpha dropout. Then without restraining your model, see if you can achieve better accuracy using MC dropout.\n",
    "   - Retrain your model using 1cycle scheduling & see if it improves training speed & model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a1096-29ab-4bbe-9a09-525a4726025e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f1fcf-dc48-4ac4-b77b-62e0c2db9c7b",
   "metadata": {},
   "source": [
    "1. No. All weights should be initialised independently. \n",
    "2. Yes. It makes much of a difference if backpropagation will adjust the weight of the bias terms.\n",
    "3. SELU avoids vanishing gradients issue that affects ReLU units because it has nonzero derivatives for negative inputs. The mean is also closer to 0, which is better for self normalisation. Given a strict set of conditions: sequential model, LeCun initialisation, standardised inputs, SELU will self normalise, so it solves the exploding or vanishing gradients issues.\n",
    "4. SELU is the standard activation function for deep nets. Leaky ReLUs & its variants are great for their training speed & address the *dying ReLUs* problem. ReLU is often preferred for its simplicity. Hyperbolic tangent can be useful because it outputs values between -1 & 1, although it is mainly used for recurrent nets). Logistic & softmax are useful for estimating probabilities for classes; logistic for binary classification, softmax for multiple classification.\n",
    "5. You run the risk moving past the global minimum, before it goes in the reverse direction back toward the global minimum but past it again, & this will happen again & again until it eventually converges at the solution. In short, it will oscillate many times before it will make it to the optimum, making training time very long.\n",
    "6. You can zero out the weights after training, if they fall below a certain threshold. You can also apply $l_1$ regularisation. You can also use the TensorFlow Model Optimisation Toolkit.\n",
    "7. Dropout does slow down training, but does not slow down inference. MC Dropout also slows down training, but it also slows down inference as well because it averages an estimate over multiple predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706c09d-cb4a-4401-bbc0-e5709904b56e",
   "metadata": {},
   "source": [
    "# 8a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935bbac9-f0a6-4a4a-9502-f62f7a5949a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model1 = keras.models.Sequential()\n",
    "model1.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model1.add(keras.layers.Dense(100, activation = \"elu\", kernel_initializer = \"he_normal\"))\n",
    "model1.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c230b64-5355-4e20-99ec-c563b59bbca5",
   "metadata": {},
   "source": [
    "# 8b.\n",
    "\n",
    "After rerunning the below code with the following learning rates: 1e-5, 5e-5, 1e-4, 5e-4, & 1e-3; I found that 1e-4 resulted in the highest accuracy. There's gotta be a better way to do this. I don't want to have to run the code a million times just to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df05c31-3379-4b9f-855e-d9896d7484c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1407/1407 [==============================] - 17s 10ms/step - loss: 1.9158 - accuracy: 0.3002 - val_loss: 2.0326 - val_accuracy: 0.2936\n",
      "Epoch 2/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6999 - accuracy: 0.3862 - val_loss: 1.7681 - val_accuracy: 0.3618\n",
      "Epoch 3/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6130 - accuracy: 0.4221 - val_loss: 1.6260 - val_accuracy: 0.4022\n",
      "Epoch 4/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5526 - accuracy: 0.4438 - val_loss: 1.6008 - val_accuracy: 0.4242\n",
      "Epoch 5/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5110 - accuracy: 0.4568 - val_loss: 1.5809 - val_accuracy: 0.4406\n",
      "Epoch 6/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4751 - accuracy: 0.4716 - val_loss: 1.5183 - val_accuracy: 0.4560\n",
      "Epoch 7/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4426 - accuracy: 0.4840 - val_loss: 1.5251 - val_accuracy: 0.4540\n",
      "Epoch 8/30\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4144 - accuracy: 0.4938 - val_loss: 1.5465 - val_accuracy: 0.4498\n",
      "Epoch 9/30\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3878 - accuracy: 0.5030 - val_loss: 1.4463 - val_accuracy: 0.4876\n",
      "Epoch 10/30\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3620 - accuracy: 0.5141 - val_loss: 1.4881 - val_accuracy: 0.4834\n",
      "Epoch 11/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3373 - accuracy: 0.5218 - val_loss: 1.4679 - val_accuracy: 0.4734\n",
      "Epoch 12/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3171 - accuracy: 0.5301 - val_loss: 1.4572 - val_accuracy: 0.4924\n",
      "Epoch 13/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2971 - accuracy: 0.5370 - val_loss: 1.4782 - val_accuracy: 0.4786\n",
      "Epoch 14/30\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2765 - accuracy: 0.5429 - val_loss: 1.4759 - val_accuracy: 0.4842\n",
      "Epoch 15/30\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2541 - accuracy: 0.5491 - val_loss: 1.4895 - val_accuracy: 0.4822\n",
      "Epoch 16/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2343 - accuracy: 0.5564 - val_loss: 1.4671 - val_accuracy: 0.4978\n",
      "Epoch 17/30\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2147 - accuracy: 0.5646 - val_loss: 1.4261 - val_accuracy: 0.5006\n",
      "Epoch 18/30\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1978 - accuracy: 0.5708 - val_loss: 1.4861 - val_accuracy: 0.4902\n",
      "Epoch 19/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1797 - accuracy: 0.5774 - val_loss: 1.4391 - val_accuracy: 0.5004\n",
      "Epoch 20/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1596 - accuracy: 0.5837 - val_loss: 1.4372 - val_accuracy: 0.4998\n",
      "Epoch 21/30\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1434 - accuracy: 0.5888 - val_loss: 1.4691 - val_accuracy: 0.4934\n",
      "Epoch 22/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1282 - accuracy: 0.5968 - val_loss: 1.4663 - val_accuracy: 0.4932\n",
      "Epoch 23/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1071 - accuracy: 0.6023 - val_loss: 1.4510 - val_accuracy: 0.5056\n",
      "Epoch 24/30\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0893 - accuracy: 0.6091 - val_loss: 1.4991 - val_accuracy: 0.5010\n",
      "Epoch 25/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0778 - accuracy: 0.6110 - val_loss: 1.4977 - val_accuracy: 0.4882\n",
      "Epoch 26/30\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0570 - accuracy: 0.6194 - val_loss: 1.5054 - val_accuracy: 0.5012\n",
      "Epoch 27/30\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0444 - accuracy: 0.6229 - val_loss: 1.6562 - val_accuracy: 0.4642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd0dd26d60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nadam_optimisation = keras.optimizers.Nadam(learning_rate = 1e-4, beta_1 = 0.9, beta_2 = 0.999)\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"my_best_model.h5\", save_best_only = True)\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "\n",
    "((X_train, y_train), (X_test, y_test)) = keras.datasets.cifar10.load_data()\n",
    "X_val, X_train = X_train[:5000] / 255.0, X_train[5000:] / 255.0\n",
    "y_val, y_train = y_train[:5000], y_train[5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "model1.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = nadam_optimisation,\n",
    "              metrics = [\"accuracy\"])\n",
    "model1.fit(X_train, y_train, epochs = 30,\n",
    "           validation_data = (X_val, y_val),\n",
    "           callbacks = [checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa2b097-feeb-4215-b7b3-852a430a1941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4261 - accuracy: 0.5006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.426145076751709, 0.5005999803543091]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = keras.models.load_model(\"my_best_model.h5\")\n",
    "model1.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906bb07-5a2e-4d61-b041-292c1d1aceca",
   "metadata": {},
   "source": [
    "# 8c.\n",
    "\n",
    "After changing the architecture & rerunning the below code with the following learning rates: 5e-5, 1e-4, 5e-4, 1e-3; I found that the learning rate = 1e-4 performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83afe51d-3a63-4dce-86ec-272f47b8c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model2 = keras.models.Sequential()\n",
    "model2.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "model2.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model2.add(keras.layers.Dense(100, kernel_initializer = \"he_normal\"))\n",
    "    model2.add(keras.layers.BatchNormalization())\n",
    "    model2.add(keras.layers.Activation(\"elu\"))\n",
    "model2.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965bf6d0-49f9-444e-be59-bda546a70654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1407/1407 [==============================] - 33s 16ms/step - loss: 1.9623 - accuracy: 0.2986 - val_loss: 1.7037 - val_accuracy: 0.3946\n",
      "Epoch 2/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.7173 - accuracy: 0.3861 - val_loss: 1.5872 - val_accuracy: 0.4338\n",
      "Epoch 3/30\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.6404 - accuracy: 0.4136 - val_loss: 1.5242 - val_accuracy: 0.4574\n",
      "Epoch 4/30\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.5843 - accuracy: 0.4343 - val_loss: 1.5016 - val_accuracy: 0.4656\n",
      "Epoch 5/30\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.5440 - accuracy: 0.4481 - val_loss: 1.4744 - val_accuracy: 0.4792\n",
      "Epoch 6/30\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.5110 - accuracy: 0.4641 - val_loss: 1.4533 - val_accuracy: 0.4864\n",
      "Epoch 7/30\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.4763 - accuracy: 0.4741 - val_loss: 1.4368 - val_accuracy: 0.4984\n",
      "Epoch 8/30\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4469 - accuracy: 0.4873 - val_loss: 1.4193 - val_accuracy: 0.4916\n",
      "Epoch 9/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4202 - accuracy: 0.4973 - val_loss: 1.4054 - val_accuracy: 0.5062\n",
      "Epoch 10/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4026 - accuracy: 0.5013 - val_loss: 1.3976 - val_accuracy: 0.5004\n",
      "Epoch 11/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3812 - accuracy: 0.5123 - val_loss: 1.3939 - val_accuracy: 0.5060\n",
      "Epoch 12/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3595 - accuracy: 0.5153 - val_loss: 1.3717 - val_accuracy: 0.5146\n",
      "Epoch 13/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3439 - accuracy: 0.5234 - val_loss: 1.3839 - val_accuracy: 0.5108\n",
      "Epoch 14/30\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3286 - accuracy: 0.5280 - val_loss: 1.3666 - val_accuracy: 0.5192\n",
      "Epoch 15/30\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3092 - accuracy: 0.5360 - val_loss: 1.3668 - val_accuracy: 0.5238\n",
      "Epoch 16/30\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3045 - accuracy: 0.5366 - val_loss: 1.3792 - val_accuracy: 0.5242\n",
      "Epoch 17/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2901 - accuracy: 0.5417 - val_loss: 1.3530 - val_accuracy: 0.5334\n",
      "Epoch 18/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2744 - accuracy: 0.5492 - val_loss: 1.3594 - val_accuracy: 0.5294\n",
      "Epoch 19/30\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2582 - accuracy: 0.5540 - val_loss: 1.3447 - val_accuracy: 0.5274\n",
      "Epoch 20/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2450 - accuracy: 0.5582 - val_loss: 1.3505 - val_accuracy: 0.5260\n",
      "Epoch 21/30\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2364 - accuracy: 0.5611 - val_loss: 1.3596 - val_accuracy: 0.5294\n",
      "Epoch 22/30\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2196 - accuracy: 0.5714 - val_loss: 1.3462 - val_accuracy: 0.5278\n",
      "Epoch 23/30\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.2097 - accuracy: 0.5697 - val_loss: 1.3586 - val_accuracy: 0.5236\n",
      "Epoch 24/30\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.2019 - accuracy: 0.5718 - val_loss: 1.3734 - val_accuracy: 0.5242\n",
      "Epoch 25/30\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.1879 - accuracy: 0.5784 - val_loss: 1.3592 - val_accuracy: 0.5318\n",
      "Epoch 26/30\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1807 - accuracy: 0.5797 - val_loss: 1.3473 - val_accuracy: 0.5400\n",
      "Epoch 27/30\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1688 - accuracy: 0.5871 - val_loss: 1.3479 - val_accuracy: 0.5344\n",
      "Epoch 28/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1633 - accuracy: 0.5874 - val_loss: 1.3656 - val_accuracy: 0.5288\n",
      "Epoch 29/30\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1459 - accuracy: 0.5936 - val_loss: 1.3569 - val_accuracy: 0.5310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc832f0d90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nadam_optimisation = keras.optimizers.Nadam(learning_rate = 1e-4, beta_1 = 0.9, beta_2 = 0.999)\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"my_best_batch_normalised_model.h5\", save_best_only = True)\n",
    "\n",
    "model2.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = nadam_optimisation,\n",
    "              metrics = [\"accuracy\"])\n",
    "model2.fit(X_train, y_train, epochs = 30,\n",
    "           validation_data = (X_val, y_val),\n",
    "           callbacks = [checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c398f5b3-3943-42b8-8384-17465c26203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.3447 - accuracy: 0.5274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.344652771949768, 0.527400016784668]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = keras.models.load_model(\"my_best_batch_normalised_model.h5\")\n",
    "model2.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f82b2f-b621-4783-b699-5d7ef676b7dc",
   "metadata": {},
   "source": [
    "Convergence is definitely faster. It takes less epochs for the second model to reach the lowest validation loss in the first model & it continues to reach an even lower validation loss than the first model, reaching its lowest at its 24th epoch.\n",
    "\n",
    "The second model is more accurate than the first model; 52.74% vs. 50.06%, respectively.\n",
    "\n",
    "It took longer to train, & it's due to the addition of the extra batch normalisation calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a07b90-91f3-48db-a164-e00f3ecc1443",
   "metadata": {},
   "source": [
    "# 8d.\n",
    "\n",
    "After changing the architecture & rerunning the below code with the following learning rates: 5e-5, 1e-4, 5e-4, 1e-3; I found that the learning rate 5e-4 produced the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f72cad7-96b5-435b-8055-86ac57805d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model3 = keras.models.Sequential()\n",
    "model3.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model3.add(keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"))\n",
    "model3.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "nadam_optimisation = keras.optimizers.Nadam(learning_rate = 5e-4, beta_1 = 0.9, beta_2 = 0.999)\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"my_best_selu_model.h5\", save_best_only = True)\n",
    "\n",
    "# Need to standardise data for SELU\n",
    "X_mean = X_train.mean(axis = 0)\n",
    "X_std = X_train.std(axis = 0)\n",
    "X_train_scaled = (X_train - X_mean) / X_std\n",
    "X_val_scaled = (X_val - X_mean) / X_std\n",
    "X_test_scaled = (X_test - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c2d738-56a4-4924-ac6e-51c9f8ec2fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1407/1407 [==============================] - 16s 9ms/step - loss: 1.8886 - accuracy: 0.3256 - val_loss: 1.8767 - val_accuracy: 0.3250\n",
      "Epoch 2/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6874 - accuracy: 0.4030 - val_loss: 1.7037 - val_accuracy: 0.4076\n",
      "Epoch 3/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5878 - accuracy: 0.4389 - val_loss: 1.6332 - val_accuracy: 0.4208\n",
      "Epoch 4/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5166 - accuracy: 0.4686 - val_loss: 1.5636 - val_accuracy: 0.4576\n",
      "Epoch 5/30\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.4573 - accuracy: 0.4883 - val_loss: 1.5640 - val_accuracy: 0.4494\n",
      "Epoch 6/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4040 - accuracy: 0.5080 - val_loss: 1.5162 - val_accuracy: 0.4706\n",
      "Epoch 7/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3596 - accuracy: 0.5235 - val_loss: 1.4822 - val_accuracy: 0.4950\n",
      "Epoch 8/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3220 - accuracy: 0.5405 - val_loss: 1.5244 - val_accuracy: 0.4768\n",
      "Epoch 9/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2846 - accuracy: 0.5544 - val_loss: 1.4781 - val_accuracy: 0.4854\n",
      "Epoch 10/30\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2463 - accuracy: 0.5697 - val_loss: 1.5285 - val_accuracy: 0.4942\n",
      "Epoch 11/30\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2178 - accuracy: 0.5797 - val_loss: 1.4653 - val_accuracy: 0.5002\n",
      "Epoch 12/30\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1878 - accuracy: 0.5919 - val_loss: 1.5279 - val_accuracy: 0.4914\n",
      "Epoch 13/30\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1591 - accuracy: 0.6043 - val_loss: 1.4902 - val_accuracy: 0.4962\n",
      "Epoch 14/30\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1227 - accuracy: 0.6153 - val_loss: 1.4946 - val_accuracy: 0.5110\n",
      "Epoch 15/30\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.1028 - accuracy: 0.6223 - val_loss: 1.5408 - val_accuracy: 0.5108\n",
      "Epoch 16/30\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.0763 - accuracy: 0.6352 - val_loss: 1.4939 - val_accuracy: 0.5084\n",
      "Epoch 17/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0489 - accuracy: 0.6428 - val_loss: 1.5395 - val_accuracy: 0.5082\n",
      "Epoch 18/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0304 - accuracy: 0.6491 - val_loss: 1.5324 - val_accuracy: 0.5090\n",
      "Epoch 19/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0062 - accuracy: 0.6586 - val_loss: 1.5065 - val_accuracy: 0.5150\n",
      "Epoch 20/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9889 - accuracy: 0.6655 - val_loss: 1.5285 - val_accuracy: 0.5188\n",
      "Epoch 21/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9666 - accuracy: 0.6726 - val_loss: 1.6211 - val_accuracy: 0.5062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc54022250>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "               optimizer = nadam_optimisation,\n",
    "               metrics = [\"accuracy\"])\n",
    "model3.fit(X_train_scaled, y_train, epochs = 30,\n",
    "           validation_data = (X_val_scaled, y_val),\n",
    "           callbacks = [checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb52543-d93b-4a8b-ac6d-04969fd0c10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4653 - accuracy: 0.5002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4653328657150269, 0.5001999735832214]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = keras.models.load_model(\"my_best_selu_model.h5\")\n",
    "model3.evaluate(X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac060702-f741-408d-8058-ac9ddd082ca9",
   "metadata": {},
   "source": [
    "We get 50.0.2% accuracy, which is not very great; it performs similar to the original model. It also converges at a similar rate to the original model, but it definitely is the fastest to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f28ee-744a-44af-9aab-69eed36472ee",
   "metadata": {},
   "source": [
    "# 8e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30e4e0aa-0d24-46cb-a969-a23047bc2702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 1.8801 - accuracy: 0.3298 - val_loss: 1.7870 - val_accuracy: 0.3704\n",
      "Epoch 2/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6563 - accuracy: 0.4122 - val_loss: 1.7225 - val_accuracy: 0.3992\n",
      "Epoch 3/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5666 - accuracy: 0.4482 - val_loss: 1.6472 - val_accuracy: 0.4370\n",
      "Epoch 4/30\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.5069 - accuracy: 0.4708 - val_loss: 1.5703 - val_accuracy: 0.4596\n",
      "Epoch 5/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4470 - accuracy: 0.4942 - val_loss: 1.5639 - val_accuracy: 0.4470\n",
      "Epoch 6/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4000 - accuracy: 0.5080 - val_loss: 1.5913 - val_accuracy: 0.4656\n",
      "Epoch 7/30\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.3590 - accuracy: 0.5284 - val_loss: 1.5483 - val_accuracy: 0.4780\n",
      "Epoch 8/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3196 - accuracy: 0.5433 - val_loss: 1.5319 - val_accuracy: 0.4860\n",
      "Epoch 9/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2847 - accuracy: 0.5571 - val_loss: 1.5008 - val_accuracy: 0.4912\n",
      "Epoch 10/30\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2499 - accuracy: 0.5686 - val_loss: 1.5584 - val_accuracy: 0.4834\n",
      "Epoch 11/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2165 - accuracy: 0.5822 - val_loss: 1.6076 - val_accuracy: 0.4868\n",
      "Epoch 12/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1860 - accuracy: 0.5917 - val_loss: 1.5603 - val_accuracy: 0.4978\n",
      "Epoch 13/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1589 - accuracy: 0.6001 - val_loss: 1.5596 - val_accuracy: 0.4968\n",
      "Epoch 14/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1345 - accuracy: 0.6111 - val_loss: 1.6402 - val_accuracy: 0.5010\n",
      "Epoch 15/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1078 - accuracy: 0.6201 - val_loss: 1.5902 - val_accuracy: 0.4996\n",
      "Epoch 16/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0809 - accuracy: 0.6294 - val_loss: 1.6851 - val_accuracy: 0.4990\n",
      "Epoch 17/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0595 - accuracy: 0.6386 - val_loss: 1.5917 - val_accuracy: 0.5134\n",
      "Epoch 18/30\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0367 - accuracy: 0.6447 - val_loss: 1.6810 - val_accuracy: 0.5062\n",
      "Epoch 19/30\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0213 - accuracy: 0.6509 - val_loss: 1.6976 - val_accuracy: 0.5166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbbd200d940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model4 = keras.models.Sequential()\n",
    "model4.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model4.add(keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"))\n",
    "model4.add(keras.layers.AlphaDropout(rate = 0.1))\n",
    "model4.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "nadam_optimisation = keras.optimizers.Nadam(learning_rate = 5e-4, beta_1 = 0.9, beta_2 = 0.999)\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"my_best_selu_alpha_dropout_model.h5\", save_best_only = True)\n",
    "           \n",
    "model4.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "               optimizer = nadam_optimisation,\n",
    "               metrics = [\"accuracy\"])\n",
    "model4.fit(X_train_scaled, y_train, epochs = 30,\n",
    "           validation_data = (X_val_scaled, y_val),\n",
    "           callbacks = [checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18657e2f-f7f4-41ed-85a1-f659b1f4ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 2ms/step - loss: 1.5008 - accuracy: 0.4912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5007606744766235, 0.4912000000476837]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = keras.models.load_model(\"my_best_selu_alpha_dropout_model.h5\")\n",
    "model4.evaluate(X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c57f3f-28ba-430b-ad24-e63b0f1b8691",
   "metadata": {},
   "source": [
    "Ok, so that's alpha dropout. Now, let's try MC dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7479fab1-3c0d-4a0a-b2a7-49aa787bf0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training = True)\n",
    "\n",
    "mc_alpha_dropout_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer \n",
    "    for layer in model4.layers\n",
    "])\n",
    "\n",
    "def mc_alpha_dropout_predict_prob(model, X, n_samples = 10):\n",
    "    y_probs = [model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(y_probs, axis = 0)\n",
    "\n",
    "def mc_alpha_dropout_predict_classes(model, X, n_samples = 10):\n",
    "    y_probs = mc_alpha_dropout_predict_prob(model, X, n_samples)\n",
    "    return np.argmax(y_probs, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab35113-afd0-4b9f-977f-984e9ce5e2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "157/157 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4906"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "y_pred = mc_alpha_dropout_predict_classes(mc_alpha_dropout_model, X_val_scaled)\n",
    "np.mean(y_pred == y_val[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72aaf53-fb43-4abd-80b3-2545b7334708",
   "metadata": {},
   "source": [
    "Ok. So with alpha dropout, we get 49.12% accuracy. With MC dropout, we get 49.06% accuracy. Both are equally useless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2cd2d-0b1d-4500-ab83-8db3f4d1f1ef",
   "metadata": {},
   "source": [
    "# 8f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0e7ca24-6a0e-45e6-adc4-2ecc440b5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model5 = keras.models.Sequential()\n",
    "model5.add(keras.layers.Flatten(input_shape = [32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model5.add(keras.layers.Dense(100, activation = \"selu\", kernel_initializer = \"lecun_normal\"))\n",
    "model5.add(keras.layers.AlphaDropout(rate = 0.1))\n",
    "model5.add(keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"my_best_1cycle_model.h5\", save_best_only = True)\n",
    "\n",
    "model5.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "               optimizer = keras.optimizers.SGD(learning_rate = 1e-3),\n",
    "               metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec868339-644c-4953-b70b-568628a87112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "900/900 [==============================] - 6s 6ms/step - loss: 1.9826 - accuracy: 0.2984 - val_loss: 1.7364 - val_accuracy: 0.3902\n",
      "Epoch 2/20\n",
      "900/900 [==============================] - 5s 6ms/step - loss: 1.6777 - accuracy: 0.4042 - val_loss: 1.6006 - val_accuracy: 0.4222\n",
      "Epoch 3/20\n",
      "900/900 [==============================] - 5s 6ms/step - loss: 1.5726 - accuracy: 0.4418 - val_loss: 1.5918 - val_accuracy: 0.4440\n",
      "Epoch 4/20\n",
      "900/900 [==============================] - 5s 6ms/step - loss: 1.5149 - accuracy: 0.4606 - val_loss: 1.5198 - val_accuracy: 0.4596\n",
      "Epoch 5/20\n",
      "900/900 [==============================] - 5s 6ms/step - loss: 1.4730 - accuracy: 0.4782 - val_loss: 1.5253 - val_accuracy: 0.4736\n",
      "Epoch 6/20\n",
      "900/900 [==============================] - 5s 5ms/step - loss: 1.4397 - accuracy: 0.4917 - val_loss: 1.5285 - val_accuracy: 0.4766\n",
      "Epoch 7/20\n",
      "900/900 [==============================] - 5s 5ms/step - loss: 1.4149 - accuracy: 0.5011 - val_loss: 1.5505 - val_accuracy: 0.4644\n",
      "Epoch 8/20\n",
      "900/900 [==============================] - 5s 5ms/step - loss: 1.3917 - accuracy: 0.5116 - val_loss: 1.4811 - val_accuracy: 0.4980\n",
      "Epoch 9/20\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 1.3705 - accuracy: 0.5182 - val_loss: 1.5299 - val_accuracy: 0.4884\n",
      "Epoch 10/20\n",
      "900/900 [==============================] - 5s 5ms/step - loss: 1.3226 - accuracy: 0.5373 - val_loss: 1.5346 - val_accuracy: 0.5012\n",
      "Epoch 11/20\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 1.2548 - accuracy: 0.5589 - val_loss: 1.5327 - val_accuracy: 0.4950\n",
      "Epoch 12/20\n",
      "900/900 [==============================] - 4s 5ms/step - loss: 1.1833 - accuracy: 0.5827 - val_loss: 1.4924 - val_accuracy: 0.5122\n",
      "Epoch 13/20\n",
      "900/900 [==============================] - 5s 6ms/step - loss: 1.1203 - accuracy: 0.6069 - val_loss: 1.5278 - val_accuracy: 0.5190\n",
      "Epoch 14/20\n",
      "900/900 [==============================] - 5s 6ms/step - loss: 1.0527 - accuracy: 0.6297 - val_loss: 1.5100 - val_accuracy: 0.5220\n",
      "Epoch 15/20\n",
      "900/900 [==============================] - 5s 6ms/step - loss: 0.9863 - accuracy: 0.6534 - val_loss: 1.5583 - val_accuracy: 0.5252\n",
      "Epoch 16/20\n",
      "900/900 [==============================] - 6s 6ms/step - loss: 0.9133 - accuracy: 0.6774 - val_loss: 1.6026 - val_accuracy: 0.5254\n",
      "Epoch 17/20\n",
      "900/900 [==============================] - 5s 5ms/step - loss: 0.8351 - accuracy: 0.7062 - val_loss: 1.6254 - val_accuracy: 0.5372\n",
      "Epoch 18/20\n",
      "900/900 [==============================] - 5s 5ms/step - loss: 0.7562 - accuracy: 0.7338 - val_loss: 1.7229 - val_accuracy: 0.5300\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate = None,\n",
    "                 last_iterations = None, last_rate = None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        keras.backend.set_value(self.model.optimizer.learning_rate, rate)\n",
    "\n",
    "batch_size = 50\n",
    "n_epochs = 20\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate = 0.05)\n",
    "history = model5.fit(X_train_scaled, y_train, epochs = n_epochs, batch_size = batch_size,\n",
    "                     validation_data = (X_val_scaled, y_val),\n",
    "                     callbacks=[onecycle, checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3b4bb09-35f2-40fd-bd14-1da065b95526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4811 - accuracy: 0.4980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4811267852783203, 0.49799999594688416]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = keras.models.load_model(\"my_best_1cycle_model.h5\")\n",
    "model5.evaluate(X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde4ae7-3388-4832-a900-89ff4b5f560e",
   "metadata": {},
   "source": [
    "By far, the fastest to train, but still performance is similar to the original model, convergence is also similar to the original model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
