{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60dd61a1-0b2c-400e-b517-6db07dd2d2b8",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "\n",
    "So far, we have treated machine learning models & their training algorithms mostly like black boxes. If you went through some of previous lessons, you may have been surprised by how much you can get doen without knowing anything about what's under the hood: you optimised a regression system, you improved a digit image classifier, & you even built a spam classifier from scratch -- all this without knowing how they actually work. Indeed, in many situations you don't really need to know the implementation details.\n",
    "\n",
    "However, having a good understanding of how things work can help you quickly home in on an appropriate model, the right training algorithm to use, & a good set of hyperparameters for your task. Understanding what's under the hood will also help you debug issues & perform error analysis more efficiently. Lastly, most of the topics discussed in this lesson will be essential in understanding, building, & training neural networks.\n",
    "\n",
    "In this lesson, we will start by looking at the linear regression model, one of the simplest models there is. We will discuss two very different ways to train it.\n",
    "\n",
    "* Using a direct \"closed-form\" equation that directly computes the model parameters that best fit the model to the training set (i.e., the model parameters that minimise the cost function over the training set).\n",
    "* Using an iterative optimisation approach, called gradient descent (GD), that gradually tweaks the model parameters to minimise the cost function over the training set, eventually converging to the same set of parameters as the first method. We will look at a few variants of gradient descent that we will use again & again when we study neural networks in future lessons: batch GD, mini-batch GD, & stochastic GD.\n",
    "\n",
    "Next, we will look at polynomial regression, a more complex model that can fit nonlinear datasets. Since this model has more parameters than linear regression, it is more prone to overfitting the training data, so we will look at how to detect whether or not this is the case, using learning curves, & then we will look at several regularisation techniques that can reduce the risk of overfitting the training set.\n",
    "\n",
    "Finally, we will look at two more models that are commonly used for classification tasks: logistic regression & softmax regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae887b-5cc9-4102-81b8-616ded7224b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0cf712-e688-40ed-8f19-a1b8d224faa8",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In chapter 1, we looked at a simple regression model of life satisfaction: $life\\_satisfaction = \\theta_0 + \\theta_1 * GDP\\_per\\_capita$.\n",
    "\n",
    "This model is just a linear function of the input feature `GDP_per_capita`. $\\theta_0$ & $\\theta_1$ are the model's parameters.\n",
    "\n",
    "More generally, a linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the *bias term* (also called the *intercept term*).\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$$\n",
    "\n",
    "* $\\hat{y}$ is the predicted value.\n",
    "* $n$ is the number of features.\n",
    "* $x_i$ is the $i^{th}$ feature value.\n",
    "* $\\theta_j$ is the $j^{th}$ model parameter (including the bias term $\\theta_0$ & the feature weights $\\theta_1$, $\\theta_2$, ..., $\\theta_n$).\n",
    "\n",
    "This can be written much more concisely using a vectorised form.\n",
    "\n",
    "$$\\hat{y} = h_{\\theta}(x) = \\theta * x$$\n",
    "\n",
    "* $\\theta$ is the model's *parameter vector*, containing the bias term $\\theta_0$ & the feature weights $\\theta_1$ to $\\theta_n$.\n",
    "* $x$ is the instance's *feature vector*, containing $x_0$ to $x_n$, with $x_0$ always equal to 1.\n",
    "* $\\theta * x$ is the dot product of the vectors $\\theta$ & $x$, which is of course equal to $\\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$.\n",
    "* $h_{\\theta}$ is the hypothesis function, using the model paramters $\\theta$.\n",
    "\n",
    "Okay, that's the linear regression model, so now how do we train it? Well, recall that training a model means setting its parameters so that the model best fits the training set. For this purpose, we first need a measure of how well (or poorly) the model fits the training data. In chapter 2, we saw that the most common performance measure of a regression model is the root mean square error (RMSE). Therefore, to train a linear regression model, you need to find the value of $\\theta$ that minimises the RMSE. In practice, it is simpler to minimise the mean square error (MSE) & it leads to the same result (because the value that minimises a function also minimises it square root).\n",
    "\n",
    "The MSE of a linear regression hypothesis $h_{\\theta}$ on a training set X is calculated:\n",
    "\n",
    "$$MSE(X, h_{\\theta}) = \\frac{1}{m}\\sum_{i = 1}^{m}(\\theta^{T}x^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "Most of these notations were presented in chapter 2. The only difference is that we write $h_\\theta$ instead of just *h* in order to make it clear that the model is parametrised by the vector $\\theta$. To simplify notations, we will just write MSE($\\theta$) instead of MSE (X, $h_{\\theta}$).\n",
    "\n",
    "## The Normal Equation\n",
    "\n",
    "To find the value of $\\theta$ that minimises the cost function,there is a *closed form solution* -- in other words, a mathematical equation that gives the result directly. This is called the *normal equation*.\n",
    "\n",
    "$$\\hat{\\theta} = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "* $\\hat{\\theta}$ is the value of $\\theta$ that minimises the cost function.\n",
    "* $y$ is the vector of target values containing $y^{1}$ to $y^{m}$.\n",
    "\n",
    "Let's generate some linear-looking data to test this equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6bbe07f-52ca-4c4c-bdee-b93c0d552b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "plt.plot(X, y, \"bo\", markersize = 3)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8294417-95ec-4e47-a9cb-8d4b8d4e16b7",
   "metadata": {},
   "source": [
    "Now let's compute $\\hat{\\theta}$ using the normal equation. We will use the `inv()` function from numpy's linear algebra module (`np.linalg`) to compute the inverse of a matrix, & the `dot()` method for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cad072a9-bee4-4ac6-b2fb-afb1f98d07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55633-df85-4e32-93ba-61959d26b73e",
   "metadata": {},
   "source": [
    "The actual function we used to generate the data is $y = 4 + 3x + Gaussian\\ noise$. Let's see what the equation found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf22a937-6ab1-434a-ad85-089517f35eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.24584654],\n",
       "       [2.90228764]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc6bb71-f98e-4fb5-895c-20977944936e",
   "metadata": {},
   "source": [
    "We would have hoped for $\\theta_0$ = 4 & $\\theta_1$ = 3. Close enough though, but the noise made it impossible to recover the exact parameters of the original function.\n",
    "\n",
    "Now you can make predictions using $\\hat{\\theta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ba6088b-a536-44b9-9c61-eac331c3da5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.24584654],\n",
       "       [10.05042181]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495e3d8-1637-47b8-989f-812c1a5e598f",
   "metadata": {},
   "source": [
    "Let's plot this model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2728add1-8244-4dcf-a7a2-44b611c9162e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.legend([\"Predictions\"], loc = \"upper left\")\n",
    "plt.plot(X, y, \"bo\", markersize = 3)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848fcfd6-f0dd-467e-93fd-b5d7e4ece123",
   "metadata": {},
   "source": [
    "Performing linear regression using scikit-learn is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2de3848-8a5d-43fe-94af-615b31933b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.24584654]), array([[2.90228764]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5aa7259-7261-4cb1-bab9-c61caf33a4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.24584654],\n",
       "       [10.05042181]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274c5be-be0a-45d2-9b92-25229919b7d2",
   "metadata": {},
   "source": [
    "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (the name stands for \"least squares\"), which you can call directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93ab3cdd-ba0e-4c31-82cb-e26c1f522714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.24584654],\n",
       "       [2.90228764]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond = 1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efac86-8663-4a4f-bbeb-ea48aea6862d",
   "metadata": {},
   "source": [
    "This function computes $\\hat{\\theta} = X^{+}y$, where $X^{+}$ is the *pseudoinverse* of $X$ (specifically the Moore-Penrose inverse). You can use `np.linalg.pinv()` to compute the pseudoinverse directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6411849c-b5bc-47c2-983b-314f5953280c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.24584654],\n",
       "       [2.90228764]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad348d-70e1-4b54-91d7-837d68221c76",
   "metadata": {},
   "source": [
    "The pseudoinverse itself is computed using a standard matrix factorisation technique called *Singular Value Decomposition* (SVD) that can decompose the training set matrix $X$ into the matrix multiplication of three matrices $U$ $\\sum$ $V^{T}$. The pseudo inverse is computed as $X^{+} = V\\sum^{+}U^{T}$. To compute the matrix $\\sum^{+}$, the algorithm takes $\\sum$ & sets to zero all values smaller than a tiny threshold value, then it replaces all the non-zero values with their inverse,& finally it transposes the resulting matrix. This approach is more efficient than computing the normal equation, plus it handles edge cases nicely: indeed, the normal equation may not work if the matrix $X^{T}X$ is not invertible(i.e., singular), such as *m* < *n* or if some features are redundant, but the pseudoinverse is always defined.\n",
    "\n",
    "## Computational Complexity\n",
    "\n",
    "The normal equation computes the inverse of $X^{T}X$, which is an $(n + 1)(n + 1)$ matrix (where *n* is the number of features). The *computational complexity* of inverting such a matrix is typically about $O(n^{2.4})$ to $O(n^3)$ (depending on the implementation). In other words, if you double the number of features, you multiply the computation time by roughly $2^{2.4} = 5.3$ to $2^3 = 8$.\n",
    "\n",
    "The SVD approach used by scikit-learn's `LinearRegression` class is about $O(n^2)$. If you double the number of features, you multiply the computation time by roughly 4.\n",
    "\n",
    "Also, once you have trained your linear regression model (using the normal equation or any other algorithm), predictions are very fast: the computational complexity is linear with regards to both the number of instances you want to make predictions on & the number of features. In other words, making predictions on twice as many instances (or twice as many features) will just take roughly twice as much time.\n",
    "\n",
    "Now we will look at very different ways to train a linear regression model, better suited for cases where there are a large number of features, or too many training instances to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23407670-485b-4b94-af7f-4d2a6f6a2075",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86670a-d57c-485e-87d7-cb6bdf91f0b4",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "*Gradient Descent* is a very generic optimisation algorithm capable of finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimise a cost function.\n",
    "\n",
    "Supose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly is to go downhill in the direction of the steepest slope. This is exactly what gradient does; it measures the local gradient of the error function with regards to the parameter vector $\\theta$, & it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum!\n",
    "\n",
    "Concretely, you start bu filling $\\theta$ with random values (this is called *random initialisation), & then you improve it gradually, taking one, baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm *converges* to a minimum.\n",
    "\n",
    "<img src = \"Images/Gradient Descent.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "An important parameter in gradient descent is the size of the steps, determined by the *learning rate* hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time.\n",
    "\n",
    "<img src = \"Images/Learning Rate.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "On the other hand, if the learning rate is too high, you might jump across the valley & end up on the other side, possibly even higher up than before. This might make the algorithm diverge, with larger & larger values, failling to find a good solution.\n",
    "\n",
    "<img src = \"Images/Learning Rate 1.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Finally, not all cost functions look like nice regular bowls. There may be holes, ridges, plateaus, & all sorts of irregular terrains, making convergence to the minimum very difficult. The below figure shows the two main challenges with gradient descent: if he random initialisation starts the algorithm on the left, then it will converge to a *local minimum*, which is not a good as the *global minimum*. If it starts on the right, then it will tak a very. long time to cross the plateau, & if you stop too early you will never reach the global minimum.\n",
    "\n",
    "<img src = \"Images/Gradient Descent Pitfalls.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Fortunately, the MSE cost function for a linear regression model happens to be a *convex function*,which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. These two facts have great consequence: gradient descent is guaranteed to approach arbitrarily close to the global minimum (if you wait long enough & if the learning rate is not too high).\n",
    "\n",
    "In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scales. The below figure shows gradient descent on a training set where features 1 & 2 have the same scale (on the left), & on a training set where feature 1 has much smaller values than feature 2 (on the right).\n",
    "\n",
    "<img src = \"Images/Gradient Descent 1.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "As you can see, on the left, the gradient descent algorithm goes straight toward the minimum, thereby reaching it quickly, whereas on the right, it first goes in a direction almost orthogonal to the direction of the global minimum,& it ends with a long march down an almost flat valley. It will eventually reach the minimum, but it will take a long time.\n",
    "\n",
    "This diagram also iilustrates the fact that training a model means searching for a combination of model parameters that minimises a cost function (over the training set). It is a search in the mode's *parameter space*: the more parameters a model has, the more dimensions htis space has,& the harder the search is: searching for a needle in a 300-dimensional haystack is much trickier than in three dimensions. Fortunately, since the cost function is convex in the case of linear regression, the needle is simply at the bottom of the bowl.\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "To implement gradient descent, you need to compute the gradient of the cost function with regards to each model paramter $\\theta_j$. In other words, you need to claculate how much the cost function will change if you change $\\theta_j$ just a little bit. This is called a *partial derivative*. It is like asking \"what is the slope of the mountain under my feet if I face east?\" * then asking the same question facing north (& so on for the rest of the other dimensions, if you can imagine a universe with more than three dimensions). The below function computes the partial derivative of the cost function with regards to parameter $\\theta_j$, noted $\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta)$.\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta) = \\frac{2}{m} \\sum^{m}_{i = 1}(\\theta^{T}x^{(i)} - y^{(i)})x^{(i)}_j$$\n",
    "\n",
    "Instead of computing these partial derivatives individually, you can use the below function to compute them all in one go. The gradient vector, noted $\\triangledown_{\\theta}MSE(\\theta)$, contains all the partial derivatives of the cost function (one for each model parameter).\n",
    "\n",
    "$$\\triangledown_{\\theta}MSE(\\theta) = \\Bigg(\\begin{split} \n",
    "{\\frac{\\partial}{\\partial\\theta_0} MSE(\\theta)} \\\\\n",
    "{\\frac{\\partial}{\\partial\\theta_1} MSE(\\theta)} \\\\\n",
    "{...} \\\\\n",
    "{\\frac{\\partial}{\\partial\\theta_n}MSE(\\theta)} \n",
    "\\end{split}\\Bigg) = \\frac{2}{m}X^T(X\\theta - y)$$\n",
    "\n",
    "One you have the gradient vector, which points uphill, just go in the opposite direction to go downhill. This means subtracting $\\triangledown_{\\theta} MSE(\\theta)$ from $\\theta$. This is where the learning rate $\\eta$ comes into play: multiple the gradient vector by $\\eta$ to determine the size of the downhill step.\n",
    "\n",
    "$$\\theta^{(next\\ step)} = \\theta - \\eta\\triangledown_{\\theta}MSE(\\theta)$$\n",
    "\n",
    "Let's look at a quick implementation of this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1beae470-82b9-4124-93ac-55f27a6a1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5521f-4379-4153-8536-ab5cdaceca28",
   "metadata": {},
   "source": [
    "That wasn't too hard! Sure was hard to understand though. Let's look at the resulting `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddf4bf3e-e0f6-4cc4-9896-8372a99755ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.24584654],\n",
       "       [2.90228764]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73074b26-f970-4a98-9f6d-6a265d58a973",
   "metadata": {},
   "source": [
    "Hey! That's exactly what the normal equation found! Gradient descent worked perfectly. But what if we had used a different learning rate `eta`?. The below figure shows the first 10 steps of gradient descent using three different learning rates (the dashed line represents the starting point).\n",
    "\n",
    "<img src = \"Images/Gradient Descent with Different Learning Rates.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "On the left, the learning rate is too low: the algorithm will eventually reach the solution, but it will take a long time. In the middle, the learning rate looks pretty good: in just a few iterations, it has already converged to the solution. On the right, the learning rate is too high: the algorithm diverges, jumping all over the place & actually getting further & further away from the solution at every step.\n",
    "\n",
    "To find a good learning rate, you can use grid search. However, you may want to limit the number of iterations so that grid search can eliminate models that take too long to converge.\n",
    "\n",
    "You may wonder how to set the number of iterations. If it is too low, you will still be far away from the optimal solution when algorithm stops, but if it is too high, you will waste time while the model parameters do not change anymore. A simple solution is to set a very large number of iterations but to interrup the algorithm when the gradient vector becomes tiny -- that is, when its norm becomes smaller than a tiny number $\\epsilon$ (called the *tolerance*) -- because this happens when the gradient descent has (almost) reached the minimum).\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "The main problem with batch gradient descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large.At the opposite extreme, *stochastic gradient descent* just picks a random instance in the training set at every step & computes the gradients based only on that single instance. Obviously, this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration (SGD can be implemented as an out-of-core algorithm).\n",
    "\n",
    "One the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than batch gradient descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up & down, decreasing only on average. Over time, it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal.\n",
    "\n",
    "<img src = \"Images/Stochastic Gradient Descent.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "When the cost function is very irregular, this can actually help the algorithm jump out of local minima, so stochastic gradient descent has a chance of finding the global minimum than batch gradient descent does. \n",
    "\n",
    "Therefore randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress & escape local minima), then get smaller & smaller, allowing the algorithm to settle at the global minimum. This process is akin to *simulated annealing*, an algorithm inspired from the process of annealing in metallurgy where molten meta is slowly cooled down. The function that determines the learning rate at each iteration is called the *learning schedule*. If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. If the learning rate is reduced too slowly, you may jump around the minimum for a long time & end up with a suboptimal solution if you halt training too early.\n",
    "\n",
    "The following code implements stochastic gradient descent using a simple learning schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7896f1ff-f2f2-48fc-8a34-9e0368cfb8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "for epoch in range (n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index + 1]\n",
    "        yi = y[random_index:random_index + 1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01669a57-b421-464d-94e4-9e139d51e9c9",
   "metadata": {},
   "source": [
    "By convention, we iterate by rounds of *m* iterations; each round is called an *epoch*. While the batch gradient descent code iterated 1,000 times through the whole training set, this code goes through the training set only 50 times & reaches a fairly good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85ef9bcd-0ec2-4853-a38d-8881203a0741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.26899635],\n",
       "       [2.87439285]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d846a-6ae0-40e7-98a2-0f3999ce07be",
   "metadata": {},
   "source": [
    "The following figure shows the first 20 steps of traianing (notice how irregular the steps are).\n",
    "\n",
    "<img src = \"Images/Stochastic Gradient Descent 1.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Note that since instances are picked randomly, some instances may be picked severaltimes per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set (making sure to shuffle the input features & the labels jointly), then go through it instance by instance, then shuffle it again, & so on. However, this generally converges more slowly.\n",
    "\n",
    "To perform linear regression using SGD with scikit-learn, you can use the `SGDRegressor` class, which defaults to optimising the squared error cost function. The following code runs for maximum 1000 epochs (`max_iter = 1000`) or until the loss drops by less than 1e-3 during one epoch (`tol = 1e-3`), starting with a learning rate of 0.1 (`eta0 = 0.1`), using the default learning schedule (different from the preceding one), & it does not use any regularisation (`penalty = None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c4c221a-66cd-405d-b18c-a2dd04f1d7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=0.1, penalty=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter = 1000, tol = 1e-3, penalty = None, eta0 = 0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4c2b8-9ab2-4db7-850d-e2cafa8291ab",
   "metadata": {},
   "source": [
    "Once again, you find a solution quite close to the one retured by the noraml equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6ddad94-c32d-4942-b24c-2dd202768d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.24300814]), array([2.92031118]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bc053-ffa8-4f1f-8a3d-ba9ec3f75c28",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "\n",
    "The last gradient descent algorithm we will look at is called *mini-batch gradient descent*. It is quite simple to understand once you know batch & stochastic gradient descent: at each step, instead of computing the gradients based on the full training set (as in batch GD) or based on just one instance (as in stochastic GD), mini-batch GD computes the gradients on small random subsets of instances called *mini-batches*. The main advantage of mini-batch GD over stochastic GD is that you can get a performance boost from hardware optimisation of matrix operations, especially when using GPUs.\n",
    "\n",
    "The algorithm's progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. As a result, mini-batch GD will end up walking around a bit closer to the minimum than SGD. But, on the other hand, it may be harder for it to escpae from local minima (in the case of problems that suffer from local minima, unlike linear regression as we saw earlier). The below figure shows the paths taken by the three gradient descent algorithms in parameter space during training. They all end up near the minimum, but batch GD's path actually stops at the minimum, while both stochastic GD & mini-batch GD continue to walk around. However, don't forget that batch GD takes a lot of time to take each step, & stochastic GD & mini-batch GD would also reach the minimum if you used a good learning schedule.\n",
    "\n",
    "<img src = \"Images/Compare Gradient Descent.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Let's compare the algorithm we've discussed so far for linear regression (recall that *m* is the number of training instances & *n* is the number of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a7700e9-a0a4-4325-b5f2-da111010e0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Large m</th>\n",
       "      <th>Out-of-Core Support</th>\n",
       "      <th>Large n</th>\n",
       "      <th>Hyperparams</th>\n",
       "      <th>Scaling Required</th>\n",
       "      <th>Scikit-Learn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal Equation</td>\n",
       "      <td>Fast</td>\n",
       "      <td>N</td>\n",
       "      <td>Slow</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVD</td>\n",
       "      <td>Fast</td>\n",
       "      <td>N</td>\n",
       "      <td>Slow</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Batch GD</td>\n",
       "      <td>Slow</td>\n",
       "      <td>N</td>\n",
       "      <td>Fast</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stochastic GD</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Y</td>\n",
       "      <td>Fast</td>\n",
       "      <td>&gt;= 2</td>\n",
       "      <td>Y</td>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mini-batch GD</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Y</td>\n",
       "      <td>Fast</td>\n",
       "      <td>&gt;= 2</td>\n",
       "      <td>Y</td>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Algorithm Large m Out-of-Core Support Large n Hyperparams  \\\n",
       "0  Normal Equation    Fast                   N    Slow           0   \n",
       "1              SVD    Fast                   N    Slow           0   \n",
       "2         Batch GD    Slow                   N    Fast           2   \n",
       "3    Stochastic GD    Fast                   Y    Fast        >= 2   \n",
       "4    Mini-batch GD    Fast                   Y    Fast        >= 2   \n",
       "\n",
       "  Scaling Required      Scikit-Learn  \n",
       "0                N               n/a  \n",
       "1                N  LinearRegression  \n",
       "2                Y      SGDRegressor  \n",
       "3                Y      SGDRegressor  \n",
       "4                Y      SGDRegressor  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "algo_comp = {\"Algorithm\":[\"Normal Equation\", \"SVD\", \"Batch GD\", \"Stochastic GD\", \"Mini-batch GD\"],\n",
    "             \"Large m\": [\"Fast\", \"Fast\", \"Slow\", \"Fast\", \"Fast\"],\n",
    "             \"Out-of-Core Support\": [\"N\", \"N\", \"N\", \"Y\", \"Y\"],\n",
    "             \"Large n\": [\"Slow\", \"Slow\", \"Fast\", \"Fast\", \"Fast\"],\n",
    "             \"Hyperparams\": [\"0\", \"0\", \"2\", \">= 2\", \">= 2\"],\n",
    "             \"Scaling Required\": [\"N\", \"N\", \"Y\", \"Y\", \"Y\"],\n",
    "             \"Scikit-Learn\": [\"n/a\", \"LinearRegression\", \"SGDRegressor\", \"SGDRegressor\", \"SGDRegressor\"]}\n",
    "pd.DataFrame(algo_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecc5c2-a787-4090-80d8-d8ba299b93f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715e24f-eb41-4e43-9d2b-5c4e34bb34a2",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "What if your data is actually more complex than a simple straight line. Surprisingly, you can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers to each feature as new features, then train a linear model on this extended set of features. This technique is called *polynomial regression*.\n",
    "\n",
    "Let's look at an example. First, let's generate some nonlinear data, based on a simple *quadratic equation* (plus some noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cdcb50c0-7c84-417b-a00a-11191c71578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "plt.plot(X, y, \"bo\", markersize = 3)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d3f22-988e-412a-9473-7be7d007ef56",
   "metadata": {},
   "source": [
    "Clearly, a straight line will never fit this data properly. So let's use scikit-learn's `PolynomialFeatures` class to transform our training data, adding the square ($2^{nd}$-degree polynomial) of each feature in the training set as new features (in this case, there is just one feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2364448-ed38-4864-8591-310133162822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06601708])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree = 2, include_bias = False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b242abe5-41f9-48b4-91d9-72a0e45b69f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06601708,  0.00435826])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907c8cb-1520-4b30-80fc-23f0cad5d7d0",
   "metadata": {},
   "source": [
    "`X_poly` now contains the original feature of X plus the square this feature. Now you can fit a `LinearRegression` model to this extended training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21996a6a-6477-4307-bcaf-233fee97e308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.56541137]), array([[1.00630308, 0.58698328]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45032fbd-92db-4a23-87b8-38fd62adda92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "\n",
    "plt.plot(X, y, \"bo\", markersize = 3)\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth = 2, label = \"Predictions\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18122cab-8e81-4068-a6c9-7ce77e1d5d91",
   "metadata": {},
   "source": [
    "Not bad. The model estimates $\\hat{y} = 0.48x^2 + 0.93x + 1.78$ when in fact the original function was $y = 0.5x^2 + x + 2 + Gaussian\\ noise$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a3b14-a74d-4920-a7ff-ed1df96621f0",
   "metadata": {},
   "source": [
    "Note that when there are multiple features, polynomial regression is capable of finding relationships between features (which is something a plain linear regression model cannot do). This is made possible by the fact that `PolynomialFeatures` also adds all combinations of features up to the given degree. For example, if there were two features *a* & *b*, `PolynomialFeatures` with `degree = 3` would not only add the features $a^2$, $a^3$, $b^2$ & $b^3$, but also the combinations $ab$, $a^2b$, & $ab^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4737c6c-c4dc-4693-a598-7b3cb51dc7c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450b1ca-1dfd-426b-b635-22deba0bb5bf",
   "metadata": {},
   "source": [
    "# Learning Curves\n",
    "\n",
    "If you perform high-degree polynomial regression, you will likely fit the training data much better than with plain linear regression; for example, if we apply a 300-degree polynomial model to the preceding training data, & compare the result with a pure linear model & a quadratic model (2nd-degree polynomial). Notice how the 300-degree polynomial wiggles around to get as close as possible to the training instances.\n",
    "\n",
    "<img src = \"Images/Polynomial Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Of course, this high-degree polynomial regression model is severely overfitting the training data, while the linear model is underfitting it. The model that will generalise best in this case is the quadratic model. It makes sense since the data was generated using a quadratic model, but in general, you won't know what function generated the data, so how can you decide how complex your model should be? How can you tell that your model is overfitting or underfitting the data?\n",
    "\n",
    "In chapter 2, you used cross-validation to get an estimate of a model's generalisation performance. If a model performs well on the training data but generalises poorly according to the cross-validation metrics, when your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex.\n",
    "\n",
    "Another way is to look at the *learning curves*: these are plots of the model's performance on the training set & the validation set as a function of the training set size (or the training iteration). To generate the plots, simply train the model several times on different sized subsets of the training set. The following code defines a function that plots the learning curves of a model given some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9a87d23-c0bb-41b4-b661-839628a008fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth = 2, label = \"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth = 3, label = \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a2784-195c-4e8a-9701-f72e891b2544",
   "metadata": {},
   "source": [
    "Let's look at the learning curves of the plain linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7a3855e-e017-41f6-a53b-39c4dea3bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "plt.axis([0, 80, 0, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14817e88-90f4-42ec-aed1-432276d818f1",
   "metadata": {},
   "source": [
    "This deserves a bit of explanation. First, let's look at the performance on the training data: when there are just one or two instances in the training set, the model can fit them perfectly, which is why the curve starts at zero. But as new instances are added to the training set, it becomes impossible for the model to fit the training data perfectly, both because the data is noisy & because it is not linear at all. So the error on the training data goes up until it reaches a plateau, at which point adding new instances to the training set doesn't make the average error much better or worse. Now let's look at the performance of the model on the validation data. When the model is trained on very few training instances, it is incapable of generalising properly, which isi why the validation error is initially quite big. Then as the model is shown more training examples,it learns & thus the validation error slowly goes down. However, once again a straight line cannot do a good job modeling the data, so the error ends up at a plateau, very close to the other curve.\n",
    "\n",
    "These learning curves are typical of an underfitting model. Both curves have reached a plateau; they are close & fairly high.\n",
    "\n",
    "Now let's look at the learning curves of a 10th-degree polynomial model on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a82216e7-ee64-4f17-be41-988fe39edee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([(\"poly_features\", PolynomialFeatures(degree = 10, include_bias = False)), \n",
    "                                  (\"lin_reg\", LinearRegression())])\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "plt.axis([0, 80, 0, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9550c-221a-467b-bf0a-0dad158aa829",
   "metadata": {},
   "source": [
    "These learning curves look a bit like the previous ones, but there are two very important difference:\n",
    "\n",
    "* The error on the training data is much lower than with the linear regression model.\n",
    "* There is a gap between the curves. This means that the model performs significantly better on the training data than on the validation data, which is the hallmark of an overfitting model. However, if you used a much larger training set, the two curves would continue to get closer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcce1e-d442-41a9-8de1-0bd435f0552c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e097d36-8b7a-45f0-941c-cf4f5b784100",
   "metadata": {},
   "source": [
    "# Regularised Linear Models\n",
    "\n",
    "As we saw in previous chapters, a good way to reduce overfitting is to regularise the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularise a polynomial model is to reduce the number of polynomial degrees.\n",
    "\n",
    "For a linear model, regularisation is typically achieved by constraining the weights of the model. We will now look at ridge regression, lasso regression, & elastic net, which implement three different ways to constrain the weights.\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "*Ridge regression* is a regularised version of linear regression: a *regularisation term* equal to $\\alpha\\sum^{n}_{i = 1}\\theta^2_i$ is added to the cost function. This forces the linear algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularisation term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model's performance using the unregularised performance measure.\n",
    "\n",
    "The hyperparameter $\\alpha$ controls how much you want to regularise the model. If $\\alpha$ = 0, then ridge regression is just linear regression. If $\\alpha$ is very alrge, then all weights end up very close to zero & the result is a flat line going through the data's mean. Here is the ridge regression cost function.\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha\\frac{1}{2}\\sum^{n}_{i = 1}\\theta^2_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fc8c8-b666-4cfe-95a6-363ce9643972",
   "metadata": {},
   "source": [
    "Note that the bias term $\\theta_0$ is not regularised (the sum starts at i = 1, not 0). If we define $w$ as the vector of feature weights ($\\theta_1$ to $\\theta_n$), then the regularisation term is simply equal to $\\frac{1}{2}(||w||_2)^2$ where $||w||_2$ represents the $l_2$ norm of the weight vector. For gradient descent, just add $\\alpha w$ to the MSE gradient vector.\n",
    "\n",
    "<img src = \"Images/Ridge Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "The above figure shows several ridge regression models trained on some linear data using different $\\alpha$ values. One the left, plain ridge models are used, leading to linear predictions. On the right, the data is first expanded using `PolynomialFeatures(degree = 10`, then it is scaled using a `StandardScaler`, & final the ridge models are applied to the resulting features: this is polynomial regression with ridge regularisation. Note how increasing $\\alpha$ leads to a flatter (i.e., less extreme, more reasonable) preddctions; this reduces the model's variance but increases its bias.\n",
    "\n",
    "As with linear regression, we can perform ridge regression either by computing a closed-form equation or by performing gradient descent. The pros & cons are the same. The following function shows the closed-form solution (where $A$ is the $(n + 1)(n + 1)$ *identity matrix* except with a 0 in the top-left cell, corresponding to the bias term).\n",
    "\n",
    "$$\\hat{\\theta} = (X^TX + \\alpha A)^{-1}X^Ty$$\n",
    "\n",
    "Here is how to perform ridge regression with scikit-learn using a closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73eb8fd5-ac49-4729-8f80-f7945cd0cdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.70912199]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge(alpha = 1, solver = \"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd917cff-db77-44a0-9314-86df744d51fd",
   "metadata": {},
   "source": [
    "& using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4fa82ed2-2f2e-4894-96af-678c79fe1544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.65785032])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(penalty = \"l2\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7a797-15e1-4b1f-af54-df6d5874fb01",
   "metadata": {},
   "source": [
    "The `penalty` hyperparameter sets the type of regularisation term to use. Specifying \"`l2`\" indicates that you want SGD to add a regularisation term to the cost function equal to half the square of the $l_2$ norm of the weight vector: this is simply ridge regression.\n",
    "\n",
    "## Lasso Regression\n",
    "\n",
    "*Least Absolute Shrinkage & Selection Operation Regression* (simply called *Lasso regression*) is another regularisation version of linear regression: just like ridge regression, it adds a regularisation term to the cost function, but it used the $l_1$ norm of the weight vector instead of half the square of teh $l_2$ norm.\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha\\sum^{n}_{i = 1}|\\theta_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1779dd9-2667-4411-bfa7-0aaa2d984eb4",
   "metadata": {},
   "source": [
    "The below figure shows several lasso regression models trained on some linear data using different $\\alpha$ values.\n",
    "\n",
    "<img src = \"Images/Lasso Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "An important characteristic of Lasso regression is that it tends to completely eliminates the weights of the least important features (i.e, set them to zero). For example, the dashed line in the right plot (with $\\alpha = 10^{-7}$) looks quadratic, almost linear: all the weights for the high-degree polynomial features are equal to zero. In other words, Lasso regression automatically performs feature selection & outputs a *sparse model* (i.e., with few nonzero feature weights).\n",
    "\n",
    "<img src = \"Images/Lasso vs Ridge Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "You can get a sense of why this is the case. one the top-left plot, the background contours (ellipses) represent an unregularised MSE cost function ($\\alpha$ = 0), & the white circles show the batch gradient descent path with that cost function. The foreground contours (diamonds) represent the $l_1$ penalty, & the triange shows the BGD path for this penalty only ($\\alpha \\rightarrow \\infty$). Notice how the path first reaches $\\theta_1$ = 0, then rolls down the gutter until it reacher $\\theta_2$ = 0. On the top-right plot, the contours represent the same cost function plus an $l_1$ penalty with $\\alpha$ = 0.5. The global minimum is on the $\\theta_2$ = 0 axis. BGD first reaches $\\theta_2$ = 0, then rolls down the gutter until it reaches the global minimum. The two bottom plots show the same thing but uses an $l_2$ penalty instead. The regularised minimum is closer to $\\theta$ = 0 than the unregularised minimum, but the weights do not get fully eliminated.\n",
    "\n",
    "The Lasso cost function is not differentiable at $\\theta_i$ = 0 (for i = 1, 2, ..., n), but gradient descent still works fine if you use a *subgradient vector* $g$ instead whenany $\\theta_i$ = 0. Below shows a subgradient vector equation you can use for gradient descent with the Lasso cost function.\n",
    "\n",
    "$$g(\\theta, J) = \\triangledown_{\\theta}MSE(\\theta) + \\alpha \\Bigg(\\begin{split}\n",
    "sign(\\theta_1) \\\\\n",
    "sign(\\theta_2) \\\\\n",
    "{\\vdots} \\\\\n",
    "sign(\\theta_n)\n",
    "\\end{split} \\Bigg) where\\ sign(\\theta_i) = \\Biggl\\{\\begin{split}\n",
    "-1\\ if\\ \\theta_i < 0 \\\\\n",
    "0\\ if\\ \\theta_i = 0 \\\\\n",
    "1\\ if\\ \\theta_i > 0\n",
    "\\end{split}$$\n",
    "\n",
    "Here is a small example using the `Lasso` class. Note that you could instead use an `SGDRegressor(penalty = \"l1\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6131778f-551c-4369-8e80-61172bf8dd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.66533413])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha = 0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f285a7-2f96-482c-8f88-9056907a7c66",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "Elastic net is a middle ground between ridge regression & lasso regression. The regularisation term is a simple mix of both ridge & lasso's regularisation terms, & you can control the mix ratio *r*. When *r* = 0, elastic net is equivalent to ridge regression, & when *r* = 1, it is equivalent to lasso regression.\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + r\\alpha\\sum^{n}_{i = 1}|\\theta_i| + \\frac{1 - r}{2}\\alpha\\sum^{n}_{i = 1}\\theta^2_i$$\n",
    "\n",
    "So when should you use plain linear regression (i.e., without any regularisation), ridge, lasso, & elastic net? It is almost always preferable to have at least a little bit of regularisation, so generally you should avoid plain linear regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer lasso or elastic net since they tend to reduce the useless features' weights down to zero as we have discussed. In general, elastic net is perferred over lasso since lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "\n",
    "Here is a short example using scikit-learn's `ElasticNet` (`l1_ratio` corrsponds to the mix ratio *r*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd91cd7e-dec9-4997-8abb-bbe927843a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.67097706])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha = 0.1, l1_ratio = 0.5)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eaaaa1-c719-445a-8b10-6e53d29cc20c",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "A very different way to regularise iterative learning algorithms such as gradient descent is to stop training as soon as the validation error reaches a minimum. This is called *early stopping*. The below figure shows a complex model (in this case a high-degree polynomial regression model) being trained using batch gradient descent. As the epochs go by, the algorithm learns & its prediction error (RMSE) on the training set naturally goes down, & so does it prediction error on the validation set. However, after a while, the validation error stops decreasing & actually starts to go back up. This indicates that the model has started to overfit the training data. With early stopping, you must stop training as soon as the validation error reaches the minimum. It is such a simple & efficeint regularisation technique that some call it \"beautiful free lunch\".\n",
    "\n",
    "<img src = \"Images/Early Stopping.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Here is a basic implementatino of early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de69e59e-8ffd-4602-9751-39ce4d574d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "poly_scaler = Pipeline([(\"poly_features\", PolynomialFeatures(degree = 90, include_bias = False)), \n",
    "                        (\"std_scaler\", StandardScaler())])\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "                     \n",
    "sgd_reg = SGDRegressor(max_iter = 1, tol = -np.infty, warm_start = True, \n",
    "                       penalty = None, learning_rate = \"constant\", eta0 = 0.0005)\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        bestmodel = clone(sgd_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d472a30-8ffe-406c-a21d-b59ed00e1af3",
   "metadata": {},
   "source": [
    "Note that with `warm_start = True`, when the `fit()` method is called, it just continues training where it left off instead of restarting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c83de-e498-4c03-9a3a-596250f0c228",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08a307-e761-4f03-9548-8fad53a19fbe",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "As we discussed before, some regression algorithms can be used for classification as well (& vice versa). *Logistic regression* is common used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled \"1\"), or else it predicts that it does not (i.e., it belongs to the negative class, labeled \"0\"). This makes it a binary classifier.\n",
    "\n",
    "## Estimating Probabilities\n",
    "\n",
    "So how does it work? Just like a linear regression model, a logistic regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the linear regression model does, it outputs the *logistic* of this result.\n",
    "\n",
    "$$\\hat{p} = h_{\\theta}(x) = \\sigma(x^T\\theta)$$\n",
    "\n",
    "The logistic -- noted $\\sigma(.)$ -- is a *sigmoid funcrion* (i.e., S-shaped) that outputs a number between 0 & 1. It is defined as the following function.\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$\n",
    "\n",
    "<img src = \"Images/Logistic Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Once the logistic regression model has estimated the probability $\\hat{p} = h_{\\theta}(x)$ that an instance $x$ belongs to the positive class, it can make its predictions $\\hat{y}$ easily.\n",
    "\n",
    "$$\\hat{y} = \\Biggl\\{ \\begin{split}\n",
    "0\\ if\\ \\hat{p} < 0.5 \\\\\n",
    "1\\ if\\ \\hat{p} >= 0.5\n",
    "\\end{split}$$\n",
    "\n",
    "Notice that $\\sigma(t)$ < 0.5 when t < 0, & $\\sigma(t)$ >= 0.5 when t >= 0, so a logistic regression predicts 1 if $x^T\\theta$ is positive & 0 if it is negative.\n",
    "\n",
    "## Training & Cost Function\n",
    "\n",
    "Good, now you know how a logistic regression model estimates probabilities & makes predictions. But how is it trained? The objective of training is to set the parameter vector $\\theta$ so that the model estimates high probabilities for positive instance(y = 1) & low probabilities for negative instance (y = 0). This idea is captured by the cost function shown below, for a single training instance x.\n",
    "\n",
    "$$c(\\theta) = \\Biggl\\{ \\begin{split}\n",
    "-log(\\hat{p})\\ if\\ y = 1 \\\\\n",
    "-log(1 - \\hat{p})\\ if\\ y = 0\n",
    "\\end{split}$$\n",
    "\n",
    "This cost function makes sense because -log(t) grows very large when *t* approaches 0, so the cost will be large if the model estimates a probability close to 0 for a positive instance, & it will also be very large if the model estimates a probability close to 1 for a negative instance. On the other hand, -log(t) is close to 0 when *t* is close to 1, so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to 1 for a positive instance, which is precisely what we want.\n",
    "\n",
    "The cost function over the whole training set is simply the average cost over all training instances. It can be written in a single expression called the *log loss*, demonstrated below.\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum^{m}_{i = 1}[y^{(i)}log(\\hat{p}^{(i)}) + (1 - y^{(i)})log(1 - \\hat{p}^{(i)})]$$\n",
    "\n",
    "The bad news is that there is no known closed-form equation to compute the value of $\\theta$ that minimises the cost function (there is no equivalent of the normal equation). But the good news is that this cost function is convex, so gradient descent (or any other optimisation algorithm) is guaranteed to find the gloabl minimum (if the learning rate is not too large & you wait long enough). The partial derivtives of the cost function with regards to the jth model paramter $\\theta_j$ is given:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_j} = \\frac{1}{m}\\sum^{m}_{i = 1}(\\sigma(\\theta^Tx^{(i)}) - y^{(i)})x^{(i)}_j$$\n",
    "\n",
    "For each instance, the expression computes the prediction error & multiplies it by the jth feature value, & then it computes the average over all training instances. Once you have the gradient vector containing all the partial derivatives you can use it in the batch gradient descent algorithm. That's it: you now know how to train a logistic regression model. For stochastic GD, you would of course take on instance at a time, & for mini-batch GD, you would use a mini-batch at a time.\n",
    "\n",
    "## Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829344b-7177-4b75-b8df-bfae932f52b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
