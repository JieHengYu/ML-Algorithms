{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60dd61a1-0b2c-400e-b517-6db07dd2d2b8",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "\n",
    "So far, we have treated machine learning models & their training algorithms mostly like black boxes. If you went through some of previous lessons, you may have been surprised by how much you can get done without knowing anything about what's under the hood: you optimised a regression system, you improved a digit image classifier, & you even built a spam classifier from scratch -- all this without knowing how they actually work. Indeed, in many situations you don't really need to know the implementation details.\n",
    "\n",
    "However, having a good understanding of how things work can help you quickly home in on an appropriate model, the right training algorithm to use, & a good set of hyperparameters for your task. Understanding what's under the hood will also help you debug issues & perform error analysis more efficiently. Lastly, most of the topics discussed in this lesson will be essential in understanding, building, & training neural networks.\n",
    "\n",
    "In this lesson, we will start by looking at the linear regression model, one of the simplest models there is. We will discuss two very different ways to train it.\n",
    "\n",
    "* Using a direct \"closed-form\" equation that directly computes the model parameters that best fit the model to the training set (i.e., the model parameters that minimise the cost function over the training set).\n",
    "* Using an iterative optimisation approach, called gradient descent (GD), that gradually tweaks the model parameters to minimise the cost function over the training set, eventually converging to the same set of parameters as the first method. We will look at a few variants of gradient descent that we will use again & again when we study neural networks in future lessons: batch GD, mini-batch GD, & stochastic GD.\n",
    "\n",
    "Next, we will look at polynomial regression, a more complex model that can fit nonlinear datasets. Since this model has more parameters than linear regression, it is more prone to overfitting the training data, so we will look at how to detect whether or not this is the case, using learning curves, & then we will look at several regularisation techniques that can reduce the risk of overfitting the training set.\n",
    "\n",
    "Finally, we will look at two more models that are commonly used for classification tasks: logistic regression & softmax regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae887b-5cc9-4102-81b8-616ded7224b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0cf712-e688-40ed-8f19-a1b8d224faa8",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In chapter 1, we looked at a simple regression model of life satisfaction: $life\\_satisfaction = \\theta_0 + \\theta_1 * GDP\\_per\\_capita$.\n",
    "\n",
    "This model is just a linear function of the input feature `GDP_per_capita`. $\\theta_0$ & $\\theta_1$ are the model's parameters.\n",
    "\n",
    "More generally, a linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the *bias term* (also called the *intercept term*).\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$$\n",
    "\n",
    "* $\\hat{y}$ is the predicted value.\n",
    "* $n$ is the number of features.\n",
    "* $x_i$ is the $i^{th}$ feature value.\n",
    "* $\\theta_i$ is the $i^{th}$ model parameter (including the bias term or intercept $\\theta_0$ , where $x_0 = 1$, & the feature weights $\\theta_1$, $\\theta_2$, ..., $\\theta_n$).\n",
    "\n",
    "This can be written much more concisely using a vectorised form.\n",
    "\n",
    "$$\\hat{y} = h_{\\theta}(x) = \\theta * x$$\n",
    "\n",
    "* $\\theta$ is the model's *parameter vector*, containing the bias term $\\theta_0$ & the feature weights $\\theta_1$ to $\\theta_n$.\n",
    "* $x$ is the instance's *feature vector*, containing $x_0$ to $x_n$, with $x_0$ always equal to 1.\n",
    "* $\\theta * x$ is the dot product of the vectors $\\theta$ & $x$, which is of course equal to $\\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$.\n",
    "* $h_{\\theta}$ is the hypothesis function, using the model parameters $\\theta$.\n",
    "\n",
    "**Note: In machine learning, vectors are often represented as column vectors \n",
    "(Ex: $\\left[{\\begin{array}{c}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\\\\\n",
    "\\end{array}} \\right]$), \n",
    "which are 2D arrays with a single column. If $\\theta$ & $x$ are column vectors, then the prediction is: $\\hat{y}= \\theta^Tx$ where $\\theta^T$ is the transpose of $\\theta$ \n",
    "(a row vector instead of a column vector, Ex: $\\left[{\\begin{array}{ccc}\n",
    "a\\ b\\ c\\\\\n",
    "\\end{array}} \\right]$) & $\\theta^Tx$ is the matrix multiplication of $\\theta^T$ & $x$. It is, of course, the same prediction, except it is now represented as a single cell matrix rather than a scalar value.**\n",
    "\n",
    "Okay, that's the linear regression model, so now how do we train it? Well, recall that training a model means setting its parameters so that the model best fits the training set. For this purpose, we first need a measure of how well (or poorly) the model fits the training data. In chapter 2, we saw that the most common performance measure of a regression model is the root mean square error (RMSE). Therefore, to train a linear regression model, you need to find the value of $\\theta$ that minimises the RMSE. In practice, it is simpler to minimise the mean square error (MSE) & it leads to the same result (because the value that minimises a function also minimises it square root).\n",
    "\n",
    "The MSE of a linear regression hypothesis $h_{\\theta}$ on a training set $X$ is calculated:\n",
    "\n",
    "$$MSE(X, h_{\\theta}) = \\frac{1}{m}\\sum_{i = 1}^{m}(\\theta^{T}x_i - y_i)^2$$\n",
    "\n",
    "Most of these notations were presented in chapter 2. The only difference is that we write $h_\\theta$ instead of just $h$ in order to make it clear that the model is parameterised by the vector $\\theta$. To simplify notations, we will just write $MSE(\\theta)$ instead of $MSE(X, h_{\\theta})$.\n",
    "\n",
    "## The Normal Equation\n",
    "\n",
    "To find the value of $\\theta$ that minimises the cost function, there is a *closed-form solution* -- in other words, a mathematical equation that gives the result directly. This is called the *normal equation*.\n",
    "\n",
    "$$\\hat{\\theta} = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "* $\\hat{\\theta}$ is the value of $\\theta$ that minimises the cost function.\n",
    "* $y$ is the vector of target values containing $y^{1}$ to $y^{n}$.\n",
    "\n",
    "Let's generate some linear-looking data to test this equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6bbe07f-52ca-4c4c-bdee-b93c0d552b92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate 100 numbers in a column vector (shape: 100, 1) between 0 & 1, then multiply them by 2, i.e., between 0 & 2.\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "\n",
    "# 4 + 3X + random number between 0 & 1 for all 100 numbers of X (shape: 100, 1).\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Plot X against y using blue coordinates.\n",
    "plt.plot(X, y, \"bo\", markersize = 3)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8294417-95ec-4e47-a9cb-8d4b8d4e16b7",
   "metadata": {},
   "source": [
    "Now let's compute $\\hat{\\theta}$ using the normal equation. We will use the `inv()` function from numpy's linear algebra module (`np.linalg`) to compute the inverse of a matrix, & the `dot()` method for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad072a9-bee4-4ac6-b2fb-afb1f98d07db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate values of X_0 == 1 to the left of X values (shape: 100, 2).\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Compute the best feature weights using the normal equation.\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55633-df85-4e32-93ba-61959d26b73e",
   "metadata": {},
   "source": [
    "The actual function we used to generate the data is $y = 4 + 3x + Gaussian\\ noise$. Let's see what the equation found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf22a937-6ab1-434a-ad85-089517f35eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.37930703],\n",
       "       [2.63893119]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc6bb71-f98e-4fb5-895c-20977944936e",
   "metadata": {},
   "source": [
    "We would have hoped for $\\theta_0 = 4$ & $\\theta_1 = 3$. Close enough though, but the noise made it impossible to recover the exact parameters of the original function.\n",
    "\n",
    "Now you can make predictions using $\\hat{\\theta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba6088b-a536-44b9-9c61-eac331c3da5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.37930703],\n",
       "       [9.65716941]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate an array of new values, X_new, to predict y (shape: 2, 1).\n",
    "X_new = np.array([[0], [2]])\n",
    "\n",
    "# Concatenate values of X_0 == 1 to the left of the X_new (shape: 2, 2).\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "\n",
    "# Predict the y values using our computed \"best\" feature weights.\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495e3d8-1637-47b8-989f-812c1a5e598f",
   "metadata": {},
   "source": [
    "Let's plot this model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2728add1-8244-4dcf-a7a2-44b611c9162e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the relationship between X_new & its predicted y values using a red line.\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.legend([\"Predictions\"], loc = \"upper left\")\n",
    "\n",
    "# Plot the coordinates of X & y in blue.\n",
    "plt.plot(X, y, \"bo\", markersize = 3)\n",
    "\n",
    "# Set the x-axis limit to [0, 2] & y-axis limit to [0, 15].\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848fcfd6-f0dd-467e-93fd-b5d7e4ece123",
   "metadata": {},
   "source": [
    "Performing linear regression using scikit-learn is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2de3848-8a5d-43fe-94af-615b31933b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.37930703]), array([[2.63893119]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression instance.\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Fit the linear regression instance to our X & y values.\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "# Get the intercept & coefficient.\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5aa7259-7261-4cb1-bab9-c61caf33a4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.37930703],\n",
       "       [9.65716941]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict y for X_new using our linear regression model.\n",
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274c5be-be0a-45d2-9b92-25229919b7d2",
   "metadata": {},
   "source": [
    "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (the name stands for \"least squares\"), which you can call directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ab3cdd-ba0e-4c31-82cb-e26c1f522714",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.37930703],\n",
       "       [2.63893119]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the best feature weights, residuals, rank using least-squares regression.\n",
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond = 1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efac86-8663-4a4f-bbeb-ea48aea6862d",
   "metadata": {},
   "source": [
    "This function computes $\\hat{\\theta} = X^{+}y$, where $X^{+}$ is the *pseudoinverse* of $X$ (specifically the Moore-Penrose inverse). You can use `np.linalg.pinv()` to compute the pseudoinverse directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6411849c-b5bc-47c2-983b-314f5953280c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.37930703],\n",
       "       [2.63893119]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the feature weights using the pseudoinverse method.\n",
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad348d-70e1-4b54-91d7-837d68221c76",
   "metadata": {},
   "source": [
    "The pseudoinverse itself is computed using a standard matrix factorisation technique called *Singular Value Decomposition* (SVD) that can decompose the training set matrix $X$ into the matrix multiplication of three matrices $U$ $\\sum$ $V^{T}$. The pseudoinverse is computed as $X^{+} = V\\sum^{+}U^{T}$. To compute the matrix $\\sum^{+}$, the algorithm takes $\\sum$ & sets to zero all values smaller than a tiny threshold value, then it replaces all the non-zero values with their inverse, & finally it transposes the resulting matrix. This approach is more efficient than computing the normal equation, plus it handles edge cases nicely: indeed, the normal equation may not work if the matrix $X^{T}X$ is not invertible (i.e., singular, determinant = 0), such as $m$ (# of samples) < $n$ (# of features) or if some features are redundant, but the pseudoinverse is always defined.\n",
    "\n",
    "## Computational Complexity\n",
    "\n",
    "The normal equation computes the inverse of $X^{T}X$, which is an $(n + 1)(n + 1)$ matrix (where $n$ is the number of features). The *computational complexity* of inverting such a matrix is typically about $O(n^{2.4})$ to $O(n^3)$ (depending on the implementation). In other words, if you double the number of features, you multiply the computation time by roughly $2^{2.4} = 5.3$ to $2^3 = 8$.\n",
    "\n",
    "The SVD approach used by scikit-learn's `LinearRegression` class is about $O(n^2)$. If you double the number of features, you multiply the computation time by roughly 4.\n",
    "\n",
    "Also, once you have trained your linear regression model (using the normal equation or any other algorithm), predictions are very fast: the computational complexity is linear with regards to both the number of instances you want to make predictions on & the number of features. In other words, making predictions on twice as many instances (or twice as many features) will just take roughly twice as much time.\n",
    "\n",
    "Now we will look at very different ways to train a linear regression model, better suited for cases where there are a large number of features, or too many training instances to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23407670-485b-4b94-af7f-4d2a6f6a2075",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86670a-d57c-485e-87d7-cb6bdf91f0b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "*Gradient Descent* is a very generic optimisation algorithm capable of finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimise a cost function.\n",
    "\n",
    "Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly is to go downhill in the direction of the steepest downward slope. This is exactly what gradient does; it measures the local gradient of the error function with regards to the parameter vector $\\theta$, & it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum!\n",
    "\n",
    "Concretely, you start by filling $\\theta$ with random values (this is called *random initialisation*), & then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm *converges* to a minimum.\n",
    "\n",
    "<img src = \"Images/Gradient Descent.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "An important parameter in gradient descent is the size of the steps, determined by the *learning rate* hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time.\n",
    "\n",
    "<img src = \"Images/Learning Rate.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "On the other hand, if the learning rate is too high, you might jump across the valley & end up on the other side, possibly even higher up than before. This might make the algorithm diverge, with larger & larger values, failling to find a good solution.\n",
    "\n",
    "<img src = \"Images/Learning Rate 1.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Finally, not all cost functions look like nice regular bowls. There may be holes, ridges, plateaus, & all sorts of irregular terrains, making convergence to the minimum very difficult. The below figure shows the two main challenges with gradient descent: if the random initialisation starts the algorithm on the left, then it will converge to a *local minimum*, which is not as good as the *global minimum*. If it starts on the right, then it will take a very long time to cross the plateau, & if you stop too early you will never reach the global minimum.\n",
    "\n",
    "<img src = \"Images/Gradient Descent Pitfalls.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Fortunately, the MSE cost function for a linear regression model happens to be a *convex function*, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. These two facts have great consequence: gradient descent is guaranteed to approach arbitrarily close to the global minimum (if you wait long enough & if the learning rate is not too high).\n",
    "\n",
    "In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scales. The below figure shows gradient descent on a training set where features 1 & 2 have the same scale (on the left), & on a training set where feature 1 has much smaller values than feature 2 (on the right).\n",
    "\n",
    "<img src = \"Images/Gradient Descent 1.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "As you can see, on the left, the gradient descent algorithm goes straight toward the minimum, thereby reaching it quickly, whereas on the right, it first goes in a direction almost orthogonal to the direction of the global minimum, & it ends with a long march down an almost flat valley. It will eventually reach the minimum, but it will take a long time.\n",
    "\n",
    "This diagram also illustrates the fact that training a model means searching for a combination of model parameters that minimises a cost function (over the training set). It is a search in the model's *parameter space*: the more parameters a model has, the more dimensions this space has, & the harder the search is: searching for a needle in a 300-dimensional haystack is much trickier than in three dimensions. Fortunately, since the cost function is convex in the case of linear regression, the needle is simply at the bottom of the bowl.\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "To implement gradient descent, you need to compute the gradient of the cost function with regards to each model parameter $\\theta_i$. In other words, you need to calculate how much the cost function will change if you change $\\theta_i$ just a little bit. This is called a *partial derivative*. It is like asking \"what is the slope of the mountain under my feet if I face east?\", then asking the same question facing north (& so on for the rest of the other dimensions, if you can imagine a universe with more than three dimensions). The below function computes the partial derivative of the cost function with regards to parameter $\\theta_i$, noted $\\frac{\\partial}{\\partial\\theta_i}MSE(\\theta)$.\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_i}MSE(\\theta) = \\frac{2}{m} \\sum^{m}_{i = 1}(\\theta^{T}x_i - y_i)x_i$$\n",
    "\n",
    "Instead of computing these partial derivatives individually, you can use the below function to compute them all in one go. The gradient vector, noted $\\triangledown_{\\theta}MSE(\\theta)$, contains all the partial derivatives of the cost function (one for each model parameter).\n",
    "\n",
    "$$\\triangledown_{\\theta}MSE(\\theta) = \\Bigg(\\begin{split} \n",
    "{\\frac{\\partial}{\\partial\\theta_0} MSE(\\theta)} \\\\\n",
    "{\\frac{\\partial}{\\partial\\theta_1} MSE(\\theta)} \\\\\n",
    "{...} \\\\\n",
    "{\\frac{\\partial}{\\partial\\theta_n}MSE(\\theta)} \n",
    "\\end{split}\\Bigg) = \\frac{2}{m}X^T(X\\theta - y)$$\n",
    "\n",
    "Once you have the gradient vector, which points uphill, just go in the opposite direction to go downhill. This means subtracting $\\triangledown_{\\theta} MSE(\\theta)$ from $\\theta$. This is where the learning rate $\\eta$ comes into play: multiple the gradient vector by $\\eta$ to determine the size of the downhill step.\n",
    "\n",
    "$$\\theta^{(next\\ step)} = \\theta - \\eta\\triangledown_{\\theta}MSE(\\theta)$$\n",
    "\n",
    "Let's look at a quick implementation of this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1beae470-82b9-4124-93ac-55f27a6a1d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning rate.\n",
    "eta = 0.1\n",
    "\n",
    "# Number of iterations.\n",
    "n_iterations = 1000\n",
    "\n",
    "# Sample size.\n",
    "m = 100\n",
    "\n",
    "# Generate two random numbers from a standard normal distribution (mean = 0, sd = 1, shape: 2, 1) as a starting point for our model parameters.\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# For 1000 iterations.\n",
    "for iteration in range(n_iterations):\n",
    "    \n",
    "    # Compute the gradient vector for the iteration.\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    \n",
    "    # Calculate the model parameters for the iteration.\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5521f-4379-4153-8536-ab5cdaceca28",
   "metadata": {},
   "source": [
    "That wasn't too hard! Sure was hard to understand though. Let's look at the resulting `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf4bf3e-e0f6-4cc4-9896-8372a99755ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.37930703],\n",
       "       [2.63893119]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model parameters after 1000 iterations.\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73074b26-f970-4a98-9f6d-6a265d58a973",
   "metadata": {},
   "source": [
    "Hey! That's exactly what the normal equation found! Gradient descent worked perfectly. But what if we had used a different learning rate `eta`? The below figure shows the first 10 steps of gradient descent using three different learning rates (the dashed line represents the starting point).\n",
    "\n",
    "<img src = \"Images/Gradient Descent with Different Learning Rates.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "On the left, the learning rate is too low: the algorithm will eventually reach the solution, but it will take a long time. In the middle, the learning rate looks pretty good: in just a few iterations, it has already converged to the solution. On the right, the learning rate is too high: the algorithm diverges, jumping all over the place & actually gets further & further away from the solution at every step.\n",
    "\n",
    "To find a good learning rate, you can use grid search. However, you may want to limit the number of iterations so that grid search can eliminate models that take too long to converge.\n",
    "\n",
    "You may wonder how to set the number of iterations. If it is too low, you will still be far away from the optimal solution when algorithm stops, but if it is too high, you will waste time while the model parameters do not change anymore. A simple solution is to set a very large number of iterations but to interrupt the algorithm when the gradient vector becomes tiny -- that is, when its norm becomes smaller than a tiny number $\\epsilon$ (called the *tolerance*) -- because this happens when the gradient descent has (almost) reached the minimum).\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "The main problem with batch gradient descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, *stochastic gradient descent* just picks a random instance in the training set at every step & computes the gradients based only on that single instance. Obviously, this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration (SGD can be implemented as an out-of-core algorithm).\n",
    "\n",
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than batch gradient descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up & down, decreasing only on average. Over time, it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal.\n",
    "\n",
    "<img src = \"Images/Stochastic Gradient Descent.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "When the cost function is very irregular, this can actually help the algorithm jump out of local minima, so stochastic gradient descent has a better chance of finding the global minimum than batch gradient descent does. Therefore randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress & escape local minima), then get smaller & smaller, allowing the algorithm to settle at the global minimum. This process is akin to *simulated annealing*, an algorithm inspired from the process of annealing in metallurgy where molten metal is slowly cooled down. The function that determines the learning rate at each iteration is called the *learning schedule*. If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. If the learning rate is reduced too slowly, you may jump around the minimum for a long time & end up with a suboptimal solution if you halt training too early.\n",
    "\n",
    "The following code implements stochastic gradient descent using a simple learning schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7896f1ff-f2f2-48fc-8a34-9e0368cfb8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of iterations where we will pass a training set through the algorithm completely.\n",
    "n_epochs = 50\n",
    "\n",
    "# Parameters for learning rate calculation.\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "# Define function for learning rate.\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "# Generate two random numbers from a standard normal distribution (mean = 0, sd = 1, shape: 2, 1) as a starting point for our model parameters.\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# For each iteration.\n",
    "for epoch in range (n_epochs):\n",
    "    \n",
    "    # For each iteration of our sample size.\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Grab a random integer from a uniform distribution of integers in the interval [0, m) for a training set indexing.\n",
    "        random_index = np.random.randint(m)\n",
    "        \n",
    "        # Get the X values from the training set at index: random_index\n",
    "        xi = X_b[random_index:random_index + 1]\n",
    "        \n",
    "        # Get the y values from the training set at index: random_index\n",
    "        yi = y[random_index:random_index + 1]\n",
    "        \n",
    "        # Compute the gradient using the X & y values.\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        \n",
    "        # Compute the learning rate.\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        \n",
    "        # Calculate the model parameters for this iteration.\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01669a57-b421-464d-94e4-9e139d51e9c9",
   "metadata": {},
   "source": [
    "By convention, we iterate by rounds of *m* iterations; each round is called an *epoch*. An epoch is the passing of a dataset through an algorithm completely. While the batch gradient descent code iterated 1,000 times through the whole training set, this code goes through the training set only 50 times & reaches a fairly good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ef9bcd-0ec2-4853-a38d-8881203a0741",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.43927546],\n",
       "       [2.65451544]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model parameters after 50 iterations.\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d846a-6ae0-40e7-98a2-0f3999ce07be",
   "metadata": {},
   "source": [
    "The following figure shows the first 20 steps of training (notice how irregular the steps are).\n",
    "\n",
    "<img src = \"Images/Stochastic Gradient Descent 1.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set (making sure to shuffle the input features & the labels jointly), then go through it instance by instance, then shuffle it again, & so on. However, this generally converges more slowly.\n",
    "\n",
    "To perform linear regression using SGD with scikit-learn, you can use the `SGDRegressor` class, which defaults to optimising the squared error cost function. The following code runs for maximum 1000 epochs (`max_iter = 1000`) or until the loss drops by less than 1e-3 during one epoch (`tol = 1e-3`), starting with a learning rate of 0.1 (`eta0 = 0.1`), using the default learning schedule (different from the preceding one), & it does not use any regularisation (`penalty = None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4c221a-66cd-405d-b18c-a2dd04f1d7c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=0.1, penalty=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create a SGDRegressor instance.\n",
    "sgd_reg = SGDRegressor(max_iter = 1000, tol = 1e-3, penalty = None, eta0 = 0.1)\n",
    "\n",
    "# Fit the SGDRegresson instance to our data.\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4c2b8-9ab2-4db7-850d-e2cafa8291ab",
   "metadata": {},
   "source": [
    "Once again, you find a solution quite close to the one returned by the normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6ddad94-c32d-4942-b24c-2dd202768d34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.35283724]), array([2.62754525]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the intercept & coefficient from our model.\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bc053-ffa8-4f1f-8a3d-ba9ec3f75c28",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "\n",
    "The last gradient descent algorithm we will look at is called *mini-batch gradient descent*. It is quite simple to understand once you know batch & stochastic gradient descent: at each step, instead of computing the gradients based on the full training set (as in batch GD) or based on just one instance (as in stochastic GD), mini-batch GD computes the gradients on small random subsets of instances called *mini-batches*. The main advantage of mini-batch GD over stochastic GD is that you can get a performance boost from hardware optimisation of matrix operations, especially when using GPUs.\n",
    "\n",
    "The algorithm's progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. As a result, mini-batch GD will end up walking around a bit closer to the minimum than SGD. But on the other hand, it may be harder for it to escape from local minima (in the case of problems that suffer from local minima, unlike linear regression as we saw earlier). The below figure shows the paths taken by the three gradient descent algorithms in parameter space during training. They all end up near the minimum, but batch GD's path actually stops at the minimum, while both stochastic GD & mini-batch GD continue to walk around. However, don't forget that batch GD takes a lot of time to take each step, & stochastic GD & mini-batch GD would also reach the minimum if you used a good learning schedule.\n",
    "\n",
    "<img src = \"Images/Compare Gradient Descent.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Let's compare the algorithms we've discussed so far for linear regression (recall that $m$ is the number of training instances & $n$ is the number of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a7700e9-a0a4-4325-b5f2-da111010e0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Large m</th>\n",
       "      <th>Out-of-Core Support</th>\n",
       "      <th>Large n</th>\n",
       "      <th>Hyperparams</th>\n",
       "      <th>Scaling Required</th>\n",
       "      <th>Scikit-Learn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal Equation</td>\n",
       "      <td>Fast</td>\n",
       "      <td>N</td>\n",
       "      <td>Slow</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVD</td>\n",
       "      <td>Fast</td>\n",
       "      <td>N</td>\n",
       "      <td>Slow</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Batch GD</td>\n",
       "      <td>Slow</td>\n",
       "      <td>N</td>\n",
       "      <td>Fast</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stochastic GD</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Y</td>\n",
       "      <td>Fast</td>\n",
       "      <td>&gt;= 2</td>\n",
       "      <td>Y</td>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mini-batch GD</td>\n",
       "      <td>Fast</td>\n",
       "      <td>Y</td>\n",
       "      <td>Fast</td>\n",
       "      <td>&gt;= 2</td>\n",
       "      <td>Y</td>\n",
       "      <td>SGDRegressor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Algorithm Large m Out-of-Core Support Large n Hyperparams  \\\n",
       "0  Normal Equation    Fast                   N    Slow           0   \n",
       "1              SVD    Fast                   N    Slow           0   \n",
       "2         Batch GD    Slow                   N    Fast           2   \n",
       "3    Stochastic GD    Fast                   Y    Fast        >= 2   \n",
       "4    Mini-batch GD    Fast                   Y    Fast        >= 2   \n",
       "\n",
       "  Scaling Required      Scikit-Learn  \n",
       "0                N               n/a  \n",
       "1                N  LinearRegression  \n",
       "2                Y      SGDRegressor  \n",
       "3                Y      SGDRegressor  \n",
       "4                Y      SGDRegressor  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "algo_comp = {\"Algorithm\":[\"Normal Equation\", \"SVD\", \"Batch GD\", \"Stochastic GD\", \"Mini-batch GD\"],\n",
    "             \"Large m\": [\"Fast\", \"Fast\", \"Slow\", \"Fast\", \"Fast\"],\n",
    "             \"Out-of-Core Support\": [\"N\", \"N\", \"N\", \"Y\", \"Y\"],\n",
    "             \"Large n\": [\"Slow\", \"Slow\", \"Fast\", \"Fast\", \"Fast\"],\n",
    "             \"Hyperparams\": [\"0\", \"0\", \"2\", \">= 2\", \">= 2\"],\n",
    "             \"Scaling Required\": [\"N\", \"N\", \"Y\", \"Y\", \"Y\"],\n",
    "             \"Scikit-Learn\": [\"n/a\", \"LinearRegression\", \"SGDRegressor\", \"SGDRegressor\", \"SGDRegressor\"]}\n",
    "pd.DataFrame(algo_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecc5c2-a787-4090-80d8-d8ba299b93f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715e24f-eb41-4e43-9d2b-5c4e34bb34a2",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "What if your data is actually more complex than a simple straight line? Surprisingly, you can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers to each feature as new features, then train a linear model on this extended set of features. This technique is called *polynomial regression*.\n",
    "\n",
    "Let's look at an example. First, let's generate some nonlinear data, based on a simple *quadratic equation* (plus some noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdcb50c0-7c84-417b-a00a-11191c71578e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample size.\n",
    "m = 100\n",
    "\n",
    "# Generate 100 random numbers between 0 & 1, multiply by 6 & subtract 3 (shape: 100, 1)\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "\n",
    "# 0.5X^2 + X + 2 + random number between 0 & 1 for all values of X (shape: 100, 1)\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "# Plot (X, y) coordinates using blue points.\n",
    "plt.plot(X, y, \"bo\", markersize = 3)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d3f22-988e-412a-9473-7be7d007ef56",
   "metadata": {},
   "source": [
    "Clearly, a straight line will never fit this data properly. So let's use scikit-learn's `PolynomialFeatures` class to transform our training data, adding the square ($2^{nd}$-degree polynomial) of each feature in the training set as new features (in this case, there is just one feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2364448-ed38-4864-8591-310133162822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create a PolynomialFeatures instance.\n",
    "poly_features = PolynomialFeatures(degree = 2, include_bias = False)\n",
    "\n",
    "# Fit the PolynomialFeatures instance to X -- this squares the X values.\n",
    "X_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b242abe5-41f9-48b4-91d9-72a0e45b69f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22517707,  0.05070471])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907c8cb-1520-4b30-80fc-23f0cad5d7d0",
   "metadata": {},
   "source": [
    "`X_poly` now contains the original feature of X & the square of this feature. Now you can fit a `LinearRegression` model to this extended training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21996a6a-6477-4307-bcaf-233fee97e308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.87155764]), array([[1.03876663, 0.5461036 ]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a LinearRegression instance.\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Fit our LinearRegression instance to our extended training data. \n",
    "lin_reg.fit(X_poly, y)\n",
    "\n",
    "# Get the intercept & coefficients from our model. ([intercept], [[X, X^2]])\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45032fbd-92db-4a23-87b8-38fd62adda92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsdElEQVR4nO3deZxT5dn/8c/NMCwCoiyKihRQFFdQp+CoRVzxEUXFrQKidaG0VdTHymKtS92XtihaASlWwK2u+BMExTqVyqAOiooiQgVZROQBRNaRmbl+f9wTZoBZkkySk5N8369XXpPMnCRXMsm5zr2c63ZmhoiIZK96QQcgIiLBUiIQEclySgQiIllOiUBEJMspEYiIZDklAhGRLJe0ROCcG++c+945N6/S71o4595yzi0s/7lnsp5fRESik8wWwT+AM3b63XDgbTPrBLxdfltERALkknlCmXOuPfC6mR1efnsB0NPMVjrn9gEKzOzgpAUgIiK1qp/i59vbzFYClCeDvarb0Dk3CBgE0KRJk2M6d+6cohBFRDLDnDlz/s/MWte2XaoTQdTMbCwwFiAvL8+KiooCjkhEJCRKSmDAANycOd9Es3mqZw2tKu8Sovzn9yl+fhGRzPfYY/D881FvnupE8BpwWfn1y4DJKX5+EZHM9t13cOutMd0lmdNHnwUKgYOdc8udc1cC9wGnOecWAqeV3xYRkUQZOhR+/BF69476LkmdNZQoVY0RbNu2jeXLl7N169aAosoOjRo1om3btuTm5gYdiojUZuZM6NEDGjaEzz/HHXjgHDPLq+1uaTtYXJvly5fTrFkz2rdvj3Mu6HAykpmxZs0ali9fTocOHYIOR0RqUlICv/udvz58OBxwQNR3DW2Jia1bt9KyZUslgSRyztGyZUu1ukTC4G9/g88+gw4dYNiwmO4a2kQAKAmkgN5jkRBYuRJuucVfHzkSGjeO6e6hTgQiIgLceCNs2ABnnw19+sR8dyWCOsjJyaFr164cfvjhXHjhhWzevDnux7r88st58cUXAbjqqqv44osvqt22oKCAWbNmbb89evRoJkyYEPdzi0iIvf02PPusbwU8/HBcD6FEUAeNGzdm7ty5zJs3jwYNGjB69Ogd/l5aWhrX444bN45DDz202r/vnAgGDx7MwIED43ouEQmxn36qGCC+5RY/PhAHJYIE+cUvfsGiRYsoKCjgpJNOol+/fhxxxBGUlpZy00038fOf/5wjjzySMWPGAH5GzjXXXMOhhx5K7969+f77ipOse/bsSWS67LRp0zj66KPp0qULp5xyCkuWLGH06NH89a9/pWvXrsycOZPbb7+dhx56CIC5c+dy7LHHcuSRR3Leeeexbt267Y85bNgwunXrxkEHHcTMmTMB+Pzzz+nWrRtdu3blyCOPZOHChal820QkToWFUHD2n2HBAjjoIN89FKfQTh/dQbIGNKM8x6KkpIQ33niDM87wVbc/+OAD5s2bR4cOHRg7dizNmzfnww8/pLi4mOOPP57TTz+djz/+mAULFvDZZ5+xatUqDj30UK644oodHnf16tVcffXVvPvuu3To0IG1a9fSokULBg8eTNOmTfn9738PwNtvv739PgMHDmTUqFGceOKJ3Hrrrdxxxx2MHDlye5wffPABU6dO5Y477mDGjBmMHj2a6667jv79+/PTTz/F3YoRkdQpLIQrTlrMnOI7Afjid49xaMOGcT9eZiSCgGzZsoWuXbsCvkVw5ZVXMmvWLLp167Z93v2bb77Jp59+ur3/f/369SxcuJB3332XSy65hJycHPbdd19OPvnkXR5/9uzZ9OjRY/tjtWjRosZ41q9fzw8//MCJJ54IwGWXXcaFF164/e99+/YF4JhjjmHJkiUA5Ofnc/fdd7N8+XL69u1Lp06d4n9DRCQlCt4x/lx8DbuxhefcL1m86VSq70yuXWYkgoDOjo6MEeysSZMm26+bGaNGjaJXr147bDN16tRap2aaWUKnbzYsP2LIycmhpKQEgH79+tG9e3emTJlCr169GDduXJVJSUTSR1/3CgczlR9ozoiGf+WZnnV7PI0RJFmvXr14/PHH2bZtGwBfffUVmzZtokePHjz33HOUlpaycuVK3nnnnV3um5+fz7///W8WL14MwNq1awFo1qwZGzZs2GX75s2bs+eee27v/584ceL21kF1vv76azp27MiQIUPo06cPn376aZ1er4gk2YYNHPzYEADe73MPz/yrDfn5dXvIzGgRpLGrrrqKJUuWcPTRR2NmtG7dmldffZXzzjuPf/3rXxxxxBEcdNBBVe6wW7duzdixY+nbty9lZWXstddevPXWW5x99tlccMEFTJ48mVGjRu1wn6eeeorBgwezefNmOnbsyJNPPlljfM8//zyTJk0iNzeXNm3acGuMVQtFJMVuvRVWrIBu3ej18q8hp+4PGdqic/Pnz+eQQw4JKKLsovdaJE18/DHkldeQKyqCo46qcXPnXFRF59Q1JCISBqWlMGgQlJXBkCG1JoFYKBGIiITBo4/6VkDbtvCnPyX0oUOdCMLQrRV2eo9F0sCyZRVF5R57DJo1S+jDhzYRNGrUiDVr1mhHlUSR9QgaNWoUdCgiGaewEO691/+szdr+18LGjazp2TeuonK1Ce2sobZt27J8+XJWr14ddCgZLbJCmYgkTmEhnHKKLxXUoIGvG1fdFNAF973CwTMn8yPN6D77ESYWVr9tvEKbCHJzc7VqloikhcJCKCiAnj2j20kXFPgkUFrqfxYUVHO/9evZ995rABjBvSzZtl/129ZBaBOBiEg6iOXoPqJnT79t5D49e1az4fDhNPvxW953x/KEG1zztnWgRCAiUgdRH91Xkp/vE0aNrYiZM2H0aMjNpeGT47hjaU7ULY5YKRGIiNRB1Ef3O8nPr2GnvnWrP2cAYMQIuvY/jK51D7VaSgQiInUQ1dF9rO65B778Ejp3hptvTsAD1kyJQESkjmo8uo/VvHlw333++hNPQB3WGYhWaM8jEBHJOKWlcMUVsG0bDB5MYc4JUZ9rUBdqEYiIpIuHH4YPP4S2bfng/Pu3z0aqXx9+9SsYODA5g8VqEYiIpINFiyrKSIwezdsf7r59NlJxMYwZ46epJqN1oEQgIhI0M7j6atiyBfr3h969t89GiixSaFYxPTXRlAhERIL2xBN+D9+qFYwcCVTMRvr1r31CyMmJbXpqLDRGICISpKVL4fe/99dHjfLJoFxkNtLAgQmenroTJQIRkaCY+RPHNmyAc8+Fiy+ucrOETk+tgrqGRESC8o9/wPTpsOee8PjjFQMCKaZEICIShBUr4IYb/PWHH4Y2bQILRYlARCTVzGDwYFi/Hnr3hgEDAg1HiUBEJNUmToTXX4fmzf0JAgF1CUUEkgicczc45z53zs1zzj3rnNNaiCKSHVasgCFD/PWHH4b99gs2HgJIBM65/YAhQJ6ZHQ7kAL9MdRwiIikXOXFs/Xo46yw/LzQNBNU1VB9o7JyrD+wGfBtQHCIiqfPkk/DGG36WUBp0CUWkPBGY2QrgIWApsBJYb2Zv7rydc26Qc67IOVekBepFJPSWLq2YJTRqFOy7b7DxVBJE19CewDlAB2BfoIlzbpchczMba2Z5ZpbXunXrVIcpIpI4ZWW+vPSPP/oTx/r1CzqiHQTRNXQqsNjMVpvZNuBl4LgA4hARSY2//c0XDmrVKq26hCKCSARLgWOdc7s55xxwCjA/gDhERJLvq69g6FB/fcwY2GuvYOOpQhBjBO8DLwIfAZ+VxzA21XGIiCRdSQlcdpkvLz1gAPTtG3REVQpk1pCZ3WZmnc3scDO71MyKg4hDRKQqY8dCr17+Z508+CDMnu3PFRg1KiGxJYOqj4qIVDJ2rF8DAODN8vmMgwbF8UAffwy33uqvjx8Pe+yRiPCSQiUmREQqeemlmm9HJdIVVFIC114Lp5+ekNiSRYlARKSS88+v+XZURoyAL76Azp3hvvsSElcyqWtIRKSSSDfQSy/5JBBzt9CMGb6GUP36vrjcbrslPMZEUyIQEdnJoEFxjgusXQuXX+6v33Yb5OUlMqykUdeQiGS9wkK4917/M26RNQZWrPDrSg4fnrD4kk0tAhHJaoWFcMop8NNP0KCBPwE4rvWBJ0yAF16Apk1h0iTfNRQSahGISFYrKPBJoLTU/ywo2PHvUbUW/vtfuOYaf/3RR6FjxyRFmxzhSVkiIknQs6dvCURaBD17VvwtqtZCSQlceils3AgXXZQ2awzEQolARLJafr7fwRcU+CRQeUdfUADFxb54aHGxv71LIrjzTp8x2raF0aMDKShXWFh1/NFSIhCRrJefX/UOtGVLnwTA/2zZcqcNZs6Eu+7yO/8JE/yCMymWiDEOjRGIiFRjzRqoV76XrFfP395u3Tro399niBEj4KSTAomxtjGOaCgRiIhUo2dPaNgQcnL8z+3jB5G1h5ctg+7d4fbbA42xQQMf485jHNFS15CISDWqHT8YN86fetysGTzzDOTmpl+MMXBmlui4Ei4vL8+KioqCDkNExNcQysvzheUmTfLdQzWo60BuXTjn5phZrac3q0UgIhKtLVvg4ov9z4EDo0oCCTlZLck0RiAiEq0bboB58+Dgg+Gxx2rdPBEDuamgRCAiWaNONYVeeMGvOdywITz3nC8lUYtEDOSmgrqGRCQr1KmbZvFiP0sI4M9/hq5do7pbIgZyU0GJQESyQlXdNFHtmIuLfemI9evh3HPht7+N6XmrO1ktnahrSESyQtzdNEOHQlERtG/v1x4OoIREsqlFICJZIa5umpdfhkce8ecJPP98ICUkUkGJQESyRkzdNF9/DVdc4a8/+CB065a0uIKmriERkZ1t3QoXXlgxLjBkSNARJZUSgYjIzm64AT76CDp0yNhxgcqUCEREKps0ya8r0LAhvPhixo4LVKZEICIS8fnn8Otf++uPPAJHHx1sPCmiRCAiArBhA1xwAWze7JeejJxAlgWUCEREzPwMoS+/hMMOg8cfz/hxgcqUCEQkNOpUK6gmf/mLHw9o1syfO9CkSWqeN03oPAIRCYWklXT+979h2DB//amnKFxzEAX3Vpx0FpZS0nWhRCAioRB3raCafPutX1+gtBSGDaOwzXm77PST8rxpRl1DIhIKCS/pXFwM558Pq1bBySfDXXdVudMPSynpulCLQERCIeElnYcMgdmzYf/9/foC9etv3+lHWgSR5wlDKem6CGTNYufcHsA44HDAgCvMrNphGK1ZLCIJ9cQTMGiQP2nsvffgmGO2/ynINYYTLd3XLH4YmGZmFzjnGgC7BRSHiGSb2bPhmmv89bFjd0gCEI71AxIt5YnAObc70AO4HMDMfgJ+SnUcIpKFvv0W+vb1fT/XXOMXoJdABos7AquBJ51zHzvnxjnnmuy8kXNukHOuyDlXtHr16tRHKSKZZetWnwRWroQePfy5AwIEkwjqA0cDj5vZUcAmYPjOG5nZWDPLM7O81q1bpzpGEckkZjB4MLz/PrRr508ey80NOqq0EUQiWA4sN7P3y2+/iE8MIiLJ8cgj8NRT0LgxTJ4MOrjcQcoTgZl9Byxzzh1c/qtTgC9SHYeIZIm33oIbb/TX//EP6No1yGjSUlCzhq4Fni6fMfQ18KuA4hCRTLZggV9prLQU/vAHuOiioCNKS4EkAjObC9Q6t1VEJG7r1sHZZ/vlJs87D/70p6AjSlsqMSEiaSNhVT5LSvzR/8KF0KULTJgA9bS7q45KTIhIWkhYlU8zuO46mDED9toLXnsNmjZNeLyZRClSRNJCVQXf4jJqFPztb758xCuv+OmiUiMlAhFJC/FU+dylK2nKFLjhBn99/Hg47rgkRZtZ1DUkImkh1iqfO3clFY75lC6//SWUlcFtt0G/fqkIOyMoEYhI2qit4FvlyqCVu5JaFn9L+2vPgo0b4ZJLfCKIQSZVHI2HEoGIhMLOLYCRI/3P3OKN/D87i+brl/muoPHjY1p4PhuWoqyNxghEJBR2Hkxeswbenl7CnE4X09U+hgMP9OUjGjWq0+PGPUgdYmoRiEgo7LJ62IlG/sRrYcFUaNkSpk6FVq3q/rg9Ex15+lMiEJFQ2GUw+d0HYPRoP0108mTo1Ckxj5tl3UIQ0FKVsdJSlSLZp8YB3KefhgED/PXnn1cNoWqk+1KVIiLVqnEA9+234VfldSr/8hclgQTQYLGIpJ1qB3A/+cQXkNu2zZ84Fjl5TOpEiUBE0k6VZxkvXgxnnAEbNvhWwEMPBRxl5lDXkIiknV0GcA/4Ho4/Hb77Dk46SdVEE6zWROCcuwZ42szWpSAeERGg0lnGGzbASWfCokV+dbFXX/UzhVIgW844jqZF0Ab40Dn3ETAemG5hmGokIuFXXAx9+8KcOdCxI7zxBuy+e0qeOpvOOK61bWVmtwCdgL8DlwMLnXP3OOcOSHJsIpLNSkuhf/+KdQXefBPatEnZ02fTGcdRdbKVtwC+K7+UAHsCLzrnHkhibCKSrczg17+Gl16C5s1h+nQ4ILXHnvGUxQ6raMYIhgCXAf8HjANuMrNtzrl6wEJgaHJDFJGsM3w4/P3v0LgxvP66HxtIsWw64ziaMYJWQF8z+6byL82szDl3VnLCEpGsde+98MADUL8+vPginHBCYKHUVhY7U9SaCMzs1hr+Nj+x4YhIVnv4Ybj5Zl9GesIEOPPMoCPKCpqIKyLpYdw4uP56f/2JJ/wCM5ISSgQiErxnn4VBg/z1kSPhyisDDSfbKBGISLBefBEuvdTPFLrrLrjuuqAjyjpKBCISnMmTfRdQaakfG7j55qAjykpKBCISjKlT4cILoaQEbrrJtwZiWGtYEidUiaCw0M8sKywMOhIRqUrU39Fp03zpiG3bfFfQ/fcrCQQoNNVHs6nuh0i6qqkIW9Tf0WnT4NxzfR2h3/0O/vpXJYGAhaZFkE11P0TSUWRH/8c/+p87H/VH9R2tnAR++1sYNUpJIA2EJhFkU90PkXRU246+1u/oG2/smAQefVRJIE2Epmsom+p+iKSjyI4+0vWz846+xu/o5Ml+YHjbNiWBNOTCsLRAXl6eFRUVBR2GSNaLa6GWF16Afv387KDrrtthTCBbFn4JinNujpnl1bZdaFoEIhK/RO1wYy7C9vTTMHAglJXBsGF+SlGlJKAJIOlBiUAkwwW2wx07FgYP9mcM33or3H77Dt1BVY05KBEEI7DBYudcjnPuY+fc60HFIJINAplx9+c/+4VlzOCee+COO3YZE9AEkPQRZIvgOmA+kJoFSEWyVG2DvAll5nf6d9zhbz/6qD9XoAqaAJI+AkkEzrm2QG/gbuB/g4hBpC7CNMiZsh1uWRnccAM88gjUqwfjx8Nll9UaW7q/f9kgqBbBSPwSl82q28A5NwgYBNCuXbvURCUShTAOciZ9h7ttG/zqV35wuEEDeOYZOP/8JD6hJFLKxwjKl7f83szm1LSdmY01szwzy2vdunWKohOpXeU+961b/UJayZbWdbY2b/Ynij39NDRt6ovJKQmEShAtguOBPs65M4FGwO7OuUlmNiCAWERi1rOnX063tNR3iY8f72dIJuuIO61bIGvWQJ8+MGsWtGzpzx7++c+DjkpilPIWgZmNMLO2ZtYe+CXwLyUBCZP8fN8LEpkEU1qa3Jk4aVtn65tv/MLys2bB/vvDf/5TpySQ1q2eDBeaWkMi6WTgQGjUKDVTH9NymuUnn/iM+OWXcMQRfu/duXNcD1VYCL/5DZx0UvUF7SS5Aj2hzMwKgIJaNywrS3YoIjFJ5dTHtJtmOWOGHwP48Ucf0CuvwB57xPVQkW6vrVt9Nxvo5LIghOPM4i+/hBUrYL/9go5EZLtUTn1Mm2mW48f7E8VKSuCii/xIecOGcT9cpNsrkgScS6NWTxYJRyLYsgW6d4cpU6BLl6CjEQmlyLkPLVv6Md6YWheRMhF33eVvDx3qO/TrRde7XN15F5VPdsvJgSuuSO7Au1QtHNVHmzWzoo0b/dS055+HM88MOiSRUIl0wRQX+57WevX8gXxUM5C2boUrr/TnBtSrB4895msIxfjc1c16CtPJeWETbfXRcAwWH3QQ9O8PGzfC2Wf7MxdDkMBE0kWkCyYy3FZWFuUMpFWr4OSTfRJo2hReey2mJFD5uaub9ZSfDyNGKAkEKRyJwDmYONE3TcvKfE3zwYP92YwiCZLJ0xcjXTCRnpx69aLoi//sM98lW1gI7drBe+9B795xP3dazXqSHYSja6jywjTPPusncRcX+0/Uiy/6Tk+ROkjrk7YSpKYxgl26ZyZPhgEDfCu8e3d49VVo06bOz63un9TK3IVpLrkEOnb0p7QXFEC3bv5DesQRAQcmYZYNtfGrm3m0QxLMNb689G7aPfFH/8dLLoG//x0aN07Kc0t6CEfX0M66d4cPPoCjj4avv/afsJdeiukhMrkbQGKXzd0XkSTYsHQTTxVf7JOAc3Dffb5+UC1JQN+lDGBmaX855phjrEqbNpn162fmh47NbrnFrKSk6m0rmTXLrHFjs5wc/3PWrFrvkjCzZpndc09qn1Oik63/m1mzzA5ruNA+5XAzsG1Ndjd7/fWo7xvUd0lqBxRZFPvY8HUNVbbbbjBpkm8ZDB3q5zgXFfmjmBYtqr1bUN0A2dAPHWbZ2n2Rv3YKc3P7U794PVvaHUzj6a9GXS6iuhlBGg8Il3B2DVXmHNx4I0yb5kfBpk2DY46Bjz6q9i5BdQOkbfEwyU6lpX4lsbPPpv7G9XDuuTT+7IOYagbt/F1q2dIf7KhmULiEPxFEnHYazJkDeXmwZAkcd5wf5KpiVlSkdsudd6b2qDyb+6El9Wrsu1+9mh/y/wduvx0DuPtuP862e2wrx+78XVqzJnEHOxp7SKFo+o+CvlQ7RlCVLVvMrr66Ytzg0kvNNm6M/v5Jlq390Okok/8XNfbdv/eebW29nxnY97Sy3g3eTNh7kKgxA409JAZRjhFkTosgolEjGDsWnnrKjyFMnOhrpH/+edCRATqLMl1ExmsytQujym7IsjJ44AHo0YOGq1cwi+M4io+ZVnpawropE9XaVjdqamVeIogYONBPMT3kEJg/3yeDceNUmkKAcO9oouky2bkb8tQuq/1ZwcOGQWkp3/7yfzmjUQHf5bRNeDdlIg521I2aWuGeNVSbww7zyeB3v/Plcq++Gt56y7cYmjcPOjqdbRmgylUvw7SjiXbmWeU1DPo0/ReHXX0pfPutn0331FPse9ZZTE/jz1/arcGQ6aLpPwr6EtMYQXUmTjRr2tSPG7Rvb/af/1S7aSr6jtUHGrwwjhHcc4//zID/ec89NWxcXGx2001mzvk7nHCC2bJlKYtVgkfWjhFUZ8AAP6X0mGP8rKIePXwH8U6F61LVdxzmrolMEcbxmqi7TBYs8DPnHnzQT7G+/XZ45x1o2zZ1wUpoZE8iAOjUyS+0PXy4Hyu46y44/nj/pSmXqh10LH2gmkYnEbUOxpaVwaOPwlFH+enU7dvDzJlw221QP7N7gqUOomk2BH1JSNfQzv79b7N27XyTuVEjs5EjzUpLU9plE03XhLqQJGpLl5qdeuqOU6d/+CHoqCRAqGuoFj16wKef+tlFW7fC9dfDySeT32Zxyk42i6ZrQl1I2S2q1qAZPPmkr8A7Ywa0auVPDpswIa5JEWqBZqFoskXQl7q2CGo98n71VbO99vJHUU2amI0aZVZaWqfnTFSMahFkr6j+90uX2rruvSpaAX36mH33XXKfU0IDtQi8qAZ/zznHn3B20UWwaRNce61vMVQaOwgqxkSXw9DRXurF+57X2BosK4MxYyjpfBh7vD+dtezJr3InUTjsVdh777hjVQs0S0WTLYK+1KVFENN0OzOzl182a9PG36FhQ7M77/TT8JIo5hjjpKO91KvLe17tfefP91NBy1sBr3KOteHbhHx29BnJLGRLi6C2o62Yz1A87zz44gu4/HK/HOYf/+hnYPznP4kNvC4xxinTj/bSsbVTl/d8l9bgUVt9tdAuXfznce+9+erO57mk0SusztknIZ+doAoySsCiyRZBX6prEUR79DJrltngwf4S0xHOjBlmBx5Y0f961VVmq1fH8ADRiTu+OJ4nU4/20vW1JSyu6dN3/CxeeaXZ2rXbnyNsJ8ZJahBliyDwnXw0l+oSQbRdKnX6Mm7ZYvbHP5rl5vonatHCbMyYqFZCi0aqd2CZutNIVfdaPOr0ni9danbRRRUJ4JBDzAoKkvuckjGyIhFEuxNNyE5i/vwd52jn5Zm9914cD5SE2CRtWwRx27LF7K67zHbbzX84dtvN7P77oxqvyrj3QuIWbSII9RhBtP2ZCemD79wZ3nwT/vlP2G8/vyTm8cdDv36wbFncr0FVFhMjY/q2zeDll+HQQ+GWW2DzZrjwQj9uNXSo/5DUItPHgiTxnE8a6S0vL8+Kiorq9BgJrfS5cSPcf7+v41JcDI0b++Uyhw6FZs2CjU3C68MP/edo5kx/+/DD4ZFH4KSTYnqYwkJ/l0iF0nfe0ecqWznn5phZXq3bZUsiSIolS/zO/4UX/O299vKzOq66SnVdJHpff+1npz3zjL/dqpX/HA0aFNfnqLDQH1Rs2wa5uf4gQ4kgO0WbCELdNRS49u19V9HMmXDssfD99/Cb3/h1EP75T3/Sj2SEpExNXbXKn7zYubNPAg0b+oVjFi2C3/427oOJggLfLWTmf6prSGqjRJAIJ5zgq5q+8AIceCB89RVcfLEveT11qlZFC7mElyZfuxb+8Ac44ABfKbSkxNe8+vJLuO++WusDJfzcGcl6SgSJ4hxccIEf1Bszxg8oz53rlwc89lh4441QJ4R0PFkrVRI2+PrDD74cdPv2cM89vpzJWWfBJ5/4Nbbbt6/1IaJJShkzcC6pE83UokRegP2Bd4D5wOfAdbXdJyllqOMQ09zszZvNHnrIrHXriimnP/+52WuvJbWgXTLmj2f7dMQ6v/7vvzcbMcJs990rPgunn25WWBhzLJpuLLEgXc8jAPYBji6/3gz4Cji0pvukQyKIe2ewcaNPCJHqpmB2+OFmkyaZbduWHjHWQjuf2BJsZNuil78xu/56/8+I/O9PPtls5sw6xZHNSVlik7aJYJcAYDJwWk3bJDsRRPMlj2VnWOXjbdpk9pe/mO23X8VO4Wc/879bvz4hryNZO2ztfKI3a5ZZ94Yf29Oun20jp+J/3bt3wt44nTUs0QpFIgDaA0uB3av42yCgCChq165dct4li61eUUK227rVbNw4s06dKnYSu+9uduONZl9/nZLXEu9jp+vOJ+jYZs0yu/euEpt/36u2uONJ2/+v28ixeUdeYvbRR8EEFqWg3z9JnrRPBEBTYA7Qt7Ztk9kiqPORfryPV1LiF8Tp0aMiIThndtZZZtOmxT2OkG1f6qBbKx9MXW3D6z9oX9N++/9xA01spLveDm64OO3/D0G/f5JcaZ0IgFxgOvC/0WyfzESQ6C9CXI/34YdmAwaYNWhQkRQOOMDs3nvNVq6sW0AZLpDxi7Iyv+Z1v362Lafif7aIjvZW77/a+2/+ENVa1OmQsDX+k9nSNhEADpgAjIz2PukwRpCSx1u1yuzuu832378iIdSvb3buub71kOQFcsIopUe0y5b5/0+lbr0y5+yNemdan3r/z5o0Kol6MDldjsKriyVdEpXUTTonghMAAz4F5pZfzqzpPukwayilSkrMXn/d7JxzKg7XwKxVK7NrrzWbPdsflaZYuu4ckhrXDz+YPfmkrzxbr17F/2Kffcxuvtls8eKYnz/djsJ3jj+dEpXUTdomgnguWZcIKluxwuyBB8wOO6xiJwRm7dubDR/uByJTkBQq7xwaNkz+IjqB+vFHs2efNTv/fP9iI+95gwZmF1xgNmVKnab+pvuONt0SlcQv2kSgonNpqMpqpGbw8ccwcaKvY/TttxV3aN/eL7F53nn+DkkoeHfvvf5s1tJSf9s5aNQog85cXbUKpkyByZNh+nRfVTbixBOhf384/3xo0SIhT5fOFWcjZy9HqpdmzP84C6n6aEhF9SUsK/Nr1j73HLzyCnz3XcXf9twTevXypS1OP91XRE1gXFu3VlTKyMnxZQxGjEjIU6RWaSl89JHf6U+ZAu+/X/HCnPNrTZx/vr/sv3+wsQYgnROVRE+JIKQqH3lHtaMtK4PZs31CmDwZFi7c8e9HHgmnnur34scfX2tBs5oUFsKECTB+vI8vVEeLZrBggd+7vfMOzJjhi79FNGjg36Ozz4ZzzoF99w0sVJFEUSIIqTo3yxcu9BVPp06Fd9/1h/ARzkGXLvCLX/gH7d4dOnTwv48xxrQ/Wiwu9kf8s2b5gN97b8eWE/gutV69/OW006Bp00BCDUoo/o9SJ0oEIZawL+jWrf7B3nrLP2BRkV+tpLJWrSAvD7p29ZcuXXwp7TAtrLNuHcybB5995nf+c+b42yUlO263997+Te3Z02fbAw+MOQmGQTSfH40DZIdoE0GIvu3ZIz8/QV/KRo38moWRpQ43b4YPPvDjC++/7y+rV8O0af4S0aABHHQQHHIIdOrk6+YfcIA/gt53X7/sVaqtXw/ffOMvixb5ls9XX8H8+TsOnEc45xcIOu44/2Yed5x/TRm4468s2h18VaW1lQiylxJBNtltt4ojYvD95kuW+NlIc+f6yyefwNKl/oh63rxdH8M52Gcfv97C3nv7wejWrf1smubN/aVZM7+Oc+PGPhnl5EC9ev5nWZlvlZSU+O6bTZv8ZeNGX69/3Trfd796te/KWbnS7+jXr6/+dTVu7Hf6hx0GRx3lFwTq2jXrunog+h18ZPGaSMLQ4jXZTYkghdKuT9Y5P0bQoQP07bv91++/vZEvXlnACS2+oFO9//oj8P/+1x+Nf/ed3zFXdRSeTI0bw89+5i8HHOCP7jt1goMP9i2VnJzUxpOmot3BRxavSavPowRGYwQpEpY+2Vrj3LbNH6WvWOHXaI5c1q3zR+3r18OGDX58YssW/7O01LcEIlOh6tdn40+5rN+US5O9m7DHfk2hSRPYYw/fsmjRAlq29C2PNm38z5YtM75bJ1HS7oBDAqMxgjSTzD7ZaAcHo9k51Bpnbi60a+cvdYh3e7L5P3j7Ee2wEilhY0ySNZQIUiRZfbLRtDRiaY2kou84VQOVkfMewK8NX9VzxHP0rCNuyTRKBCmSrD7ZaHaqsex4U9F3nIpkU1joH/enn/ztJ5/055FVfj3xdNeFpYtPJBZKBCmUjCZ7NDvVWHe8ye5aiDXZxHMEXlCw4ykTVSXAeFommnYpmUiJIOSi2amm4wyRaJNNvEfgPXv64YxIi6CqBBhPy0TTLiUTKRFkgGh2qmEdQIz3CDw/329b0xhBPAkyHZOqSF1p+qjUKOiBUfXJi8RP00elztJhJ6wjcJHkUyKQaqXLwGhYu7VEwqJe0AFI+ooMjObkaGBUJJOpRSDVyvZumaDHR0RSRYlAapSt3TLpMD4ikirqGhKpQlXjIyKZSolAqlVY6NdQLiyM7veZJOjxkWx4jyV9qGtIqlRd10i2dJkEOT6SLe+xpA+1CKRK1XWNZFOXSX4+jBiR+p1wNr3Hkh6UCKRK1XWNBN1lkg30HkuqqWtIqlRd10i2TylNBb3HkmqqNSQikqGirTWkriERkSynRJDhNA1RRGqjMYIMpmmIIhINtQgyWLKmIaqVIZJZ1CLIYMlYVjFRrQwVdBNJH0oEGSwZ0xATsUaBuqxE0osSQYZLdPXQRLQy0mXBGxHxAhkjcM6d4Zxb4Jxb5JwbHkQMEp9IK+POO+M/kteZsyLpJeUtAudcDvAYcBqwHPjQOfeamX2R6lgkPnVtZejMWZH0EkTXUDdgkZl9DeCcew44B1AiyCLZuuCNSDoKIhHsByyrdHs50H3njZxzg4BB5TeLnXPzUhBbUFoB/xd0EEmUya8vk18b6PWF3cHRbBREInBV/G6XgkdmNhYYC+CcK4qmXkZY6fWFVya/NtDrCzvnXFRF2oIYLF4O7F/pdlvg2wDiEBERgkkEHwKdnHMdnHMNgF8CrwUQh4iIEEDXkJmVOOeuAaYDOcB4M/u8lruNTX5kgdLrC69Mfm2g1xd2Ub2+UKxHICIiyaOicyIiWU6JQEQky4UmETjn7nTOfeqcm+uce9M5t2/QMSWKc+5B59yX5a/vFefcHkHHlEjOuQudc58758qccxkzVS+TS6U458Y7577P1PN3nHP7O+fecc7NL/9sXhd0TIninGvknPvAOfdJ+Wu7o9b7hGWMwDm3u5n9WH59CHComQ0OOKyEcM6dDvyrfCD9fgAzGxZwWAnjnDsEKAPGAL83s9AvQF1eKuUrKpVKAS7JlFIpzrkewEZggpkdHnQ8ieac2wfYx8w+cs41A+YA52bC/88554AmZrbROZcL/Ae4zsxmV3ef0LQIIkmgXBOqOAktrMzsTTMrKb85G39uRcYws/lmtiDoOBJse6kUM/sJiJRKyQhm9i6wNug4ksXMVprZR+XXNwDz8VUPQs+8jeU3c8svNe4vQ5MIAJxzdzvnlgH9gVuDjidJrgDeCDoIqVVVpVIyYkeSbZxz7YGjgPcDDiVhnHM5zrm5wPfAW2ZW42tLq0TgnJvhnJtXxeUcADP7g5ntDzwNXBNstLGp7bWVb/MHoAT/+kIlmteXYaIqlSLpzTnXFHgJuH6nXodQM7NSM+uK713o5pyrsXsvrRamMbNTo9z0GWAKcFsSw0mo2l6bc+4y4CzgFAvLwE0lMfzvMoVKpYRcef/5S8DTZvZy0PEkg5n94JwrAM4Aqh34T6sWQU2cc50q3ewDfBlULInmnDsDGAb0MbPNQccjUVGplBArH1D9OzDfzP4SdDyJ5JxrHZl56JxrDJxKLfvLMM0aeglfUrUM+AYYbGYrgo0qMZxzi4CGwJryX83OlBlRAM6584BRQGvgB2CumfUKNKgEcM6dCYykolTK3cFGlDjOuWeBnvgyzauA28zs74EGlUDOuROAmcBn+H0KwM1mNjW4qBLDOXck8BT+c1kP+KeZ/anG+4QlEYiISHKEpmtIRESSQ4lARCTLKRGIiGQ5JQIRkSynRCAikuWUCERiVF65crFzrkX57T3Lb/8s6NhE4qFEIBIjM1sGPA7cV/6r+4CxZvZNcFGJxE/nEYjEobw8wRxgPHA1cFR5FVKR0EmrWkMiYWFm25xzNwHTgNOVBCTM1DUkEr//AVYCGbdwi2QXJQKRODjnuuJXJzsWuKF8xSuRUFIiEIlReeXKx/E17JcCDwIPBRuVSPyUCERidzWw1MzeKr/9N6Czc+7EAGMSiZtmDYmIZDm1CEREspwSgYhIllMiEBHJckoEIiJZTolARCTLKRGIiGQ5JQIRkSz3/wHIkYk2DM5y9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an array spanning [-3, 3] with 100 equivalent steps (shape: 100, 1)\n",
    "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "\n",
    "# Find the squares of X_new (shape: 100, 2).\n",
    "X_new_poly = poly_features.transform(X_new) \n",
    "\n",
    "# Use our linear regression model to predict y values (shape: 100, 1)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "\n",
    "# Plot our (X, y) coordinates in blue.\n",
    "plt.plot(X, y, \"bo\", markersize = 3)\n",
    "\n",
    "# Plot x_new against y_new (our polynomial regression line) in red.\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth = 2, label = \"Predictions\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18122cab-8e81-4068-a6c9-7ce77e1d5d91",
   "metadata": {},
   "source": [
    "Not bad. The model estimates $\\hat{y} = 0.54x^2 + 1.04x + 1.87$ when in fact the original function was $y = 0.5x^2 + x + 2 + Gaussian\\ noise$.\n",
    "\n",
    "Note that when there are multiple features, polynomial regression is capable of finding relationships between features (which is something a plain linear regression model cannot do). This is made possible by the fact that `PolynomialFeatures` also adds all combinations of features up to the given degree. For example, if there were two features *a* & *b*, `PolynomialFeatures` with `degree = 3` would not only add the features $a$, $a^2$, $a^3$, $b$, $b^2$ & $b^3$, but also the combinations $ab$, $a^2b$, & $ab^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4737c6c-c4dc-4693-a598-7b3cb51dc7c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450b1ca-1dfd-426b-b635-22deba0bb5bf",
   "metadata": {},
   "source": [
    "# Learning Curves\n",
    "\n",
    "If you perform high-degree polynomial regression, you will likely fit the training data much better than with plain linear regression; for example, if we apply a 300-degree polynomial model to the preceding training data, & compare the result with a pure linear model & a quadratic model ($2^{nd}$-degree polynomial). Notice how the 300-degree polynomial wiggles around to get as close as possible to the training instances.\n",
    "\n",
    "<img src = \"Images/Polynomial Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Of course, this high-degree polynomial regression model is severely overfitting the training data, while the linear model is underfitting it. The model that will generalise best in this case is the quadratic model. It makes sense since the data was generated using a quadratic model, but in general, you won't know what function generated the data, so how can you decide how complex your model should be? How can you tell that your model is overfitting or underfitting the data?\n",
    "\n",
    "In chapter 2, you used cross-validation to get an estimate of a model's generalisation performance. If a model performs well on the training data but generalises poorly according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex.\n",
    "\n",
    "Another way is to look at the *learning curves*: these are plots of the model's performance on the training set & the validation set as a function of the training set size (or the training iteration). To generate the plots, simply train the model several times on different sized subsets of the training set. The following code defines a function that plots the learning curves of a model given some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9a87d23-c0bb-41b4-b661-839628a008fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a function to plot the learning curves.\n",
    "def plot_learning_curves(model, X, y):\n",
    "    \n",
    "    # Split the data into training & test set (80:20)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    # Initiate empty lists to store error values.\n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    # For value in the training set.\n",
    "    for m in range(1, len(X_train)):\n",
    "        \n",
    "        # Fit the model to all values up to but not including that value.\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        \n",
    "        # Predict the y values for subsetted training data.\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        \n",
    "        # Predict the y values for the testing set.\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        \n",
    "        # Add mean squared error of predicted y values to list train_errors.\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        \n",
    "        # Add mean squared error of predicted labels to list val_errors.\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "        \n",
    "    # Plot the mean squared error of our training set across different subset sizes.\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth = 2, label = \"train\")\n",
    "    \n",
    "    # Plot the mean squared error of our training labels across different subset sizes.\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth = 3, label = \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a2784-195c-4e8a-9701-f72e891b2544",
   "metadata": {},
   "source": [
    "Let's look at the learning curves of the plain linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7a3855e-e017-41f6-a53b-39c4dea3bc40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAArLElEQVR4nO3de3gU9bkH8O+bEEggQLgECQm3KoJChUhEEC94qQpVwaot7REvPUdqRYXKUdFaCUdtPbVVD1rhYNV6Ky1KuRyk9YLgpQUxICAEFeSWAEKIJNwhl/f88e6ym80me5uws8v38zzz7MzuZObdze477/zmNzOiqiAiouSVEu8AiIioaTHRExElOSZ6IqIkx0RPRJTkmOiJiJIcEz0RUZILmehFJF1ElovIahFZJyJTgswjIjJVRDaKyBoRObtpwiUiokg1C2OeowAuUdUDIpIG4GMR+buqLvObZziAXp7hXADTPI9ERBRnISt6NQc8k2meIfAsq5EAXvHMuwxAlojkOBsqERFFI5yKHiKSCmAFgNMA/EFVPwmYJRdAid90qee5nQHLGQtgLAC0atVqYJ8+fVBSAuzeba/n5QGnnBLFuyAiOkmsWLFij6pmR/I3YSV6Va0BMEBEsgDMEZF+qrrWbxYJ9mdBljMDwAwAKCgo0KKiItx3H/DEE/b6nXcC998fSfhERCcXEdka6d9E1OtGVSsALAFwZcBLpQC6+k3nAdgRzjLT0nzjVVWRRENEROEIp9dNtqeSh4hkALgMwBcBs80HcJOn981gAJWquhNhYKInImpa4TTd5AB42dNOnwJglqouEJHbAUBVpwNYCGAEgI0ADgG4NdwAmOiJiJpWyESvqmsA5Ad5frrfuAIYF00ATPRERE0r7mfGMtETETUtJnoioiTHRE9ElOSY6ImIklzcE33z5r7xY8fiFwcRUbKKe6JnRU9E1LSY6ImIkhwTPRFRkmOiJyJKckz0RERJjomeiCjJMdETESU5JnoioiTHRE9ElORcleh5ZiwRkfPinuj9L4HAip6IyHmuSvQHD8YvDiKiZBX3RJ+TA7RsaeO7dgFbtsQ1HCKipBP3RJ+WBpx/vm96yZK4hUJElJTinugB4OKLfeOLF8cvDiKiZOSKRD9smG988WJANW6hEBElHVck+oEDgcxMGy8pATZvjm88RETJxBWJPi0NuOAC3zSbb4iInOOKRA+wnZ6IqKm4JtH7t9MvWcJ2eiIip7gm0efnA23a2Pj27cDGjfGNh4goWbgm0TdrBlx4oW+a/emJiJwRMtGLSFcRWSwi60VknYiMDzLPMBGpFJFVnuHhaIIJ7GZJRESxaxbGPNUAJqrqShFpDWCFiLyrqsUB832kqlfFEkzgAVlVQCSWJRIRUciKXlV3qupKz/h+AOsB5DZFMP37A1lZNv7NN8BXXzXFWoiITi4RtdGLSA8A+QA+CfLyEBFZLSJ/F5G+0QSTmlq3nZ7NN0REsQs70YtIJoDZACao6r6Al1cC6K6q/QE8A2BuA8sYKyJFIlJUVlYWdD3sT09E5KywEr2IpMGS/Ouq+rfA11V1n6oe8IwvBJAmIh2DzDdDVQtUtSA7OzvouvwTPfvTExHFLpxeNwLgBQDrVfXJBubp7JkPIjLIs9zyaAL67neBtm1tfPdua6snIqLohdPrZiiAMQA+F5FVnuceBNANAFR1OoDrAfxcRKoBHAYwWjW6WjwlBejeHVizxqa3b7ebkxARUXRCJnpV/RhAo50cVfVZAM86FVRubt1EX1Dg1JKJiE4+rjkz1l+uX+fN7dvjFwcRUTJgoiciSnJM9ERESc6Vib5LF984Ez0RUWxcmehZ0RMROcf1iX7HjvjFQUSUDFyZ6Dt2BJo3t/HKSuDgwfjGQ0SUyFyZ6EXYTk9E5BRXJnqA7fRERE5hoiciSnJM9ERESY6JnogoyTHRExElOdcmeva6ISJyhmsTPSt6IiJnuDbR+1f0O3cCtbXxi4WIKJG5NtFnZADt29t4TY3dVpCIiCLn2kQPsPmGiMgJTPREREmOiZ6IKMkx0RMRJTkmeiKiJOfqRM+TpoiIYufqRM+Knogodkz0RERJztWJvmNHIC3NxnlLQSKi6Lg60aek1G2n543CiYgi5+pED7D5hogoViETvYh0FZHFIrJeRNaJyPgg84iITBWRjSKyRkTOdipAJnoiotg0C2OeagATVXWliLQGsEJE3lXVYr95hgPo5RnOBTDN8xgzJnoiotiErOhVdaeqrvSM7wewHkBuwGwjAbyiZhmALBHJcSJAJnoiothE1EYvIj0A5AP4JOClXAAlftOlqL8xgIiMFZEiESkqKysLa51M9B6FhfGOgIgSVNiJXkQyAcwGMEFV9wW+HORPtN4TqjNUtUBVC7Kzs8NaL8+O9ZgyJd4REFGCCivRi0gaLMm/rqp/CzJLKYCuftN5ABzpDHnSV/SrVwMFBb5xL1b4RBSmcHrdCIAXAKxX1ScbmG0+gJs8vW8GA6hU1Z1OBOif6E+6WwoWFgIDBgArVtj0gAGACHD11azwiShs4VT0QwGMAXCJiKzyDCNE5HYRud0zz0IAmwBsBPA8gDucCjAjA2jXzsarq4Ewm/bxxRfAK68Au3Y5FUkcTJ4MdPXbUcrKsscFC+zxs89OeEhElHhCdq9U1Y8RvA3efx4FMM6poALl5gJ799p4SQlwyimNz79oETBiBHDsGJCeDvz0p8C99wI9ejRVhE3kq6/sDWdn2xbujjuAX//a9/rZntMVHn7YKn025xBREK4/MxYATj/dN/73vzc+76efAqNGWZIHgCNHgOeeA047DRgzBnjvPeDw4SYL1VnvvmuPl11m1f1jjwGqwKFDdedbtYpNOUTUoIRI9Dfc4Bt/9VXLdcEUFwPDhwMHDti094JoAFBTA7z2GvC971lT0KWXWnG8aBHwzTcNLzOu3nnHHr/3vbrVekaGPb75JtC2LTB/vk3/7Ge+9vzA6j7UNBElLdE4ZbiCggItKioKa97Dh625Zv9+m162DDg34LzbrVuBoUN9PXM6dAA++ggoLQV+8xtg8eLG19G+PdCvH3DNNcA991hLSFxVVdmb2L/fmm/y8uq+7k3UwSr5nBw7cv3887aMDh2Aiy4CVq60o9k1NfYB1ta64I0SUSREZIWqFkT0N4mQ6AFrZ3/pJRsfNw549lnfa3v3Wt7asMGmMzOB998HzjnHN8+yZVbRL1pkB2ob89Zb1sYfVx9/DFxwAdCnD7B+fej5RYAJE4CXX/Yd0Ahl6FDg6ad93TcLC1npR2DPHtuBWrnSCo19+2y7vG+fta6lpPgGEeDoUStajhyx8e7dgUGDbDjnHOA730me7e7hw/Y7W7vW3u/3v1/3nBiKXjSJPpxr3bjCTTf5Ev3MmcCTTwLNm9v0Qw/5knzz5sDcuXWTPAAMHmwDYFX/++8DH3xgX8R163zNPQDwj3+4INF72ue3D7ke/zcduPhioHfvEH/Ttm34SR4A/vlP+6D697eDH1Om1E30TPzHqdqx8UWL7LuzfLntaMWitNT+BV7t2gH5+b7hnHOAXr3cmfxraux34/0cvv0WKC+3YcsWYOPGul2hmze3Yu2++4CePcNfz6FDtsHYu9e3wUxJseW1b2/3rMjKsueoYQlT0dfW2hdk2zabnjsXGDnSehgWFPi+VK+/DvzkJ5HFomrN3T/8oU2fdVbdc5Pi4rzzULP0E5zVtQLFJa2RmgrcdZfl3bZtg8wfmJRF6h54CDZ9771W0VdVAa1a2Z1dnnzSdo/y84GWLev+zUmS+I8etUT15ZeWZIqLgSVL4nPCXm6uHU+67DJrfcvNBVJTI1tGRYXtFK5fb7+fHTusZW/HDjuOdf31wM03W+euhtTWAp98Ynu7//qXdXrwL47ClZoKjB5tRUt1tW0wqqtt+d6huto2FuvWAZs3hz5+lpICtGnj+4rX1gItWtixvd/+1r7aySSpm24A4MEHrb0dAK67DnjjDeD88+2LBwBXXGGFaTQV0P79VlHV1Njf79ljFUNcVFQAHTtivl6NkbVz6rzUqRPw+OP2w2y0igkn0asCd98NPPNMw3//1FPAtddaO0PgMpIk8ataUl+wwIaPP7bvQSgtWtjO0MCBwJlnWmXZpg3QurVtI4G6CaxFC+vum5Fh/7t16yxhLl9uj+HsjKWkWELu3NmG9u1tve3a2br377fvblmZDRs2WGeDUNLSgB/8ALjlFlt+ba19Lnv2AP/3f8C8ebZxCJeI9XTr1882kMuXh/+3TurbF5g9O4y94QSS9Il+/Xr7QQG26/bEE8B4z9Xx09KAzz+P7R967rm+L+S8eXZgNi7mzAF+8ANcnrUc71acE3SWNm3s+GyXLjb072+V0vF20MAkHGq6uNh+Ffn5wU/E8h7gnTrVVty1q7UtuLK7UmjeCnX2bPtfb9wY+m/atAGGDbMK+6KL7Lvo37MrFqrWzv/ZZ9bmv3KlNetUVjqz/KbQpYs1h555pjWhdOhgG56cHDu05O0cpmrNXb/+tT1GIiXFNhi5ub5qXdWOAXibivYFXnkrQGYm8MILtiErKrIYvHtohw7ZcPCgbTDHjLHTVfzPU2wKR49ak9f27b7jNt5jN0eOWBzeYiFQ0id6wHKL98/8C8z777dKNxb33gv87nc2PnGib/yEu+MOfDHtfZwBO2qckmItLL/9rbXrNiQlxXpi3nyznUvg/aGFzf8Dray0b/6ZZ9pGoCE//znwwAP2yzgRFX4M6/j2Wzsov3ChbUsbuzVljx6WrHr3tmHgQDs/rdkJPKpVXW0J/7337NjAmjVWYUeqRQt7D336WNL0Fgc5OVY8zZhhn0soHTtac+kVVwBDhtTvCBaOZcvsGFhNjX2WzZpZc05qqq/9XcT2Vvr2tXNo0tMbX+axY5bsRXzDnDnWaePIEd983tbJUFJTbSd27FjbO/NuDA4dsp9FRYUNlZW27poaXxNUSopt/L2D97QX71Bebhv0HTsar5G2bWt4Y3NSJPqpU31VvFeXLrbrnZkZW0wLFthlZCw+252Oi169cPfGu/AM7gZgP665c+1L+utfW4+jUFVMVpZtuMaPj6CNsqF2/sOHLdNcfbVlPG9ffX/e5yP9PoVK3KGOPfjNs327dToqm78UzS8agubNbc9vyxZr3must1VmJnD55cBVV9m5GJ07R/Y2TpRjx4Ddu605Ztcua+6pqLDHffvsfWRnW1Lu2NFa3Hr0CN2uv2aN9cZdutQ+Xm/SbdbM9nSvvdY6aUV6fCCeVq2y4w9ffx3vSCK3YYNtlIOJJtFDVeMyDBw4UKOxa5dqaqqqfR1tmDkzqkXVs3evqogtMyVFdd8+Z5Ybkc2bdR8ytTUqj7+/996rO0tNjeru3aqrVqkuXKj6hz+oXnxx3c/EO3TurDptmuqxY/a3R4+qbt2q+umntty//U31pZdUn3lG9dVXVd99V3XtWtXyctXahyfXXbFd7aLu9OjRvg8NUL31VtX1633zTA5YRuB04DIDAaolJRbgJZfYdK9eqqNGqT74oOqrr+o25OkdFxdr82bVQT+DhoaOHVX/4z/sMzxypPEw6gl8H+RKFRWq117r+5937ap6yy2qr7yiunq16tdfq+7cafPNndvw78jpISXFYjnvPNUrrlC95hrVH/5QdcwY1dtuU92xo+H3BKBII8y3CVfRA1Z1vfWWjV90kZ0M5VQXtPx8qwQAO7B75ZXOLDdszz+P58Z+hnF4DoDtbhcXh/f+tm61M4dfegnYtKnua5062e5leXn4oeTlWSV33XV20Dv1kcL61fXkycFP2urTxxpGhw61gyf799twxRXA9OlWgu7da+1R+/f7dse8Ffzhw/j2ub/gw/+ch23ohhJ0xTZ0w250QiscRBYqkIUK7EdrzMSPUYXmId9Ps2aK/D6HMXTt/+KaRRNwwYViTTHB9ipC7Wkk6YHpZKRqv+k2bcI7V+Hzz22vefly2yNs2dKGjAzr8ZaV5Tvwnp7ua3pKTbV1VVX5BhHbo/Yuo3Vr28vKy/N1D4/USVHRq6p+9plq27aq3burfvFF1IsJ6u67fVvdBx5wdtkh1dZq7UXD9AysOx7Ds89GvpiqKtUZM1S7dHGuAsnOVh07VvWDD2yPQlWDV+c/+5keS2upxeij/8DlugXdwl/JXXepAlp74UU6A/+hbVARcZznYqk+gYn6OO7T/8JD+hD+S5/ARP0IQ/UQ0n0ztmtnpdSvfmXTixerLl+uum6d6rZtWm9Pw/teS0pU/+d/9Piu1t69vvdOdAIgioo+IRO9qmptrSU0p82e7csF553n/PIbNHmyKqDv4ZLj62+NSt036bGoF3nwoOrjj6tmZfneU0qKNecMGGC7qSNH2u7i7ber/uhHqhddpHr66aotWzacTLt3t1aTxYvt83rmGdVJk1RH48/63e+qNk+tqjN/L3ypd+BZnYORugXd9CjS7IVhw+ouWEQ3oYdeincjTvDnnaf69tv2vVBAdeLE2Lduo0apPvWU6rJlNt2tgY1W+/b2OG+e6uHDvv8nUROIJtEnZNNNUyors2YOwI6aV1Q03M0pasF282trgYEDce2qhzEX1wIA7rwzeBf3SB0+bAekOnSw9xbOAbXqajtzePZs68EQTl/sSHTqZF3msj97G5k9stF6yxqkoQp/xk9wCL6jx6diIy772Wno1g3o1g3o/M4rOHzDTcd7Phw8CAx54CJcWPuBb5c8nHMIJkywrkyRatHC2g5nz7ZeAMG67vTrZ6dcl5XZEVEvNu+QA06appumduaZvmJt0SIHFhjOAcg//1k/wTmaAt8BRf9jmvFUU6P60UdW9bdrF14xnJenOniwajoORVVMp6So3nuv6qEHHwkdYKQHfENN19bac9dcEzw4z96XqtrR7RUrbDo/v/68ffrYEd8//an+elj1UxRwMjXdNKXbb/f9Th9+OMaFvfWWLejOO1Wvu0516FCbrqz0zXP0qO7v0U9Pw1fH1zt8eIzrbSJHjqjOmaN6ww2qBQWqV11lbfdTpqj+8Y+q//ynr9laVfXwLx/RRYusaWfIENWczH2aktJ4ku/b15rLHRNNz59QG4Ngy/BuABobBg9W/c1v7FgAEz9FgYneITNn+n6Xw4bFsCBv225jw+TJqsOH61hMP/5U69aqmzY59W7cp6rKjmkuW2ZdG2fNUn3hBdWnn1b96/WzIu/qGKtgCTbSbqDB5j96VPXf/73x/7+3r19JSejEH2qaTgpM9A7Zvt33O0xPj6KPtWrD1d1PfuIbz8hQnTZN5+HqOrO8/LLT74hiFmlSbWgv4Yc/bDzxX3CBnYvw6KM2vWqV6qFDwZcZbB1M/g2LdEMZzWcZ6d+EU2QEYKJ30Gmn+X57f/qT77cWttpa657iXYg/QPXmm1UB3YlTtCN2H5/thhs8PUcosQX7sfp/D2pqbPryyxtP/ICdkOb9Lo0cqXrjjb72xYcftqagp59Wff55e666uuE43LohaIq9F/+/2bXLPpviYms2W7u2/u8ynL24UHFE0hy3aZPNP3u29RmvqGh4vXVWwUTvmJ/+tO5vLS3NmlcnTLAzTd99V3XLlrq/qTo8B+i+yD5f16Cf7trlm7fqV1N03c+f0Zn4kV6IJcfX0QWlWn7f4yfsPdIJ1lgiqa626TFjQif+UEPbtqpXX6365JM27U0ggesMFlM4CdXpjYd3o+cv0mMkDW1Yp061vrcNfVadO9v5FPffb9NPPWV9kgsLbfqNN1S/+sr3420ojtWrfct4+WVL3EeOBJ9/9Wrbsw88xR/w9XZoxMmX6JuwOlmwILzfVKtWds5NYMKvmvSQ3oFn68ybkqLaqZNqixbBl/XOO032dsiNwu0ddOyYnRnorf5eftmuewHYMhpLZN4hL88SGqB6333WNDR1qk3Pm6f64Yeqn38ePMlE2osp3Or8s8/sGEW650S2M86wPRzvcY0xY1R/8ANf3E88YWfs7d8fPIbqaru2x+OP1z9HI9ahWTPVQYNs/Omn7WQ5b0WekxP8b7yXBrngAttV9z8bM9whSI47uRL9t98G/1I6pLbWvv8//alq796h/x/f/76vI82BA6pXtXo/ov/nffc12VuhRBFrt1DvcxMmxJbUeva0psUXXvA1cTz/vPV3HTXKps85x7pH9exp0z/+sXW9mjXLptetU9282ddksmOH6saNqmvW2LQTiXjQINXLLrONAVD3zMDA4YEHgn9+DSXfCy+0xxEjrHdEqFjatVMdN87G/ftnBxsyMuws8C1b6sZUW2sX3gmR106eRH/4sJ2TD9iulveKXU1Y4ZeVWeJ/5BH7DQwd6gvBO5xxhurHH6sW9K3bd7xr1/r9z/Py7Ds0aZLq2ze+0mRxUwKLpt06WDKrrrbmhzlzbPrSS2NPsk4MmZmq48erbthg06tX2670tGk2/dJLtuHwdlEeODC85ebn+7rOhfpswp3es8dOBQdUzz47+Hr9z69QtVPTAdWbbgpv/obiqPfyyZDoG+rNcuONIT8gp1VX+5rlGiskvAdXjx5VLS21K0MSNYlY9woA1ZUrfc0lgcP119vjsmVWnXuTdEMnl4UavPFGEuehQ74Y3nlH9c03bTqwT3LgMiI9nyLcDWkkywxnHex14/Haa74vin/3GMBX3Z9Ar71Wv909BdU67Y41JzwWojpiPWs4nHlinQ4WlxNxO9HVMdQ8oYrLSOcPw8mT6B980JdRf/nLxisFf03YtLN8ue9qkS1xQOdn3BBlB3yiEyia5iCnLzkRDbd0G22KjUkITZLoAbwIYDeAtQ28PgxAJYBVnuHhcFYcU6L3HhC67jrfc0uW2HOpqaqffNLQJxT5uiL4QpWXq74y+i3djO52cIooGcSazNySlJNEUyX6CwGcHSLRL4h0xTEl+l69LPTVqwM/ARtOP90OhHgdPuw7IzXS7i2RtrENHmzzvPFGZOshIgpDNIk+5K2OVfVDEekR0SUxm9KRI3bN3ZQUu3Owv1/+0q6pW1wMTJpkt6Q/cAD4/e998/z2tzZMnhz6krELF9rjPffYdXtramz6f//X7qDUp4/dXWn8eLub71df+e6yPHy4I2+XiChWYV2P3pPoF6hqvyCvDQMwG0ApgB0A/lNV1zWwnLEAxgJAt27dBm7dujXyiNesAfr3tyT/5Zf1X1+50u5mXF1t0127AiUldjHzbdt88/3lL8CPfhR8HYWFwW+PF4kwPlciokhFcz36FAfWuxJAd1XtD+AZAHMbmlFVZ6hqgaoWZGdnR7e24mJ7PPPM4K+ffbZV614lJcCQIXYDSAD47/+2x5tuspvNBqvqCwuB227zTf/+93aTCu9dQPr3Dx2niA280QQRxVnIpptQVHWf3/hCEXlORDqq6p5Ylx1UqEQfrBpfuhSYNs02APfeC5SWWtIeNQrYt69+Mt682e6wnZJid3665x7fa3fd5bt7uKpvHv87Dgfe0YiIKI5iTvQi0hnALlVVERkE20sojzmyhqzztAo1lugLC+0ec5mZ9ZMwADz1FLBzJ/Dmmza9Z0/dW749+qg1/YwZY7eN9+e/t+BdbqjbyhMRxVHIRC8iM2E9azqKSCmAyQDSAEBVpwO4HsDPRaQawGEAozWchv9oeSv6vn0bn6+V576jwZLwI4/4kjwAeJuRJk+25P7yy3Zj1V/9CujVq+7fBlb//om/seeIiOIksW4OfuyY3am7ttYq9oyMxucPdTPm0lI7WAsAv/gF8OSTwC23WKK/5RZrviEicpFoDsbG3HRzQm3YYF0cTz01dJIHQh8Izcuzx2bNrDmnbdu61TwRURJwotfNiROqfT4akycDU6fauHfDcMst9dvmiYgSVGJV9OG2z0eisNB6yKxcCfzxj/bcQw85t3wiojhLrIo+VNfKaE2Z4kvyANCzJ/vAE1HSSMyK3ulE73/Qln3giSjJJE5FX1Vl15IB7BozREQUlsRJ9Bs3WrLv2dPXR74psA88ESWZxEn0TdVsE4jt8kSUZJjoiYiSHBM9EVGSS5xE7z1Zysk+9EREJ4HESPTV1b6bjLDHDRFRRBIj0W/aZBc0A4DWreMbCxFRgnF/oi8sBHr39k3zzk1ERBFx/5mxhYXAGWcAo0fbNM9aJSKKiPsregD49tt4R0BElLASI9Hv3WuPQ4fGNw4iogSUGIneW9Ffc0184yAiSkCJkei9FX27dvGNg4goASVWom/fPr5xEBEloMRI9N6mG1b0REQRS4xEz4qeiChqiZHoWdETEUUtMRI9D8YSEUXN/Yn+2DHg4EEgNZXXuSEiioL7E71/NS8S31iIiBKQ+xO9t32eB2KJiKISMtGLyIsisltE1jbwuojIVBHZKCJrRORsRyNk+zwRUUzCqej/BODKRl4fDqCXZxgLYFrsYflhRU9EFJOQiV5VPwTQ2OUjRwJ4Rc0yAFkikuNUgKzoiYhi40QbfS6AEr/pUs9z9YjIWBEpEpGisrKy8JbOk6WIiGLiRKIP1hUm6N1BVHWGqhaoakF2dnZ4S+fJUkREMXEi0ZcC6Oo3nQdghwPLNazoiYhi4kSinw/gJk/vm8EAKlV1pwPLNazoiYhiEvKesSIyE8AwAB1FpBTAZABpAKCq0wEsBDACwEYAhwDc6miEPBhLRBSTkIleVX8c4nUFMM6xiAKxeyURUUzcf2YsK3oiopi4P9Gzoiciiom7E70qK3oiohi5O9EfPAhUVQEZGUB6eryjISJKSO5O9KzmiYhi5u5Ez/Z5IqKYuTvRs6InIooZEz0RUZJzd6Jn0w0RUczcnehZ0RMRxczdiZ4VPRFRzNyd6FnRExHFzN2JnhU9EVHM3J3oWdETEcXM3YmeFT0RUczcnehZ0RMRxczdiZ4VPRFRzNyb6GtqgMpKG8/KimsoRESJzL2JvrLSrkffpg2QmhrvaIiIEpZ7E723fZ7NNkREMXF/oueBWCKimLg30fNALBGRI9yb6FnRExE5wr2JnhU9EZEj3JvoWdETETnCvYmeFT0RkSPcm+hZ0RMROSKsRC8iV4rIlyKyUUQmBXl9mIhUisgqz/BwzJGxoicickSzUDOISCqAPwD4HoBSAJ+KyHxVLQ6Y9SNVvcqxyFjRExE5ImSiBzAIwEZV3QQAIvIXACMBBCZ6Z3kreiZ6IvJTVVWF0tJSHDlyJN6hNKn09HTk5eUhLS0t5mWFk+hzAZT4TZcCODfIfENEZDWAHQD+U1XXxRQZL4FAREGUlpaidevW6NGjB0Qk3uE0CVVFeXk5SktL0bNnz5iXF04bfbBPUgOmVwLorqr9ATwDYG7QBYmMFZEiESkqKytrfK2s6IkoiCNHjqBDhw5Jm+QBQETQoUMHx/Zawkn0pQC6+k3nwar241R1n6oe8IwvBJAmIh0DF6SqM1S1QFULsrOzG17jsWPAoUN21crWrcMIkYhOJsmc5L2cfI/hJPpPAfQSkZ4i0hzAaADzAwLqLJ6oRGSQZ7nlUUflfyD2JPiHEhE1pZCJXlWrAdwJ4G0A6wHMUtV1InK7iNzume16AGs9bfRTAYxW1cDmnfCxayUROa2w0JHFVFRU4Lnnnov470aMGIGKigpHYohUWP3oVXWhqp6uqqeq6mOe56ar6nTP+LOq2ldV+6vqYFX9V0xRsWslETltyhRHFtNQoq+pqWn07xYuXIisON0tL5xeNyeet6Ivj771h4hOApE27YY7fyMNEpMmTcLXX3+NAQMGIC0tDZmZmcjJycGqVatQXFyMUaNGoaSkBEeOHMH48eMxduxYAECPHj1QVFSEAwcOYPjw4Tj//PPxr3/9C7m5uZg3bx4yMjIiey8RcOclELwV/caN8Y2DiCjA448/jlNPPRWrVq3CE088geXLl+Oxxx5DcbGdWvTiiy9ixYoVKCoqwtSpU1EepGDdsGEDxo0bh3Xr1iErKwuzZ89u0pjdWdGH6npJRAQ0WnnXIxLZ/GEaNGhQnb7uU6dOxZw5cwAAJSUl2LBhAzp06FDnb3r27IkBAwYAAAYOHIgtW7Y4Hpc/91X0hYXAxIm+aREbHDqQQkTkpFatWh0fX7JkCd577z0sXboUq1evRn5+ftC+8C1atDg+npqaiurq6iaN0X0VfWEh8PHHwKJFNt0EW2AiOglNnuzIYlq3bo39+/cHfa2yshLt2rVDy5Yt8cUXX2DZsmWOrDNW7kv0ALBhQ7wjIKJk41CrQIcOHTB06FD069cPGRkZOOWUU46/duWVV2L69Ok466yz0Lt3bwwePNiRdcZKYunuHouCggItKiqq/8Lhw0DLlkCzZsCkScAjj5z44IjItdavX48zzjgj3mGcEMHeq4isUNWCSJbjvjZ6b0+b73yHSZ6IyAHuS/RffWWPp58e3ziIiJIEEz0RUZJjoiciSnJM9ERESY6Jnogoybkr0X/7LbBnj3Wv7NIl3tEQEcUsMzMz3iG4LNF7T5Q6/XTecISIyCHuOjOWzTZEFIGmrAcbOpf0/vvvR/fu3XHHHXcAAAoLCyEi+PDDD7F3715UVVXh0UcfxciRI5suuAi5t6InInKh0aNH469//evx6VmzZuHWW2/FnDlzsHLlSixevBgTJ05EvK46EIw7K/peveIbBxFRA/Lz87F7927s2LEDZWVlaNeuHXJycvCLX/wCH374IVJSUrB9+3bs2rULnTt3jne4ANya6FnRE1EY4lU0X3/99XjzzTfxzTffYPTo0Xj99ddRVlaGFStWIC0tDT169Ah6eeJ4cU+iV2WiJ6KEMHr0aNx2223Ys2cPPvjgA8yaNQudOnVCWloaFi9ejK1bt8Y7xDrck+h37gQOHgQ6dADat493NEREDerbty/279+P3Nxc5OTk4N/+7d9w9dVXo6CgAAMGDECfPn3iHWId7kn0rOaJKIF8/vnnx8c7duyIpUuXBp3vwIEDJyqkBrmn1w0TPRFRk2CiJyJKckz0RJRw3NRHvak4+R6Z6IkooaSnp6O8vDypk72qory8HOnp6Y4szx0HY6urgU2bbPy00+IbCxG5Wl5eHkpLS1FWVhbvUJpUeno68vLyHFmWOxL91q1AVRXQtatduZKIqAFpaWno2bNnvMNIKGE13YjIlSLypYhsFJFJQV4XEZnqeX2NiJwdcqE7dvjG2WxDRNRkQlb0IpIK4A8AvgegFMCnIjJfVYv9ZhsOoJdnOBfANM9jw3buBFassPElS+yRiZ6IyHHhNN0MArBRVTcBgIj8BcBIAP6JfiSAV9SOjiwTkSwRyVHVnY0uuaCg7jQvZkZE5LhwEn0ugBK/6VLUr9aDzZMLoE6iF5GxAMYCQAcAAWkeuOce4J57sAvYWQrsCHw5TjoC2BPvIMLAOJ2VCHEmQowA43Ra70j/IJxEH+zS/oH9msKZB6o6A8AMABCRoj2q9XK924hIkTJOxzBO5yRCjADjdJqIFEX6N+EcjC0F0NVvOg/1q+1w5iEiojgIJ9F/CqCXiPQUkeYARgOYHzDPfAA3eXrfDAZQGbJ9noiIToiQTTeqWi0idwJ4G0AqgBdVdZ2I3O55fTqAhQBGANgI4BCAW8NY94yooz6xGKezGKdzEiFGgHE6LeI4JZlPIyYiIjdd64aIiJoEEz0RUZKLS6IPdUmFeBGRF0Vkt4is9XuuvYi8KyIbPI/t4hxjVxFZLCLrRWSdiIx3aZzpIrJcRFZ74pzixji9RCRVRD4TkQWeadfFKSJbRORzEVnl7WLn0jizRORNEfnC8z0d4rY4RaS353P0DvtEZIIL4/yF5/ezVkRmen5XEcd4whO93yUVhgM4E8CPReTMEx1HA/4E4MqA5yYBWKSqvQAs8kzHUzWAiap6BoDBAMZ5Pj+3xXkUwCWq2h/AAABXenpkuS1Or/EA1vtNuzXOi1V1gF9/bzfG+T8A/qGqfQD0h32uropTVb/0fI4DAAyEdSKZAxfFKSK5AO4GUKCq/WCdYUZHFaOqntABwBAAb/tNPwDggRMdRyPx9QCw1m/6SwA5nvEcAF/GO8aAeOfBrkPk2jgBtASwEnZGtevihJ33sQjAJQAWuPX/DmALgI4Bz7kqTgBtAGyGp6OHW+MMiO1yAP90W5zwXXGgPayH5AJPrBHHGI+mm4Yul+BWp6jnnADPY6c4x3OciPQAkA/gE7gwTk9zyCoAuwG8q6qujBPA0wDuA1Dr95wb41QA74jICs/lRAD3xfkdAGUAXvI0hf1RRFrBfXH6Gw1gpmfcNXGq6nYAvwOwDXY5mUpVfSeaGOOR6MO6XAI1TkQyAcwGMEFV98U7nmBUtUZt1zgPwCAR6RfnkOoRkasA7FbVFfGOJQxDVfVsWLPnOBG5MN4BBdEMwNkApqlqPoCDcEdzUlCek0CvAfBGvGMJ5Gl7HwmgJ4AuAFqJyI3RLCseiT7RLpewS0RyAMDzuDvO8UBE0mBJ/nVV/ZvnadfF6aWqFQCWwI5/uC3OoQCuEZEtAP4C4BIReQ3uixOqusPzuBvWnjwI7ouzFECpZ+8NAN6EJX63xek1HMBKVd3lmXZTnJcB2KyqZapaBeBvAM6LJsZ4JPpwLqngJvMB3OwZvxnWJh43IiIAXgCwXlWf9HvJbXFmi0iWZzwD9qX9Ai6LU1UfUNU8Ve0B+y6+r6o3wmVxikgrEWntHYe11a6Fy+JU1W8AlIiI9wqLl8Iuae6qOP38GL5mG8BdcW4DMFhEWnp+95fCDmxHHmOcDjKMAPAVgK8B/DJeBzuCxDUT1hZWBatM/h12ReVFADZ4HtvHOcbzYU1dawCs8gwjXBjnWQA+88S5FsDDnuddFWdAzMPgOxjrqjhhbd+rPcM67+/GbXF6YhoAoMjzv58LoJ1L42wJoBxAW7/nXBUngCmwAmktgFcBtIgmRl4CgYgoyfHMWCKiJMdET0SU5JjoiYiSHBM9EVGSY6InIkpyTPREREmOiZ6IKMn9PzwDTAn4GlufAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a LinearRegression instance.\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Plot the learning curve.\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "plt.axis([0, 80, 0, 3])\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14817e88-90f4-42ec-aed1-432276d818f1",
   "metadata": {},
   "source": [
    "This deserves a bit of explanation. First, let's look at the performance on the training data: when there are just one or two instances in the training set, the model can fit them perfectly, which is why the curve starts at zero. But as new instances are added to the training set, it becomes impossible for the model to fit the training data perfectly, both because the data is noisy & because it is not linear at all. So the error on the training data goes up until it reaches a plateau, at which point adding new instances to the training set doesn't make the average error much better or worse. Now let's look at the performance of the model on the validation data. When the model is trained on very few training instances, it is incapable of generalising properly, which is why the validation error is initially quite big. Then as the model is shown more training examples, it learns & thus the validation error slowly goes down. However, once again, a straight line cannot do a good job modeling the data (recall our X & y values were created for polynomial regression), so the error ends up at a plateau, very close to the other curve.\n",
    "\n",
    "These learning curves are typical of an underfitting model. Both curves have reached a plateau; they are close & fairly high.\n",
    "\n",
    "Now let's look at the learning curves of a $10^{th}$-degree polynomial model on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a82216e7-ee64-4f17-be41-988fe39edee5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a polynomial regression pipeline (PolynomialFeatures() & LinearRegression())\n",
    "polynomial_regression = Pipeline([(\"poly_features\", PolynomialFeatures(degree = 10, include_bias = False)), \n",
    "                                  (\"lin_reg\", LinearRegression())])\n",
    "\n",
    "# Plot the learning curve\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "plt.axis([0, 80, 0, 3])\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9550c-221a-467b-bf0a-0dad158aa829",
   "metadata": {},
   "source": [
    "These learning curves look a bit like the previous ones, but there are two very important differences:\n",
    "\n",
    "* The error on the training data is much lower than with the linear regression model.\n",
    "* There is a gap between the curves. This means that the model performs significantly better on the training data than on the validation data, which is the hallmark of an overfitting model. However, if you used a much larger training set, the two curves would continue to get closer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcce1e-d442-41a9-8de1-0bd435f0552c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e097d36-8b7a-45f0-941c-cf4f5b784100",
   "metadata": {},
   "source": [
    "# Regularised Linear Models\n",
    "\n",
    "As we saw in previous chapters, a good way to reduce overfitting is to regularise the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularise a polynomial model is to reduce the number of polynomial degrees.\n",
    "\n",
    "For a linear model, regularisation is typically achieved by constraining the weights of the model. We will now look at ridge regression, lasso regression, & elastic net, which implement three different ways to constrain the weights.\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "*Ridge regression* is a regularised version of linear regression: a *regularisation term* equal to $\\alpha\\sum^{n}_{i = 1}\\theta^2_i$ is added to the cost function. This forces the linear algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularisation term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model's performance using the unregularised performance measure.\n",
    "\n",
    "The hyperparameter $\\alpha$ controls how much you want to regularise the model. If $\\alpha = 0$, then ridge regression is just linear regression. If $\\alpha$ is very large, then all weights end up very close to zero & the result is a flat line going through the data's mean. Here is the ridge regression cost function.\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha\\frac{1}{2}\\sum^{n}_{i = 1}\\theta^2_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fc8c8-b666-4cfe-95a6-363ce9643972",
   "metadata": {},
   "source": [
    "Note that the bias term $\\theta_0$ is not regularised (the sum starts at $i = 1$, not 0). If we define $w$ as the vector of feature weights ($\\theta_1$ to $\\theta_n$), then the regularisation term is simply equal to $\\frac{1}{2}(||w||_2)^2$ where $||w||_2$ represents the $l_2$ norm of the weight vector. For gradient descent, just add $\\alpha w$ to the MSE gradient vector.\n",
    "\n",
    "<img src = \"Images/Ridge Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "The above figure shows several ridge regression models trained on some linear data using different $\\alpha$ values. On the left, plain ridge models are used, leading to linear predictions. On the right, the data is first expanded using `PolynomialFeatures(degree = 10)`, then it is scaled using a `StandardScaler`, & finally the ridge models are applied to the resulting features: this is polynomial regression with ridge regularisation. Note how increasing $\\alpha$ leads to a flatter (i.e., less extreme, more reasonable) predictions; this reduces the model's variance but increases its bias.\n",
    "\n",
    "As with linear regression, we can perform ridge regression either by computing a closed-form equation or by performing gradient descent. The pros & cons are the same. The following function shows the closed-form solution (where $A$ is the $(n + 1)(n + 1)$ *identity matrix* except with a 0 in the top-left cell, corresponding to the bias term).\n",
    "\n",
    "$$\\hat{\\theta} = (X^TX + \\alpha A)^{-1}X^Ty$$\n",
    "\n",
    "Here is how to perform ridge regression with scikit-learn using a closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73eb8fd5-ac49-4729-8f80-f7945cd0cdb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.93602614]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create a Ridge regression instance.\n",
    "ridge_reg = Ridge(alpha = 1, solver = \"cholesky\")\n",
    "\n",
    "# Fit the Ridge instance to our data.\n",
    "ridge_reg.fit(X, y)\n",
    "\n",
    "# Use our ridge regression model to predict the y value for X value = 1.5.\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd917cff-db77-44a0-9314-86df744d51fd",
   "metadata": {},
   "source": [
    "& using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fa82ed2-2f2e-4894-96af-678c79fe1544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.91950276])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open a SGDRegressor instance using ridge regularisation.\n",
    "sgd_reg = SGDRegressor(penalty = \"l2\")\n",
    "\n",
    "# Fit our model instance to the data.\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "# Use our stochastic gradient descent model with ridge regularisation to predict the y value for X value = 1.5.\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7a797-15e1-4b1f-af54-df6d5874fb01",
   "metadata": {},
   "source": [
    "The `penalty` hyperparameter sets the type of regularisation term to use. Specifying \"`l2`\" indicates that you want SGD to add a regularisation term to the cost function equal to half the square of the $l_2$ norm of the weight vector: this is simply ridge regression.\n",
    "\n",
    "## Lasso Regression\n",
    "\n",
    "*Least Absolute Shrinkage & Selection Operation Regression* (simply called *Lasso regression*) is another regularisation version of linear regression: just like ridge regression, it adds a regularisation term to the cost function, but it uses the $l_1$ norm of the weight vector instead of half the square of the $l_2$ norm.\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha\\sum^{n}_{i = 1}|\\theta_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1779dd9-2667-4411-bfa7-0aaa2d984eb4",
   "metadata": {},
   "source": [
    "The below figure shows several lasso regression models trained on some linear data using different $\\alpha$ values.\n",
    "\n",
    "<img src = \"Images/Lasso Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "An important characteristic of Lasso regression is that it tends to completely eliminates the weights of the least important features (i.e, set them to zero). For example, the dashed line in the right plot (with $\\alpha = 10^{-7}$) looks quadratic, almost linear: all the weights for the high-degree polynomial features are equal to zero. In other words, Lasso regression automatically performs feature selection & outputs a *sparse model* (i.e., with few nonzero feature weights).\n",
    "\n",
    "<img src = \"Images/Lasso vs Ridge Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "You can get a sense of why this is the case. On the top-left plot, the background contours (ellipses) represent an unregularised MSE cost function ($\\alpha = 0$), & the white circles show the batch gradient descent path with that cost function. The foreground contours (diamonds) represent the $l_1$ penalty, & the triangles show the BGD path for this penalty only ($\\alpha \\rightarrow \\infty$). Notice how the path first reaches $\\theta_1 = 0$, then rolls down the gutter until it reaches $\\theta_2 = 0$. On the top-right plot, the contours represent the same cost function plus an $l_1$ penalty with $\\alpha = 0.5$. The global minimum is on the $\\theta_2 = 0$ axis. BGD first reaches $\\theta_2 = 0$, then rolls down the gutter until it reaches the global minimum. The two bottom plots show the same thing but uses an $l_2$ penalty instead. The regularised minimum is closer to $\\theta = 0$ than the unregularised minimum, but the weights do not get fully eliminated.\n",
    "\n",
    "The Lasso cost function is not differentiable at $\\theta_i = 0$ (for $i = 1, 2, ..., n$), but gradient descent still works fine if you use a *subgradient vector* $g$ instead when any $\\theta_i = 0$. Below shows a subgradient vector equation you can use for gradient descent with the Lasso cost function.\n",
    "\n",
    "$$g(\\theta, J) = \\triangledown_{\\theta}MSE(\\theta) + \\alpha \\Bigg(\\begin{split}\n",
    "sign(\\theta_1) \\\\\n",
    "sign(\\theta_2) \\\\\n",
    "{\\vdots} \\\\\n",
    "sign(\\theta_n)\n",
    "\\end{split} \\Bigg) where\\ sign(\\theta_i) = \\Biggl\\{\\begin{split}\n",
    "-1\\ if\\ \\theta_i < 0 \\\\\n",
    "0\\ if\\ \\theta_i = 0 \\\\\n",
    "1\\ if\\ \\theta_i > 0\n",
    "\\end{split}$$\n",
    "\n",
    "Here is a small example using the `Lasso` class. Note that you could instead use an `SGDRegressor(penalty = \"l1\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6131778f-551c-4369-8e80-61172bf8dd95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.89164602])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create a Lasso regression instance.\n",
    "lasso_reg = Lasso(alpha = 0.1)\n",
    "\n",
    "# Fit the Lasso instance to the data.\n",
    "lasso_reg.fit(X, y)\n",
    "\n",
    "# Using our Lasso regression model, predict the y value if X = 1.5.\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f285a7-2f96-482c-8f88-9056907a7c66",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "Elastic net is a middle ground between ridge regression & lasso regression. The regularisation term is a simple mix of both ridge & lasso's regularisation terms, & you can control the mix ratio $r$. When $r = 0$, elastic net is equivalent to ridge regression, & when $r = 1$, it is equivalent to lasso regression.\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + r\\alpha\\sum^{n}_{i = 1}|\\theta_i| + \\frac{1 - r}{2}\\alpha\\sum^{n}_{i = 1}\\theta^2_i$$\n",
    "\n",
    "So when should you use plain linear regression (i.e., without any regularisation), ridge, lasso, & elastic net? It is almost always preferable to have at least a little bit of regularisation, so generally you should avoid plain linear regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer lasso or elastic net since they tend to reduce the useless features' weights down to zero as we have discussed. In general, elastic net is preferred over lasso since lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "\n",
    "Here is a short example using scikit-learn's `ElasticNet` (`l1_ratio` corresponds to the mix ratio $r$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd91cd7e-dec9-4997-8abb-bbe927843a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.89060329])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create an ElasticNet regression instance.\n",
    "elastic_net = ElasticNet(alpha = 0.1, l1_ratio = 0.5)\n",
    "\n",
    "# Fit the ElasticNet regression instance to the data.\n",
    "elastic_net.fit(X, y)\n",
    "\n",
    "# Use our elastic net regression model to predict the y value if X = 1.5.\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eaaaa1-c719-445a-8b10-6e53d29cc20c",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "A very different way to regularise iterative learning algorithms such as gradient descent is to stop training as soon as the validation error reaches a minimum. This is called *early stopping*. The below figure shows a complex model (in this case a high-degree polynomial regression model) being trained using batch gradient descent. As the epochs go by, the algorithm learns & its prediction error (RMSE) on the training set naturally goes down, & so does it prediction error on the validation set. However, after a while, the validation error stops decreasing & actually starts to go back up. This indicates that the model has started to overfit the training data. With early stopping, you must stop training as soon as the validation error reaches the minimum. It is such a simple & efficient regularisation technique that some call it \"beautiful free lunch\".\n",
    "\n",
    "<img src = \"Images/Early Stopping.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Here is a basic implementation of early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de69e59e-8ffd-4602-9751-39ce4d574d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Sample size.\n",
    "m = 100\n",
    "\n",
    "# Create our dataset (Polynomial Regression).\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "\n",
    "# Split our data into training & validation (test) sets (split 50:50).\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size = 0.5, random_state=10)\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a pipeline that generates polynomials & their interaction features, & scales the data.\n",
    "poly_scaler = Pipeline([(\"poly_features\", PolynomialFeatures(degree = 90, include_bias = False)), \n",
    "                        (\"std_scaler\", StandardScaler())])\n",
    "\n",
    "# Run the training & validation set through the pipeline.\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "# Create a batch GD regressor instance.\n",
    "sgd_reg = SGDRegressor(max_iter = 1, tol = -np.infty, warm_start = True, \n",
    "                       penalty = None, learning_rate = \"constant\", eta0 = 0.0005)\n",
    "\n",
    "# Set tolerances.\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "\n",
    "# For 1000 iterations.\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    # Fit our batch GD regressor to our training set.\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
    "    \n",
    "    # Use our batch GD regressor to predict the y label from our prepared X label.\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    \n",
    "    # Compute the MSE between the actual y label & our predicted y label.\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    \n",
    "    # Find the minimum RMSE, the iteration where we found the minimum RMSE, & the best model.\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d472a30-8ffe-406c-a21d-b59ed00e1af3",
   "metadata": {},
   "source": [
    "Note that with `warm_start = True`, when the `fit()` method is called, it just continues training where it left off instead of restarting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c83de-e498-4c03-9a3a-596250f0c228",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08a307-e761-4f03-9548-8fad53a19fbe",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "As we discussed before, some regression algorithms can be used for classification as well (& vice versa). *Logistic regression* is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled \"1\"), or else it predicts that it does not (i.e., it belongs to the negative class, labeled \"0\"). This makes it a binary classifier.\n",
    "\n",
    "## Estimating Probabilities\n",
    "\n",
    "So how does it work? Just like a linear regression model, a logistic regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the linear regression model does, it outputs the *logistic* of this result.\n",
    "\n",
    "$$\\hat{p} = h_{\\theta}(x) = \\sigma(x^T\\theta)$$\n",
    "\n",
    "The logistic -- noted $\\sigma(.)$ -- is a *sigmoid funcrion* (i.e., S-shaped) that outputs a number between 0 & 1. It is defined as the following function.\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$\n",
    "\n",
    "<img src = \"Images/Logistic Regression.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Once the logistic regression model has estimated the probability $\\hat{p} = h_{\\theta}(x)$ that an instance $x$ belongs to the positive class, it can make its predictions $\\hat{y}$ easily.\n",
    "\n",
    "$$\\hat{y} = \\Biggl\\{ \\begin{split}\n",
    "0\\ if\\ \\hat{p} < 0.5 \\\\\n",
    "1\\ if\\ \\hat{p} >= 0.5\n",
    "\\end{split}$$\n",
    "\n",
    "Notice that $\\sigma(t) < 0.5$ when $t < 0$, & $\\sigma(t) \\leq 0.5$ when $t \\leq 0$, so a logistic regression predicts 1 if $x^T\\theta$ is positive & 0 if it is negative.\n",
    "\n",
    "## Training & Cost Function\n",
    "\n",
    "Good, now you know how a logistic regression model estimates probabilities & makes predictions. But how is it trained? The objective of training is to set the parameter vector $\\theta$ so that the model estimates high probabilities for positive instances ($y = 1$) & low probabilities for negative instances ($y = 0$). This idea is captured by the cost function shown below, for a single training instance x.\n",
    "\n",
    "$$c(\\theta) = \\Biggl\\{ \\begin{split}\n",
    "-log(\\hat{p})\\ if\\ y = 1 \\\\\n",
    "-log(1 - \\hat{p})\\ if\\ y = 0\n",
    "\\end{split}$$\n",
    "\n",
    "This cost function makes sense because $-log(t)$ grows very large when $t$ approaches 0, so the cost will be large if the model estimates a probability close to 0 for a positive instance, & it will also be very large if the model estimates a probability close to 1 for a negative instance. On the other hand, $-log(t)$ is close to 0 when $t$ is close to 1, so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to 1 for a positive instance, which is precisely what we want.\n",
    "\n",
    "The cost function over the whole training set is simply the average cost over all training instances. It can be written in a single expression called the *log loss*, demonstrated below.\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum^{m}_{i = 1}[y_ilog(\\hat{p}_i) + (1 - y_i)log(1 - \\hat{p}_i)]$$\n",
    "\n",
    "The bad news is that there is no known closed-form equation to compute the value of $\\theta$ that minimises the cost function (there is no equivalent of the normal equation). But the good news is that this cost function is convex, so gradient descent (or any other optimisation algorithm) is guaranteed to find the global minimum (if the learning rate is not too large & you wait long enough). The partial derivatives of the cost function with regards to the $i^{th}$ model parameter $\\theta_i$ is given:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_i} = \\frac{1}{m}\\sum^{m}_{i = 1}(\\sigma(\\theta^Tx_i) - y_i)x_i$$\n",
    "\n",
    "For each instance, the expression computes the prediction error & multiplies it by the $i^{th}$ feature value, & then it computes the average over all training instances. Once you have the gradient vector containing all the partial derivatives you can use it in the batch gradient descent algorithm. That's it: you now know how to train a logistic regression model. For stochastic GD, you would of course take one instance at a time, & for mini-batch GD, you would use a mini-batch at a time.\n",
    "\n",
    "## Decision Boundaries\n",
    "\n",
    "Let's use the iris dataset to illustrate logistic regression. This is a famous dataset that contains the sepal & petal length & width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, & Iris-Virginica.\n",
    "\n",
    "<img src = \"Images/Iris Flowers.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Let's try to build a classifier to detect the Iris-Virginica type based only on the petal width feature. First let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9abd5c0-1e33-4c9e-80d6-7006d6af9a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "\n",
    "X = iris[\"data\"][:, 3:]\n",
    "y = (iris[\"target\"] == 2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89c29a-3f65-4d5d-9acc-f445fe5b53f3",
   "metadata": {},
   "source": [
    "Now let's train a logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "519b1249-dfd1-42d8-98f0-4cfa8b51e193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression instance.\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the LogisticRegression instanct to the iris dataset (petal width only).\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14b900-950b-4ecc-aa27-7b81daeb54a3",
   "metadata": {},
   "source": [
    "Let's look at the model's estimated probabilities for flowers with petal widths varying from 0 to 3 cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23e30ea0-7d93-48f8-8f2e-df250a08a7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate array of values between 0 & 3 with 1000 steps.\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "\n",
    "# Use our logistic regression model to predict the probability that an iris with an X_new petal width (cm) would be an Iris Virginica.\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "# Get the petal width (cm) where the probability of a sample being Iris Virginica vs Not, is equal.\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]\n",
    "\n",
    "# Plot the petal widths for samples that were not Iris Virginica as blue squares.\n",
    "plt.plot(X[y == 0], y[y == 0], \"bs\")\n",
    "\n",
    "# Plot the petal widths for samples that were Iris Virginica as green triangles.\n",
    "plt.plot(X[y == 1], y[y == 1], \"g^\")\n",
    "\n",
    "# Plot a black dotted line for our decision boundary, from (decision_boundary, -1) to (decision_boundary, 2).\n",
    "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth = 2)\n",
    "\n",
    "# PLot the probability curve for samples that were Iris Virginica as a green solid line.\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth = 2, label = \"Iris virginica\")\n",
    "\n",
    "# PLot the probability curve for samples that were not Iris Virginica as a blue dashed line.\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth = 2, label = \"Not Iris virginica\")\n",
    "\n",
    "# Plot the decision boundary as a black dotted line.\n",
    "plt.text(decision_boundary + 0.02, 0.15, \"Decision  boundary\", color = \"k\", ha = \"center\")\n",
    "\n",
    "# Plot the decision arrows blue & green, anything to the left of the decision boundary is classifed \"Not Iris Virginica\", anything to the right \"Iris Virginica\".\n",
    "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width = 0.05, head_length = 0.1, fc = 'b', ec = 'b')\n",
    "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width = 0.05, head_length = 0.1, fc = 'g', ec = 'g')\n",
    "plt.xlabel(\"Petal width (cm)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe08dde-3c0e-4ecd-b020-5c0c74889371",
   "metadata": {},
   "source": [
    "The petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares) generally have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of overlap. Above about 2 cm, the classifier is highly confident that the flower is an Iris-Virginica (it outputs a high probability to that class), while below 1 cm, it is highly confident that it is not an Iris-Virginica (high probability for the \"Not Iris-Virginica\" class). In between these extremes, the classifier is unsure. However, if you ask it to predict the class (using the `predict()` method rather than the `predict_proba()` method), it will return whichever class is the most likely. Therefore, there is a *decision boundary* at around 1.6 cm where both probabilities are equal to 50%: if the petal width is higher than 1.6 cm, the classifier will predict that the flower is an Iris-Virginica, or else it will predict that it is not (even if it is not very confident):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db07af2-9832-470c-9fb6-ad443aec6dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use our logistic regression model to predict the class of an two irises with petal widths 1.7cm & 1.5cm.\n",
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc0ba0-86a3-4678-96f0-905d8299da2d",
   "metadata": {},
   "source": [
    "<img src = \"Images/Linear Decision Boundary.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "The above figure shows the same dataset but this time displaying two features: petal width & length. Once trained, the logistic regression classifier can estimate the probability that a new flower is an Iris-Virginica based on these two features. The dashed line represents the points where the model estimates a 50% probability: this is the model's decision boundary. Note that it is a linear boundary. Each parallel line represents the points where the model outputs a specific probability, from 15% (bottom left) to 90% (top right). All the flowers beyond the top-right line have an over 90% chance of being Iris-Virginica according to the model.\n",
    "\n",
    "Just like the other linear models, logistic regression models can be regularised using $l_1$ or $l_2$ penalties. Scikit-learn actually adds a $l_2$ penalty by default.\n",
    "\n",
    "## Softmax Regression\n",
    "\n",
    "The logistic regression model can be generalised to support multiple classes directly, without having to train & combine multiple binary classifiers. This is called *softmax regression* or *multinomial logistic regression*.\n",
    "\n",
    "The idea is quite simple: when given an instance $x$, the softmax regression model first computes a score $s_k(x)$ for each class $k$, then estimates the probability of each class by applying the *softmax function* (also called the *normalised exponential*) to the scores. The equation to compute $s_k(x)$ should look familiar, as it is just like the equation for linear regression prediction.\n",
    "\n",
    "$$s_k(x) = x^T\\theta^{(k)}$$\n",
    "\n",
    "Note that each class has its own dedicated parameter vector $\\theta^{(k)}$. All these vectors are typically stored as rows in a *parameter matrix*.\n",
    "\n",
    "Once you have computed the score of every class for the instance $x$, you can estimate the probability $\\hat{p}_k$ that the instance belongs to class $k$ by running the scores through the softmax function: it computes the exponential of every score, then normalises them (dividing by the sum of all the exponentials). The scores are generally called logits or log-odds (although they are actually unnormalised log-odds).\n",
    "\n",
    "$$\\hat{p}_k = \\sigma(s(x))_k = \\frac{e^{s_k(x)}}{\\sum^{k}_{i = 1} e^{s_i(x)}}$$\n",
    "\n",
    "* $k$ is the number of classes.\n",
    "* $s(x)$ is a vector containing the scores of each class for the instance $x$.\n",
    "* $\\sigma(s(x))_k$ is the estimated probability that the instance $x$ belongs to class $k$ given the scores of each class for that instance.\n",
    "\n",
    "Just like the logistic regression classifier, the softmax regression classifier predicts the class with the highest estimated probability (which is simply the class with the highest score).\n",
    "\n",
    "$$\\hat{y} = argmax\\ \\sigma(s(x))_k = argmax\\ s_k(x) = argmax ((\\theta^{(k)})^Tx)$$\n",
    "\n",
    "* The *argmax* operator returns the value of a variable that maximises a function. In this equation, it returns the value of *k* that maximises the estimated probability $\\sigma(s(x))_k$.\n",
    "\n",
    "Now that you know how the model estimates probabilities & make predictions, let's take a look at training. The objective is to have a model that estimates a high probability for the target class (& consequently a low probability for the other classes). Minimising the cost function show below, called the *cross-entropy*, should lead to this objective because it penalises the model when it estimates a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimate class probabilities match the target classes.\n",
    "\n",
    "$$J(\\Theta) = -\\frac{1}{m}\\sum^{m}_{i = 1}\\sum^{K}_{k = 1} y_{ik}log(\\hat{p}_{ik})$$\n",
    "\n",
    "* $y_{ik}$ is the target probability that the $i^{th}$ instance belongs to the class $k$. In general, it is either equal to 1 or 0, depending on whether the instance belongs to the class or not.\n",
    "\n",
    "Notice that when there are just two classes ($K = 2$), this cost function is equivalent to the logistic regression's cost function (log loss).\n",
    "\n",
    "The gradient vector of this cost function with regards to $\\theta^{(k)}$ is given:\n",
    "\n",
    "$$\\triangledown_{\\theta^{(k)}}J(\\Theta) = \\frac{1}{m} \\sum^{m}_{i = 1}(\\hat{p}_{ik} - y_{ik})x_{i}$$\n",
    "\n",
    "Now you can compute the gradient vector for every class, then use gradient descent (or any other optimisation algorithm) to find the parameter matrix $\\Theta$ that minimises the cost function.\n",
    "\n",
    "Let's use the softmax regression to classify the iris flowers into all three classes. Scikit-learn's `LogisticRegression` uses one-versus-all by default when you train it on more than two classes, but you can set the `multi-class` hyperparameter to `\"multinomial\"` to switch it to softmax regression instead. You must also specify a solver that supports softmax regression, such as the `\"lbfgs\"` solver. It also applies $l_2$ regularisation by default, which you can control using the hyperparameter `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d297d9c-5c9d-42dd-ae9d-720b43857d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, multi_class='multinomial')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get petal length & width.\n",
    "X = iris[\"data\"][:, (2, 3)]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "# Create a softmax regression instance.\n",
    "softmax_reg = LogisticRegression(multi_class = \"multinomial\", solver = \"lbfgs\", C = 10)\n",
    "\n",
    "# Fit the softmax regressor instance to our data.\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b7967-a0a6-4e87-ba28-a5cd2716f533",
   "metadata": {},
   "source": [
    "So the next time you find an iris with 5 cm long & 2 cm wide petals, you can ask your model to tell you what type of iris it is, & it will answer Iris-Virginica (class 2) with 94.2% probability (or Iris-Versicolor with 5.8% probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdbb20d9-f9a5-4e28-8821-e4fadc3e6d82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use our softmax regression model to predict the class of an iris with a petal length of 5cm & petal width of 2cm.\n",
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d60e7598-d172-4447-be87-1adf3dd82319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use our softmax regression model to predict the probability our iris (petal length: 5cm, petal width: 2cm) belongs to each class.\n",
    "softmax_reg.predict_proba([[5, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db54c5-b22d-41f3-bbb4-6204731ff7b5",
   "metadata": {},
   "source": [
    "<img src = \"Images/Softmax Regression Decision Boundaries.png\" alt = \"Alternative text\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "The above figure shows the resulting decision boundaries, represented by the background colours. Notice that the decision boundaries between any two classes are linear. The figure also shows the probabilities for the Iris-Versicolor class, represented by the curved lines (e.g., the line labeled with 0.45 represents the 45% probability boundary). Notice that the model can predict a class that has an estimated probability below 50%. For example, at the point where all decision boundaries meet, all classes have an equal estimated probability of 33%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
