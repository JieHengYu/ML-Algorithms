{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4263a81d-757f-4a87-a81b-bb42a710a593",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks\n",
    "\n",
    "Birds inspired us to fly, burdock plants inspired velcro, & nature has inspired countless more inventions. It seems only logical then, to look at the brain's architecture for inspiration on how to build an intelligent machine. This is the logic that sparked *artificial neural networks* (ANNs): an ANN is a machine learning model inspired by the networks of biological neurons found in our brains. However, although planes were inspired by birds, they don't flap their wings. Similarly, ANNs have gradually become quite different from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying \"units\" rather than \"neurons\"), lest we restrict our creativity to biologically plausible systems.\n",
    "\n",
    "ANNs are at the very core of deep learning. They are versatile, powerful, & scalable, making them ideal to tackle large & highly complex machine learning tasks such as classifying billions of images (e.g., Google images), powering speech recognition services (e.g., Apple's siri), recommending the best videos to watch to hundred of millions of users every day (e.g., Youtube), or learning to beat the world champion at the game of Go (DeepMind's AlphaGo).\n",
    "\n",
    "The first part of this lesson introduces artificial neural networks, starting with a quick tour of the very first ANN architectures & leading up to *Multilayer Perceptrons* (MLP), which are heavily used today. In the second part, we will look at how to implement neural networks using the popular Keras API. This is a beautifully designed & simple high-level API for building, training, evaluating, & running neural networks. But don't be fooled by its simplicity: it is expressive & flexible enough to let you build a wide variety of neural network architectures. In fact, it will probably be sufficient for most of your use cases. & should you ever need extra flexibility, you can always write custom Keras components, using its lower-level API.\n",
    "\n",
    "But first, let's go back in time to see how artificial neural networks came to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23996ba3-f447-4c31-ac0c-386cb86826e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788ae2c-56dc-4ef8-a1ea-86776e120b65",
   "metadata": {},
   "source": [
    "# From Biological to Artificial Neurons\n",
    "\n",
    "Surprisingly, ANNs have been around for a while: they were first introduced back in 1943 by the neurophysiologist Warren McCulloch & mathematician Walter Pitts. In their landmark paper, \"A Logical Calculus of Ideas Immanent in Nervous Activity\", McCulloch & Pitts presented a simplified computational model of how biological neurons might work together in animal brains to perform complex computations using *propositional logic*. This was the first artificial neural network architecture. Since then, many other architectures have been invented.\n",
    "\n",
    "The early successes of ANNs led to the widespread belief that we would soon be conversing with truly intelligent machines. When it became clear in the 1960s that this promise would go unfulfilled (at least for quite a while), funding flew elsewhere, & ANNs entered a long winter. In the early 1980s, new architectures were invented & better training techniques were developed, sparking a revival of interest in *connectionism* (the study of neural networks). But progress was slow, & by the 1990s other power machine learning techniques were invented, such as support vector machines. These techniques seemed to offer better results & stronger theoretical foundations than ANNs, so once again the study of neural networks was put on hold.\n",
    "\n",
    "We are now witnessing yet another wave of interest in ANNs. Will this wave die out like the previous ones did? Well, here are a few good reasons to believe that this time is different & that the renewed interest in ANNs will have a much more profound impact on our lives:\n",
    "\n",
    "* There are now a huge quantity of data available to train neural networks, & ANNs frequently outperform other ML techniques on very large & complex problems.\n",
    "* The tremendous increase in computing power since the 1990s now makes it possible to train large neural networks in a reasonable amount of time. This is in part due to Moore's law (the number of components in integrated circutes has doubled about every 2 years over the last 50 years), but also thanks to the gaming industry, which has stimulated the production of powerful GPU cards by the millions. Moreover, cloud platforms have made this power accessible to everyone.\n",
    "* The training algorithms have improved. To be fair, they are only slightly different from the ones used in the 1990s, but these relatively small tweaks have had a huge positive impact.\n",
    "* Some theoretical limitations of ANNs have turned out to be benign in practice. For example, many people thought that ANN training algorithms were doomed because they were likely to get stuck in local optima, but it turns out that this is rather rare in practice (& when it is the case, they are usually fairly close to the global optimum).\n",
    "* ANNs seem to have entered a virtuous circle of funding & progress. Amazing products based on ANNs regularly make the headline news, which pull more & more attention & funding towards them, resulting in more & more progress & even more amazing products.\n",
    "\n",
    "## Biological Neurons\n",
    "\n",
    "Before discussing artifical neurons, let's take a quick look at a biological neuron.\n",
    "\n",
    "<img src = \"Images/Biological Neuron.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "It is an unusual-looking cell mostly found in animal brains. It's composed of a *cell body* containing a nucleus & most of the cell's complex components, many branching extensions called *dendrites*, plus one very long extension called *axon*. The axon's length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity, the axon splits off into many branches called *telochondria*, & at the tip of these branches are minuscule strutures called *synaptic terminals* (or simply *synapses*), which are connected to the dendrites or cell bodies of other neurons. Biological neurons produce short electrical impulses called *action potentials* (APs, or just *signals*) which travel along the axons & make the synapses releases chemical signals called *neurotransmitters*. When a neuron receives a sufficient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses (actually, it depends on the neurotransmitters, as some of them inhibit the neuron from firing).\n",
    "\n",
    "Thus, individual biological neurons seem to behave in a rather simple way, but they are organised in a vast network of billions, with each neuron typically connected to thousands of other neurons. Highly complex computations can be performed by a network of fairly simple neurons, much like a complex anthill can emerge from the combined efforts of simple ants. The architecture of biological neural networks (BNNs) is still the subject of active research, but some parts of the brain have been mapped, & it seems that neurons are often organised in consecutive layers, especially in the cerebral cortex (i.e., the outer layer of your brain), as shown below.\n",
    "\n",
    "<img src = \"Images/Layers of Biological Neurons.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "## Logical Computations with Neurons\n",
    "\n",
    "McCulloch & Pitts proposed a very simple model of the biological neuron, which later became known as an *artificial neuron*: it has one or more binary (on/off) inputs & one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active. In their paper, they showed that even with such a simplified model, it is possible to build a network of artificial neurons that computes any logical proposition you want. To see how such a network works, let's build a few ANNs that perform various logical computations, assuming that a neuron is activated when at least two of its inputs are active.\n",
    "\n",
    "<img src = \"Images/ANNs.png\" width = \"500\" style = \"margin:auto\">\n",
    "\n",
    "Let's see what these networks do:\n",
    "\n",
    "* The first network on the left is the identity function: if neuron A is activated, then neuron C gets activated as well (since it receives two inputs signals from neuron A); but if neuron A is off, then neuron C is off as well.\n",
    "* The second network performs a logical AND: neuron C is activated only when both neuron A & B are activated as well (since it takes 2 input signals for neuron C to activate); but if neuron A is off, then neuron C is off as well.\n",
    "* The third network performs a logical OR: neuron C gets activated if either neuron A or neuron B is activated (or both).\n",
    "* Finally, if we suppose that an input connection can inhibit the neuron's activity (which is the case with biological neurons), then the fourth network computes a slightly more complex logical proposition: neuron C is activated only if neuron A is active & neuron B is off. If neuron A is active all the time, then you get a logical NOT: neuron C is active when neuron B is off, & vice versa. \n",
    "\n",
    "You can imagine how these networks can be combined to compute complex logical expressions.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "The *Perceptron* is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron called a *threshold logic unit* (TLU), or sometimes a *linear threshold unit* (LTU). \n",
    "\n",
    "<img src = \"Images/Threshold Logic Unit.png\" width = \"450\" style = \"margin:auto\"/>\n",
    "\n",
    "The inputs & outputs are numbers (instead of binary on/off values), & each input connection is associated with a weight. The TLU computes a weighted sum of its inputs ($z = w_1x_1 + w_2x_2 + ... + w_nx_n = x^Tw$), then applies a *step function* to that sum & outputs the result: $h_w(x) = step(z)$, where $z = x^Tw$.\n",
    "\n",
    "The most common step function used in perceptrons is the Heaviside step function. Sometimes the sign function is used instead.\n",
    "\n",
    "$$heaviside\\ (z) = \\bigg\\{\\begin{split}\n",
    "0\\ if\\ z < 0 \\\\\n",
    "1\\ if\\ z \\geq 0\n",
    "\\end{split} \\quad sgn\\ (z) = \\Biggl\\{\\begin{split}\n",
    "-1\\ if\\ z < 0 \\\\\n",
    "0\\ if\\ z = 0 \\\\\n",
    "+1\\ if\\ z > 0\n",
    "\\end{split}$$\n",
    "\n",
    "A single TLU can be used for simple linear binary classification. It computes a linear combination of inputs, & if the result exceeds a threshold, it outputs the positive class. Otherwise it outputs the negative class (just like a logistic regression or linear SVM classifier). You could, for example, use a single TLU to classify iris flowers based on petal length & width (also adding an extra bias feature $x_0 = 1$, just like we did in previous lessons). Training a TLU in this case means finding the right values for $w_0$, $w_1$, & $w_2$.\n",
    "\n",
    "A perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer (i.e., its input neurons), the layer is called a *fully connected layer*, or a *dense layer*. The inputs of the perceptron are fed to special passthrough neurons called *input neurons*: they output whatever input they are fed. All the input neurons form the *input layer*. Moreover, an extra bias feature is generally added ($x_0 = 1$): it is typically represented using a special type of neuron called a *bias neuron*, which outputs 1 all the time. A perceptron with two inputs & three outputs is represented below. This perceptron can classify instances simultaneously into three different binary classes, which makes it a multioutput classifier.\n",
    "\n",
    "<img src = \"Images/Architecture of Perceptron.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Thanks to the magic of linear algebra, it is possible to efficiently compute the outputs of a layer of artificial neurons for several instances at once.\n",
    "\n",
    "$$h_{W,b}(X) = \\phi(XW + b)$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "* As always, $X$ represents the matrix of input features. It has one row per instance & one column per feature.\n",
    "* The weight matrix $W$ contains all the connection weights except for the ones from the bias neuron. It has one row per input neuron & one column per artificial neuron in the layer.\n",
    "* The bias vector $b$ contains all the connection weights between the bias neuron & the artificial neurons. It has one bias term per artificial neuron.\n",
    "* The function $\\phi$ is called the *activation function*: when the artificial neurons are TLUs, it is a step function.\n",
    "\n",
    "So how is a perceptron trained? The perceptron training algorithm proposed by Rosenblatt was largely inspired by *Hebb's rule*. In his 1949 book, *The Organisation of Behaviour*, Donald Hebb suggested that when a biological neuron triggers another neuron often, the connection between these two neurons grows stronger. Siegrid Lowel later summarised Hebb's idea in the catchy phrase, \"Cells that fire together, wire together\"; that is, the connection weight between two neurons tends to increase when they fire simultaneously. This rule later became known as Hebb's rule (or *Hebbian learning*). Perceptrons are trained using a variant of this rule that takes into account the error made by the network when it makes a prediction; the perceptron learning rule reinforces connections that help reduce the error. More specifically, the perceptron is fed one training instance at a time, & for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction. The rule is shown here.\n",
    "\n",
    "$${w_{i, j}}^{(next\\ step)} = w_{i, j} + \\eta(y_j - \\hat{y}_j)x_i$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "* $w_{i, j}$ is the connection weight between the $i^{th}$ input neuron & the $j^{th}$ output neuron.\n",
    "* $x_i$ is the $i^{th}$ input value of the current training instance.\n",
    "* $\\hat{y}_j$ is the output of the $j^{th}$ output neuron for the current training instance.\n",
    "* $y_j$ is the target output of the $j^{th}$ output neuron for the current training instance.\n",
    "* $\\eta$ is the learning rate.\n",
    "\n",
    "The decision boundary of each output neuron is linear, so perceptrons are incapable of learning complex patterns (just like logistic regression classifiers). However, if the training instances are linearly separable, Rosenblatt demonstrated that this algorithm would converge to a solution. This is called the *perceptron convergence theorem*.\n",
    "\n",
    "Scikit-learn provides a `Perceptron` class that implements a single-TLU network. It can be used pretty much as you would expect -- for example, on the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede2d844-3d94-4f77-8da4-ecd40a27f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # Petal length & width\n",
    "y = (iris.target == 0).astype(np.int32)\n",
    "\n",
    "perceptron_classifier = Perceptron()\n",
    "perceptron_classifier.fit(X, y)\n",
    "\n",
    "y_pred = perceptron_classifier.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0c3f2-6abb-4e7b-8d2d-887679f313f6",
   "metadata": {},
   "source": [
    "You may have noticed that the perceptron learning algoritm strongly resembles stochastic gradient descent. In fact, scikit-learn's `Perceptron` class is equivalent to using an `SGDClassifier` with the following hyperparameters: `loss = \"perceptron\"`, `learning_rate = \"constant\"`, `eta0 = 1` (the learning rate), & `penalty = None` (no regularisation).\n",
    "\n",
    "Note that contrary to the logistic regression classifier, perceptrons do not output a class probability; rather, they make predictions based on a hard threshold. This is one reason to prefer logistic regression over perceptrons.\n",
    "\n",
    "In their 1969 monograph *perceptrons*, Marvin Minsky & Seymour Papert highlighted a number of serious weaknesses of perceptrons -- in particular, the fact that they are incapable of solving some trivial problems (e.g., the *exclusive OR* (XOR) classification problem, see left side of below figure). This is true of any other linear classification model (such as logistic regression classifiers), but researchers had expected much more from perceptron, & some were so disappointed that they dropped neural networks altogether in favor of higher-level problems such as logic, problem solving, & search.\n",
    "\n",
    "It turns out that some of the limitations of perceptrons can be eliminated by stacking multiple perceptrons. The resulting ANN is called a *multilayer perceptron* (MLP). A MLP can solve the XOR problem, as you can verify by computing the output of the MLP represented on the right side of the below figure: with inputs (0, 0) or (1, 1), the network outputs 0, & with inputs (0, 1) or (1, 0), it outputs 1. All connections have a weight equal to 1, except the four connections where the weight is shown. \n",
    "\n",
    "<img src = \"Images/MLP Solves XOR Classification Problem.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "## The Multilayer Perceptron & Backpropagation\n",
    "\n",
    "An MLP is composed of one (passthrough) *input layer*, one or more layers of TLUs, called *hidden layers*, & one final layer of TLUs called the *output layer*. The layers close to the input layer are usually called the *lower layers*, & the ones close to the outputs are usually called the *upper layers*. Every layer except the output layer includes a bias neuron & is fully connected to the next layer.\n",
    "\n",
    "<img src = \"Images/Architecture of Multilayer Perceptron.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers, it is called a *deep neural network* (DNN). The field of deep learning studies DNNs, & more generally models containing deep stacks of computations. Even so, many people talk about deep learning whenever neural networks are involved (even shallow ones).\n",
    "\n",
    "For many years, researchers struggled to find a way to train MLPs, without success. But in 1986, David Rumelhard, Geoffrey Hinton, & Ronald Williams published a groundbreaking paper that introduced the *backpropagation* training algorithm, which is still used today. In short, it is gradient descent using an efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network's error with regard to every single model parameter. In other words, it can find out how each connection weight & each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular gradient descent step, & the whole process is repeated until the network converges to the solution.\n",
    "\n",
    "Let's run through this algorithm in a bit more detail:\n",
    "\n",
    "* It handles one mini-batch at a time (for example, containing 32 instances each), & it goes through the full training set multiple times. Each pass is called an *epoch*.\n",
    "* Each mini-batch is passed to the network's input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed & passed on the next layer, & so on until we get the output of the last layer, the output layer. This is the *forward pass*: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "* Next, the algorithm measures the network's output error (i.e., it uses a loss function that compares the desired output & the actual output of the network, & returns some measure of the error).\n",
    "* Then it computes how much each output connection contributed to the error. This is done analytically by applying the *chain rule* (perhaps the most fundamental rule in calculus), which makes this step fast & precise.\n",
    "* The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until the algorithm reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "* Finally, the algorithm performs a gradient descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "This algorithm is so important that it's worth summarising it again: for each training instance, the backpropagation algorithm first makes a prediction (forward pass) & measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), & finally tweaks the connection weights to reduce the error (gradient descent step).\n",
    "\n",
    "In order for this algorithm to work properly, its authors made a key change to the MLP's architecture: they replaced the step function with the logistic (sigmoid) function $\\sigma(z) = 1/(1 + e^{-z})$. This was essential because the step function contains only flat segments, so there is no gradient to work with (gradient descent cannot move on a flat surface), while the logistic function has a well-defined nonzero derivative everywhere, allowing gradient descent to make some progress at every step. In fact, the backpropagation algorithm works well with many other activation functions, not just the logistic function. Here are two other popular choices:\n",
    "\n",
    "* *The hyperbolic tangent function: $tanh(z) = 2\\sigma(2z) - 1$*\n",
    "   * Just like the logistic function, this activation function is S-shaped, continuous, & differentiable, but its output value ranges from -1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer's output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "* *The rectified linear unit function: $ReLU(z) = max(0, z)$*\n",
    "   * The ReLu function is continuous but unfortunately not differentiable at $z = 0$ (the slope changes abruptly, which can make gradient descent bounce around), & its derivative is 0 for $z < 0$. In practice, however, it works very well & has the advantage of being fast to compute, so it has become the default. Most importantly, the fact that it does not have a maximum output value helps reduce some issues during gradient descent.\n",
    "   \n",
    "These popular activation functions & their derivatives are represented below. But wait! Why do we need activation functions in the first place? Well, if you chain several linear transformations, all you get is a linear transformation. For example if $f(x) = 2x + 3$ & $g(x) = 5x - 1$, then chaining these two linear functions gives you another linear function: $f(g(x)) = 2(5x - 1) + 3 = 10x + 1$. So if you don't have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, & you can't solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function.\n",
    "\n",
    "<img src = \"Images/Activation Functions & Their Derivatives.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Ok. You know where neural nets came from, what their architecture is, & how to compute their outputs. You've also learned about the backpropagation algorithm. But what exactly can you do with them?\n",
    "\n",
    "## Regression MLPs\n",
    "\n",
    "First, MLPs can be used for regression tasks. If you want to predict a single value (e.g., the price of a house, given many of its features), then you just need a single output neuron: its output is the predicted value. For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per output dimensions. For example, to locate the center of an object in an image, you need to predict 2D coordinates, so you need two output neurons. If you also want to place a bounding box around the object, then you need two more numbers: the width & the height of the object. So, you end up with four output neurons.\n",
    "\n",
    "In general, when building an MLP for regression, you do not want to use any activation function for the output neurons, so they are free to output any range of values. If you want to guarantee that the output will always be positive, then you can use the ReLU activation function in the output layer. Alternatively, you can use the *softplus* activation function, which is a smooth variant of ReLU: $softplus(z) = log(1 + e^z)$. It is close to 0 when $z$ is negative, & close to $z$ when $z$ is positive. Finally, if you want to guarantee that the predictions will fall within a given range of values, then you can use the logistic function or the hyperbolic tangent, & then scale the labels to the appropriate range: 0 to 1 for the logistic function & -1 to 1 for the hyperbolic tangent.\n",
    "\n",
    "The loss function to use during training is typically the mean squared error, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you can use the Huber loss, which is a combination of both. \n",
    "\n",
    "The below table summarise the typical architecture of a regression MLP.\n",
    "\n",
    "|Hyperparameter|Typical value|\n",
    "|:---:|:---:|\n",
    "|# input neurons|One per input feature (e.g., 28 x 28 = 784 for MNIST)|\n",
    "|# hidden layers|Depends on the problem, but typically 1 to 5|\n",
    "|# neurons per hidden layers|Depends on the problem, but typically 10 to 100|\n",
    "|# output neurons|1 per prediction dimension|\n",
    "|Hidden activation|ReLU (or SELU)|\n",
    "|Output activation|None, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs)|\n",
    "|Loss function|MSE or MAE/Huber (if outliers)|\n",
    "\n",
    "## Classification MLPs\n",
    "\n",
    "MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using a logistic activation function: the output will be a number between 0 & 1, which you can interpret as the estimated probability of the positive class. The estimated probability of the negative class is equal to one minus that number.\n",
    "\n",
    "MLPs can also easily handle multilabel binary classification tasks. For example, you could have an email classification system that predicts whether each incoming email is ham or spam, & simultaneously predicts whether it is an urgent or nonurgent email. In this case, you would need two output neurons, both using the logistic activation function: the first would output the probability that the email is spam, & the second would output the probability that it is urgent. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels: you can have nonurgent ham, urgent ham, nonurgent spam, & perhaps even urgent spam (although that would probably be an error).\n",
    "\n",
    "If each instance can belong only to a single class, out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class, & you should use the softmax activation function for the whole output layer. The softmax function will ensure that all the estimated probabilities are between 0 & 1 & that they add up to 1 (which is required if the classes are exclusive). This is called multiclass classification.\n",
    "\n",
    "<img src = \"Images/Modern MLP.png\" width = \"450\" style = \"margin:auto\"/>\n",
    "\n",
    "Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (also called the log loss) is generally a good choice.\n",
    "\n",
    "The below table summarises the typical architecture of a classification MLP.\n",
    "\n",
    "|Hyperparameter|Binary classification|Multilabel binary classification|Multiclass classification|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|Input & hidden layers|Same as regression|Same as regression|Same as regression|\n",
    "|# output neurons|1|1 per label|1 per class|\n",
    "|Output layer activation|Logistic|Logistic|Softmax|\n",
    "|Loss function|Cross entropy|Cross entropy|Cross entropy|\n",
    "\n",
    "Now you have all the concepts you need to start implementing MLPs with Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b043f6-0889-4681-a59b-6dde1e40ce53",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e46132-a185-411a-be9d-b7bce37a0e45",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras\n",
    "\n",
    "Keras is a high-level deep learning API that allows you to easily build, train, evaluate, & execute all sorts of neural networks. its documentation is available at *https://keras.io/*. The reference implementation, also called Keras, was developed by Francois Chollet as part of a research project & was released as an open source project in March 2015. It quickly gained popularity, owing to its ease of use, flexibility, & beautiful design. To perform the heavy computations required by neural networks, this reference implementation relies on a computation backend. At present, you can choose from three popular open source deep learning libraries: tensorflow, Microsoft cognitive toolkit (CNTK), & Theano. Therefore, to avoid any confusion, we will refer to this reference implementation as *multibackend Keras*.\n",
    "\n",
    "Since late 2016, other implementations have been released. You can now run Keras on Apache MXNet, Apple's Core ML, Javascript, or TypeScript (to run Keras code in a web browser), & PlaidML (which can run all sorts of GPU devices, not just Nvidia). Moreover, Tensorflow itself now comes bundled with its own Keras implentation, tf.keras. It only supports Tensorflow as the backend, but it has the advantage of offering some very useful extra features: for example, it supports Tensorflow's data API, which makes it easy to load & preprocess data efficiently. For this reason, we will use tf.keras. However, in this lesson, we will not use any of the Tensorflow-specific features, so the code should run fine on other Keras implementations as well (at least in Python), with only minor modifications, such as changing the imports.\n",
    "\n",
    "<img src = \"Images/Implementations of Keras API.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "The most popular deep learning library, after Keras & Tensorflow is Facebook's pytorch library. The good news is that its API is quite similar to Kera's (in part because both APIs were inspired by scikit-learn & chainer), so once you know Keras, it is not difficult to switch to pytorch, if you want to. Pytorch's popularity grew exponentially in 2018, largely thanks to its simplicity & excellent documentation, which were not Tensorflow 1.x's main strengths. However, Tenserflow 2 is arguably just as simple as pytorch., as it has adopted Keras as its official high-level API & its developers have greatly simplified & cleaned up the rest of the API. The documentation has also been completely reorganised, & it is much easier to find what you need now. Similarly, pytorch's main weaknesses (e.g., limited portability & no computation graph analysis) have been largely addressed in pytorch 1.0. Healthy competition is beneficial to everyone.\n",
    "\n",
    "All right, it's time to code! As tf.keras is bundled with Tensorflow, let's start by installing Tensorflow\n",
    "\n",
    "## Installing Tensorflow 2\n",
    "\n",
    "Assuming you installed jupyter & scikit-learn, use pip to install Tensorflow. If you created an isolated environment you first need to activate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fa737-5120-4cdf-be16-6310d19118c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ cd $ML_PATH                 # Your ML working directory (e.g., $HOME/ml)\n",
    "$ source my_env/bin/activate  # on Linux or macOS\n",
    "$ .\\my_env\\Scripts\\activate   # on Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232399e-0ab6-4df8-ade0-441a175ea604",
   "metadata": {},
   "source": [
    "Next, install Tensorflow 2 (if you are not using a virtual env, you will need administrator rights, or to add the `--user` option):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef61d30-c87b-49e3-807e-1a7d9c648453",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ python3 -m pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0857cb-0f9f-4034-ae64-184dfa8b680a",
   "metadata": {},
   "source": [
    "To test your installation, open a Python shell or jupyter notebook, then import Tensorflow & tf.keras & print their versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbc4351-c126-4b3b-8186-40056f341b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35c2730-9435-4cf7-a841-b271ab068481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8678b-7a5c-417a-ad71-45166c1e012c",
   "metadata": {},
   "source": [
    "The second version is the version of the Keras API implemented by tf.keras. Note that it ends with `-tf`, highlighting the fact that tf.keras implements the Keras API, plus some extra Tensorflow-specific features.\n",
    "\n",
    "Now let's use tf.keras!. We'll start by building a simple image classifier.\n",
    "\n",
    "## Building an Image Classifier Using the Sequential API\n",
    "\n",
    "First, we need to load a dataset. In thi lesson, we will tackle fashion MNIST, which is a drop-in replacement of MNIST. It has the exact same format as MNIST (70,000 grayscale images of 28 x 28 pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse, & the problem turns out to be significantly more challenging than MNIST. For example, a simple linear model reaches about 92% on MNIST, but only about 83% on fashion MNIST.\n",
    "\n",
    "### Using Keras to Load the Dataset\n",
    "\n",
    "Keras provides some utility functions to fetch & load common datasets, including MNIST, fashion MNIST, & the California housing dataset we used before. Let's load fashion MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039ff699-81d2-41dc-9633-2d1ad2664fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52263690-f05e-49ee-b6cf-1054fc9a2f7c",
   "metadata": {},
   "source": [
    "When loading MNIST or fashin MNIST using Keras rather than scikit-learn, one important difference is that every image is represented as a 28 x 28 array rather than a 1D array of size 784. Moreover, the pixel intensities are represented as integer (from 0 to 255) rather than floats (from 0.0 to 255.0). Let's take a look at the shape & data type of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f03867-5067-4442-8ba3-64515280f456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "718b28f2-a8ae-4a0e-9743-2bc698a364cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03aa75-6637-4e9c-9519-770f357a43f7",
   "metadata": {},
   "source": [
    "Note that the dataset is already split into a training set & a test set, but there is no validation set, so we'll create one now. Additionally, since we are going to train the neural network using gradient descent, we must scale the input features. For simplicity, we'll scale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also converts them to floats):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb6e39a-feb0-45cc-a735-3d5028f223a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271d138-34a4-44b4-b729-c2f5c669b758",
   "metadata": {},
   "source": [
    "With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5. Easy. For fashion MNIST, however, we need the list of class names to know what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714b4415-65e6-464f-baa5-dbb382dc1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c052a4-b95f-4c70-a42d-cc56cfe9e5aa",
   "metadata": {},
   "source": [
    "For example, the first image in the training set represents a coat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c598a60-3e93-4ab4-8468-95d4775be116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b08846-cebc-4884-b057-a714e491905e",
   "metadata": {},
   "source": [
    "The below figure shows some samples from the fashion MNIST dataset.\n",
    "\n",
    "<img src = \"Images/Fashion MNIST.png\" width = \"800\" style = \"margin:auto\"/>\n",
    "\n",
    "### Creating the Model Using the Sequential API\n",
    "\n",
    "Now let's build the neural network! Here is a classification MLP with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4021dbe6-c6da-4381-a9ed-0bff8e48531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30449e32-097a-43d8-b5b7-f47b29b8b8f4",
   "metadata": {},
   "source": [
    "Let's go through this code line by line:\n",
    "\n",
    "* The first line creates a `Sequential` model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the sequential API.\n",
    "* Next, we build the first layer & add it to the model. It is a `Flatten` layer whose role is to convert each input image into a 1D array: if it receives input data `X`, it computes `X.reshape(-1, 1)`. This layer does not have any parameters; it is just there to do simple preprocessing. Since it is the first layer in the model, you should specify the `input_shape`, which doesn't include the batch size, only the shape of the instances. Alternatively, you could add a `keras.layers.InputLayer` as the first layer, setting `input_shape = [28, 28]`.\n",
    "* Next, we add a `Dense` hidden layer with 300 neurons. It will use the ReLU activation function. Each `Dense` layer manages its own weight matrix, containing all the connection weights between the neurons & their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes:\n",
    "\n",
    "$$h_{W, b}(X) = \\phi(XW + b)$$\n",
    "\n",
    "* Then we add a second `Dense` hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "* Finally, we add a `Dense` output layer with 10 neurons (one per class) using the softmax activation function (because the classes are exclusive).\n",
    "\n",
    "Instead of adding the layers one by one as we just did, you can pass a list of layers when creating the `Sequential` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ba690a9-73df-49c4-8725-18cebae7749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.Dense(300, activation = \"relu\"),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753459e-8d0f-4096-988b-9fb1e849fe76",
   "metadata": {},
   "source": [
    "The model's `summary()` method displays all the model's layers, including each layer's name (which is automatically generated unless you set it when creating the layer), its output shape (`None` means the batch size can be anything), & its number of parameters. The summary ends with the total number of parameters, including trainable & non-trainable parameters. Here we only have trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a8325d-db9b-435b-b0ac-5bc1202c1619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb02c8-ad46-4ae3-9869-fd420a5d683a",
   "metadata": {},
   "source": [
    "Note that `Dense` layers often have a *lot* of parameters. For example, the first hidden layer has 784 x 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data. We will come back to this layer.\n",
    "\n",
    "You can easily get a model's list of layers, to fetch a layer by its index, or you can fetch it by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb9a6f4-f6ed-4fbc-a1c6-631a174fdb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x7fbe7d2ad5b0>,\n",
       " <keras.layers.core.dense.Dense at 0x7fbe7d2ad7f0>,\n",
       " <keras.layers.core.dense.Dense at 0x7fbe7d2ad820>,\n",
       " <keras.layers.core.dense.Dense at 0x7fbe7d2ada60>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5146a72f-c12f-4e9d-991b-86d3cef3ed8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_6'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dcca1b9-bc14-42fd-804a-28a73bfeb4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(\"dense_6\") is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf50fa8-ff1f-459d-924c-fda16cf78fbc",
   "metadata": {},
   "source": [
    "All the parameters of a layer can be accessed using its `get_weights()` & `set_weights()` methods. For a `Dense` layer, this includes both the connection weights & the bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66df1d53-2201-430f-bcc5-36e5978919f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04335923, -0.06105275,  0.04786379, ...,  0.0465821 ,\n",
       "        -0.03430203, -0.06442234],\n",
       "       [ 0.01385181, -0.0091    , -0.05432431, ..., -0.05929907,\n",
       "        -0.02213493,  0.02919906],\n",
       "       [ 0.0132647 ,  0.04600012, -0.06903861, ...,  0.04124334,\n",
       "         0.02369825, -0.04920958],\n",
       "       ...,\n",
       "       [ 0.04332872,  0.05580448,  0.04423104, ..., -0.04061208,\n",
       "        -0.00844724, -0.06906441],\n",
       "       [ 0.00991383,  0.02870528,  0.01056658, ...,  0.0161232 ,\n",
       "        -0.06468378, -0.05474035],\n",
       "       [-0.02467224, -0.06917337,  0.03281023, ...,  0.0644898 ,\n",
       "        -0.02557717, -0.0266502 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74beb211-30c3-4773-b340-8a4d8ef5b76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a4fad88-3758-4f14-a400-7cfa4d167b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f09c8f85-ca9e-4ab7-bda7-2bd59ef36b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e493e-c42a-42e6-96f8-7a739c341f44",
   "metadata": {},
   "source": [
    "Notice that the `Dense` layer initialised the connection weights randomly (which is needed to break symmertry), & the biases were initialised to zeros, which is fine. If you ever want to use a different initialisation method, you can set `kernel_initialiser` (*kernel* is another name for the matrix of connection weights) or `bias_initialiser` when creating the layer.\n",
    "\n",
    "### Compiling the Model\n",
    "\n",
    "After a model is created, you must call its `compile()` method to specify the loss function & the optimiser to use. Optionally, you can specify a list of extra metrics to compute during training & evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d33bc964-52fc-4175-af2a-a9b8f3bde133",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = \"sgd\",\n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b6ef4-4156-4702-af92-061763f45260",
   "metadata": {},
   "source": [
    "This code requires some explanation. First, we use the `\"sparse_categorical_crossentropy\"` loss because we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), & the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g., [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the `\"categorical_crossentropy\"` loss instead. If we were doing binary classification (with one or more binary labels), then we would use the `\"sigmoid\"` (i.e., logistic) activation function in the output layer instead of the `\"softmax\"` activation function, & we would use the `\"binary_crossentropy\"` loss.\n",
    "\n",
    "Regarding the optimiser, `\"sgd\"` means that we will train the model using simple stochastic gradient descent. In other words, Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff plus gradient descent). \n",
    "\n",
    "Finally, since this is a classifier, it's useful to measure its `\"accuracy\"` during training & evaluation.\n",
    "\n",
    "### Training & Evaluating the Model\n",
    "\n",
    "Now the model is ready to be trained. For this, we simply need to call its `fit()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db307671-19eb-4d09-b117-fee1eb85abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7144 - accuracy: 0.7659 - val_loss: 0.5053 - val_accuracy: 0.8306\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4864 - accuracy: 0.8307 - val_loss: 0.4755 - val_accuracy: 0.8310\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4417 - accuracy: 0.8456 - val_loss: 0.4132 - val_accuracy: 0.8602\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4155 - accuracy: 0.8542 - val_loss: 0.3989 - val_accuracy: 0.8640\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3956 - accuracy: 0.8601 - val_loss: 0.3786 - val_accuracy: 0.8710\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3794 - accuracy: 0.8666 - val_loss: 0.3673 - val_accuracy: 0.8766\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3667 - accuracy: 0.8706 - val_loss: 0.3746 - val_accuracy: 0.8662\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3564 - accuracy: 0.8743 - val_loss: 0.3642 - val_accuracy: 0.8718\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3454 - accuracy: 0.8778 - val_loss: 0.3509 - val_accuracy: 0.8814\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3344 - accuracy: 0.8801 - val_loss: 0.3559 - val_accuracy: 0.8762\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3268 - accuracy: 0.8830 - val_loss: 0.3382 - val_accuracy: 0.8806\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3180 - accuracy: 0.8861 - val_loss: 0.3322 - val_accuracy: 0.8828\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3111 - accuracy: 0.8890 - val_loss: 0.3400 - val_accuracy: 0.8822\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3043 - accuracy: 0.8909 - val_loss: 0.3461 - val_accuracy: 0.8774\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2966 - accuracy: 0.8934 - val_loss: 0.3228 - val_accuracy: 0.8866\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2912 - accuracy: 0.8959 - val_loss: 0.3400 - val_accuracy: 0.8760\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2851 - accuracy: 0.8979 - val_loss: 0.3307 - val_accuracy: 0.8826\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2807 - accuracy: 0.8979 - val_loss: 0.3130 - val_accuracy: 0.8878\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2736 - accuracy: 0.9015 - val_loss: 0.3208 - val_accuracy: 0.8868\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2689 - accuracy: 0.9034 - val_loss: 0.3120 - val_accuracy: 0.8896\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2651 - accuracy: 0.9048 - val_loss: 0.3194 - val_accuracy: 0.8898\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2601 - accuracy: 0.9063 - val_loss: 0.3318 - val_accuracy: 0.8808\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2548 - accuracy: 0.9076 - val_loss: 0.3348 - val_accuracy: 0.8788\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2506 - accuracy: 0.9113 - val_loss: 0.3152 - val_accuracy: 0.8886\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2459 - accuracy: 0.9108 - val_loss: 0.3180 - val_accuracy: 0.8814\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2419 - accuracy: 0.9126 - val_loss: 0.3195 - val_accuracy: 0.8864\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2385 - accuracy: 0.9157 - val_loss: 0.3080 - val_accuracy: 0.8884\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2348 - accuracy: 0.9170 - val_loss: 0.3216 - val_accuracy: 0.8870\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2304 - accuracy: 0.9182 - val_loss: 0.3037 - val_accuracy: 0.8980\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2272 - accuracy: 0.9183 - val_loss: 0.2948 - val_accuracy: 0.8952\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 30, \n",
    "                    validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840c094-90c0-434b-ae3d-661b6a5a1d1f",
   "metadata": {},
   "source": [
    "We pass it the input features (`X_train`) & the target classes (`y_train`), as well as the number of epochs to train (or else it would default to just 1, which would definitely not be enough to converge to a good solution). We also pass a validation set (this is optional). Keras will measure the loss & the extra metrics on this set at the end of each epoch, which is very useful to see how well the model really performs. If the performance on the training set is much better that on the validation set, your model is probably overfitting the training set (or there is a bug, such as a data mismatch between the training set & the validation set).\n",
    "\n",
    "That's it! The neural network is trained. At each epoch, during training, Keras displays the number of instances processed so far (along with a progress bar), the mean training time per sample, & the loss & accuracy(or any other extra metrics you asked for) on both the training set & the validation set. You can see that the training loss went down, which is a good sign, & the validation accuracy reached 89.52% after 30 epochs. That's not too far from the training accuracy, so there does not seem to be much overfitting going on.\n",
    "\n",
    "If the training set was very skeyed, with some classes being overrepresented & others underrepresented, it would be useful to set the `class_weight` argument when calling the `fit()` method, which would give a larger weight to underrepresented classes & a lower weight to overrepresented classes. These weights would be used by Keras when computing the loss. If you need per-instance weights, set the `sample_weight` argument (if both `class_weight` & `sample_weight` are provided, Keras multiplies them). Per-instance weights could be useful if some instances were labeled by experts while others were labeled using a crowdsourcing platform: you might want to give more weight to the former. You can also provide sample weights (but not class weights) for the validation set by adding them as a third item in the `validation_data` tuple.\n",
    "\n",
    "The `fit()` method returns a `History` object containing the training parameters (`history.params`), the list of epochs it went through (`history.epoch`), & most importantly a dictionary (`history.history`) containing the loss & extra metrics it measured at the end of each epoch on the training set & on the validation set (if any). If you use this dictionary to create a pandas dataframe & class its `plot()` method, you get the learning curves shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adf59f07-1c94-4476-9569-337c26572346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize = (8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627480a-6a14-45d8-99f7-ebc76ca3e141",
   "metadata": {},
   "source": [
    "You can see that both the training accuracy & the validation accuracy steadily increase during training, while the training loss & the validation loss decrease. Good! Moreover, the validation curves are close to the training curves, which means that there is not too much overfitting. In this particular case, the model looks like it performed better on the validation set than on the training set at the beginning of training. but that's not the case: indeed, the validation error is computed at the *end* of each epoch, while the training error is computed using a running mean *during* each epoch. So the training curve should be shifted by half an epoch to the left. If you do that you will see that the training & validation curves overlap almost perfectly at the beginning of training.\n",
    "\n",
    "The training set performance ends up beating the validation performance, as is generally the case when you train for long enough. You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue training. It's as simple as calling the `fit()` method again, since Keras just continues training where it left off (you should be able to reach close to 89% validation accuracy).\n",
    "\n",
    "If you are not satisfied with the performance of your model, you should go back & tune the hyperparameters. The first one to check is the learning rate. If that doesn't help, try another optimiser (& always return the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning model hyperparameters such as the number of layers, the number of neurons per layer, & the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size (it can be set in the `fit()` method using the `batch_size` argument, which defaults to 32). We will get back to hyperparameter tuning at the end of this chapter. Once you are satisfied with your model's validation accuracy, you should evaluate it on the test set to estimate the generalisation error before you deploy the model to production. You can easily do this using the `evaluate()` method (it also supports several other arguments, such as `batch_size` & `sample_wieght`; check documentation for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "359b1dd5-4f6a-454e-885e-588982e69e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 63.4691 - accuracy: 0.8429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[63.46907424926758, 0.8428999781608582]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8e31a-d301-4163-ab6c-487cfd9929ff",
   "metadata": {},
   "source": [
    "It is common to get slightly lower performance on the test set than on the validation set, because the hyperparameters are tuned on the validation set, not the test set (however, in this example, we did not do any hyperparameter tuning, so the lower accuracy is just bad luck). Remember to resist the temptation to tweak the hyperparameters on the test set, or else your estimate of the generalisation error will be too optimistic.\n",
    "\n",
    "### Using the Model to Make Predictions\n",
    "\n",
    "Next, we can use the model's `predict()` method to make predictions on new instances. Since we don't have actual new instances, we will just use the first three instances of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60b60686-d2ac-4d62-bf3c-8c5196441c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 261ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5179ad-3d3a-465f-ba55-3f34a4ee0023",
   "metadata": {},
   "source": [
    "As you can see, for each instance the model estimates one probability per class, from class 0 to class 9. For example, it estimates that the probability of class 9 (ankle boot) is 100%. In other words, it is certain the first image is ankle boots. If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the `predict()` method instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76c40170-fd57-4411-8c3f-cf4ee2a31589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(X_new)\n",
    "classes_x = np.argmax(y_pred, axis = 1)\n",
    "classes_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e3cebcc-a6e8-4833-886b-f02b46283979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[classes_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c36c3c-80e8-4442-b218-9c2c6836d0f6",
   "metadata": {},
   "source": [
    "Here, the classifier actually classified all three images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f93fb79c-cbaa-4259-96ca-0f7fe9c9d9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91d31b-d4e9-4979-ad68-ccb59894bf0d",
   "metadata": {},
   "source": [
    "<img src = \"Images/Correctly Classified Fashion MNIST.png\" width = \"550\" style = \"margin:auto\"/>\n",
    "\n",
    "Now you know how to use the sequential API to build, train, evaluate, & use a classification MLP. But what about regression?\n",
    "\n",
    "## Building a Regression MLP Using the Sequential API\n",
    "\n",
    "Let's switch to the California housing problem & tackle it using a regression neural network. For simplicity, we will use scikit-learn's `fetch_california_housing()` function to load the data. This dataset only contains numerical features & has no missing values. After loading the data, we split it into a training set, a validation set, & a test set, & we scale all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72a5c38c-06cb-421d-bc8f-c6da727b34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433dca70-c420-451e-9879-f0ab4c7d7906",
   "metadata": {},
   "source": [
    "Using the sequential API to build, train, evaluate, & use a regression MLP to make predictions is quite similar to what we did for classification. The main differences are the fact that the output layer has a single neuron (since we only want to predict a single value) & uses no activation function, & the loss function is the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04e43f0a-5dfe-44b4-8a99-f5bcbdd0c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.9050 - val_loss: 0.5646\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4930 - val_loss: 0.5125\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4592 - val_loss: 0.4963\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.4866\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4333 - val_loss: 0.4764\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4228 - val_loss: 0.4734\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.4620\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4092 - val_loss: 0.4526\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4045 - val_loss: 0.4571\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3981 - val_loss: 0.4451\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.4383\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.4364\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.4309\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3816 - val_loss: 0.4262\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.4182\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.4199\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.4150\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3674 - val_loss: 0.4179\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.4250\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.4134\n",
      "162/162 [==============================] - 0s 807us/step - loss: 3.4915\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"relu\", input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = \"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs = 20, \n",
    "                    validation_data = (X_val, y_val))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] \n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d17269-707f-4f23-9652-bb5d81081c36",
   "metadata": {},
   "source": [
    "As you can see, the sequential APi is quite easy to use. However, although `Sequential` models are extremely common, it is sometimes useful to build neural networks with more complex topologies, or with multiple inputs or outputs. For this purpose, Keras offers the functional API.\n",
    "\n",
    "## Building Complex Models Using the Functional API\n",
    "\n",
    "One example of a nonsequential neural network is a wide & deep neural network. This neural network architecture was introduced in a 2016 paper by Hneg-Tze Cheng. It connects all or part of the inputs directly to the output layer, as shown in the below figure.\n",
    "\n",
    "<img src = \"Images/Wide & Deep Neural Network.png\" width = \"400\" style = \"margin:auto\"/>\n",
    "\n",
    "This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) & simple rules (through the short path). In contrast, a regular MLP corces all the data to flow through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations.\n",
    "\n",
    "Let's build such a neural network to tackle the California housing problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecbcd8b2-ba33-4ef3-a8bd-975cea80a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape = X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation = \"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs = [input_], outputs = [output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db5e43-6a96-4562-b6b1-147012823667",
   "metadata": {},
   "source": [
    "Let's go through each line of this code:\n",
    "\n",
    "* First, we need to create an `Input` object. This is a specification of the kind of input he model will get, including its `shape` & `dtype`. A model may actually have multiple inputs, as we will see shortly.\n",
    "* Next, we create a `Dense` layer with 30 neurons, using the ReLU activation function. As soon as it is created, notice that we call it like a function, passing it the input. This is why this is called the functional API. Note that we are just telling Keras how it should connect the layers together; no actual data is being processed yet.\n",
    "* We then create a second hidden layer, & again we use it as a function. Note that we pass it the output of the first hidden layer.\n",
    "* Next, we create a `Concatenate` layer, & once again we immediately use it like a function, to concatenate the input & the output of the second hidden layer. You may prefer the `keras.layers.concatenate()` function, which creates a `Concatenate` layer & immediately calls it with the given inputs.\n",
    "* Then we create the output layer, with a single neuron & no activation function, & we call it like a function, passing it the result of the concatenation.\n",
    "* Lastly, we create a Keras `Model`, specifying which inputs & outputs to use.\n",
    "\n",
    "Once you have built the Keras model, everything is exactly like earlier, so there's no need to repeat it here: you must compile the model, train it, evaluate it, & use it to make predictions.\n",
    "\n",
    "But what if you want to send a subset of the features through the wide path & a different subset (possibly overlapping) through the deep path? In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4), & six features through the deep path (features 2 to 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c917d5e3-437f-4668-b69b-73e540417610",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape = [5], name = \"wide_input\")\n",
    "input_B = keras.layers.Input(shape = [6], name = \"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation = \"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name = \"output\")(concat)\n",
    "model = keras.Model(inputs = [input_A, input_B], outputs = [output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1409cd6-c507-477d-8ccd-0fa03dd99d60",
   "metadata": {},
   "source": [
    "<img src = \"Images/Handling Multiple Inputs.png\" width = \"400\" style = \"margin:auto\"/>\n",
    "\n",
    "The code is self-explanatory. You should name at least the most important layers, especially when the model gets a bit complex like this. note that we specified `inputs = [input_A, input_B]` when creating the model. now we can compile the model as usual, but when we call the `fit()` method, instead of passing a single input matrix `X_train`, we must pass a pair of matrices (`X_train_A`, `X_train_B`): one per input. The same is true for `X_val`, & also for `X_test` & `X_new` when you call `evaluate()` or `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0dc6fcb-5f09-4d5a-a93f-f784fd4e5897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.0269 - val_loss: 0.7836\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7053 - val_loss: 0.6677\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6198 - val_loss: 0.6421\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5884 - val_loss: 0.6240\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5675 - val_loss: 0.6086\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5512 - val_loss: 0.5966\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5384 - val_loss: 0.5865\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5281 - val_loss: 0.5776\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5198 - val_loss: 0.5700\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5130 - val_loss: 0.5637\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5072 - val_loss: 0.5581\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5024 - val_loss: 0.5535\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4981 - val_loss: 0.5489\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4939 - val_loss: 0.5453\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4907 - val_loss: 0.5418\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4875 - val_loss: 0.5391\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4845 - val_loss: 0.5353\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4817 - val_loss: 0.5308\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4793 - val_loss: 0.5286\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4769 - val_loss: 0.5257\n",
      "162/162 [==============================] - 0s 897us/step - loss: 0.4743\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"mse\", optimizer = keras.optimizers.SGD(lr = 1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_val_A, X_val_B = X_val[:, :5], X_val[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs = 20, \n",
    "                    validation_data = ((X_val_A, X_val_B), y_val))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176cf454-1b90-4de6-8e11-b6253b9ced7a",
   "metadata": {},
   "source": [
    "There are many use cases in which you may want to have multiple outputs:\n",
    "\n",
    "* The task may demand it. For instance, you may want to locate & classify the main object in a picture. This is both a regression task (finding the coordinates of the object's center, as well as its width & height) & a classification task.\n",
    "* Similarly, you may have multiple independent tasks based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks. For example, you could perform *multitask classification* on pictures of faces, using one output to classiy the person's facial expression (smiling, surprised, etc.) & another output to identify whether they are wearing glasses or not.\n",
    "* Another use case is a regularisation technique (i.e., a training constraint whose objective is to reduce overfitting & thus improve the mode's ability to generalise). For example,you may want to add some auxiliary outputs in a neaural network architecture to ensure that the underlying part of the networks learns something useful on its own, without relying on the rest of the network.\n",
    "\n",
    "<img src = \"Images/Handling Multiple Outputs.png\" width = \"550\" style = \"margin:auto\"/>\n",
    "\n",
    "Adding extra inputs is quite easy: just connect them to the appropriate layers & add them to your model's list of outputs. For example, the following code builds the network represented in the above figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "247f10cd-cf65-4b6f-8c16-e05523fe7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape = [5], name = \"wide_input\")\n",
    "input_B = keras.layers.Input(shape = [6], name = \"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation = \"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name = \"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name = \"aux_output\")(hidden2)\n",
    "\n",
    "model = keras.Model(inputs = [input_A, input_B], outputs = [output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a5c84-8ca5-4c14-8f75-5f43b3c5afe7",
   "metadata": {},
   "source": [
    "Each output will need its own loss function. Therefore, when we compile the model, we should pass a list of losses (if we pass a single loss, Keras will assume that the same loss must be used for all outputs). By default, Keras will compute all these losses & simply add them up to get the final loss used for training. We care much more about the main output than about the auxiliary output (as it is just used for regularisation), so we want to give the main output's loss a much greater weight. Fortunately, it is possible to set all the loss weights when compiling the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "480b64ee-d351-42a0-abc6-3242fe27ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = [\"mse\", \"mse\"], loss_weights = [0.9, 0.1], optimizer = \"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b1c6a-42a9-4a25-9b06-c01ca3fa224e",
   "metadata": {},
   "source": [
    "Now when we train the model, we need to provide labels for each output. In this example, the main output & the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing `y_train`, we need to pass (`y_train`, `y_train`) (& the same goes for `y_val` & `y_test`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccc39a6d-8a47-4f0d-9020-238528de7a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.1079 - main_output_loss: 1.0094 - aux_output_loss: 1.9947 - val_loss: 0.6610 - val_main_output_loss: 0.5983 - val_aux_output_loss: 1.2255\n",
      "Epoch 2/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5729 - main_output_loss: 0.5068 - aux_output_loss: 1.1677 - val_loss: 0.5779 - val_main_output_loss: 0.5294 - val_aux_output_loss: 1.0148\n",
      "Epoch 3/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5200 - main_output_loss: 0.4695 - aux_output_loss: 0.9746 - val_loss: 0.5388 - val_main_output_loss: 0.4991 - val_aux_output_loss: 0.8956\n",
      "Epoch 4/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4954 - main_output_loss: 0.4554 - aux_output_loss: 0.8559 - val_loss: 0.5216 - val_main_output_loss: 0.4902 - val_aux_output_loss: 0.8046\n",
      "Epoch 5/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4791 - main_output_loss: 0.4465 - aux_output_loss: 0.7725 - val_loss: 0.5060 - val_main_output_loss: 0.4797 - val_aux_output_loss: 0.7430\n",
      "Epoch 6/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4661 - main_output_loss: 0.4382 - aux_output_loss: 0.7165 - val_loss: 0.4901 - val_main_output_loss: 0.4670 - val_aux_output_loss: 0.6979\n",
      "Epoch 7/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4550 - main_output_loss: 0.4304 - aux_output_loss: 0.6767 - val_loss: 0.4791 - val_main_output_loss: 0.4592 - val_aux_output_loss: 0.6576\n",
      "Epoch 8/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4453 - main_output_loss: 0.4229 - aux_output_loss: 0.6465 - val_loss: 0.4761 - val_main_output_loss: 0.4587 - val_aux_output_loss: 0.6330\n",
      "Epoch 9/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4364 - main_output_loss: 0.4156 - aux_output_loss: 0.6235 - val_loss: 0.4674 - val_main_output_loss: 0.4514 - val_aux_output_loss: 0.6110\n",
      "Epoch 10/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4291 - main_output_loss: 0.4095 - aux_output_loss: 0.6060 - val_loss: 0.4568 - val_main_output_loss: 0.4413 - val_aux_output_loss: 0.5969\n",
      "Epoch 11/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4224 - main_output_loss: 0.4035 - aux_output_loss: 0.5925 - val_loss: 0.4481 - val_main_output_loss: 0.4334 - val_aux_output_loss: 0.5808\n",
      "Epoch 12/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4153 - main_output_loss: 0.3972 - aux_output_loss: 0.5783 - val_loss: 0.4407 - val_main_output_loss: 0.4265 - val_aux_output_loss: 0.5686\n",
      "Epoch 13/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4094 - main_output_loss: 0.3917 - aux_output_loss: 0.5687 - val_loss: 0.4353 - val_main_output_loss: 0.4207 - val_aux_output_loss: 0.5663\n",
      "Epoch 14/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4030 - main_output_loss: 0.3858 - aux_output_loss: 0.5576 - val_loss: 0.4329 - val_main_output_loss: 0.4194 - val_aux_output_loss: 0.5544\n",
      "Epoch 15/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3968 - main_output_loss: 0.3800 - aux_output_loss: 0.5479 - val_loss: 0.4305 - val_main_output_loss: 0.4178 - val_aux_output_loss: 0.5450\n",
      "Epoch 16/23\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3922 - main_output_loss: 0.3760 - aux_output_loss: 0.5386 - val_loss: 0.4311 - val_main_output_loss: 0.4186 - val_aux_output_loss: 0.5434\n",
      "Epoch 17/23\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3874 - main_output_loss: 0.3713 - aux_output_loss: 0.5315 - val_loss: 0.4233 - val_main_output_loss: 0.4106 - val_aux_output_loss: 0.5372\n",
      "Epoch 18/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3826 - main_output_loss: 0.3670 - aux_output_loss: 0.5230 - val_loss: 0.4272 - val_main_output_loss: 0.4168 - val_aux_output_loss: 0.5207\n",
      "Epoch 19/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3786 - main_output_loss: 0.3632 - aux_output_loss: 0.5170 - val_loss: 0.4134 - val_main_output_loss: 0.4022 - val_aux_output_loss: 0.5139\n",
      "Epoch 20/23\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3977 - main_output_loss: 0.3839 - aux_output_loss: 0.5225 - val_loss: 0.4183 - val_main_output_loss: 0.4060 - val_aux_output_loss: 0.5292\n",
      "Epoch 21/23\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3734 - main_output_loss: 0.3587 - aux_output_loss: 0.5060 - val_loss: 0.4155 - val_main_output_loss: 0.4050 - val_aux_output_loss: 0.5094\n",
      "Epoch 22/23\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3684 - main_output_loss: 0.3542 - aux_output_loss: 0.4962 - val_loss: 0.4032 - val_main_output_loss: 0.3921 - val_aux_output_loss: 0.5028\n",
      "Epoch 23/23\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3635 - main_output_loss: 0.3493 - aux_output_loss: 0.4906 - val_loss: 0.4082 - val_main_output_loss: 0.3977 - val_aux_output_loss: 0.5029\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs = 23, \n",
    "                    validation_data = ([X_val_A, X_val_B], [y_val, y_val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ab9ea-fd3d-41df-a1ec-4463546720f9",
   "metadata": {},
   "source": [
    "When we evaluate the model, Keras will return the total loss, as well as all the individual losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69c722ea-a733-420f-b3df-6ce16d3ef46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 6.4533 - main_output_loss: 6.4452 - aux_output_loss: 6.5266\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07062dda-4655-4629-8ec6-e01457babde2",
   "metadata": {},
   "source": [
    "Similarly, the `predict()` method will return predictions for each output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43a2081f-2393-4daa-a131-282bb1daa091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 137ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f63dfe-d711-4224-a466-87e0d6b0902a",
   "metadata": {},
   "source": [
    "As you can see, you can build any sort of architecture you want quite easily with the functional API. Let's look at one last way to you can build Keras models.\n",
    "\n",
    "## Using the Subclassing API to Build Dynamic Models\n",
    "\n",
    "Both the sequential API & the functional API are declarative: you start by declaring which layers you want to use & how they should be connected, & only then can you start feeding the model some data for training or inference. This has many advantages: the model can easily be saved, cloned, & shared; its structure can be displayed & analysed; the framework can infer shapes & check types, so errors can be caught early (i.e., before any data ever goes through the model). It's also fairly easy to debug, since the whole model is a static graph of layers. But the flip side is just that: it's static. Some models involve loops, varying shapes, conditional branching, & other dynamic behaviours. For such cases, or simply if you prefer a more imperative programming style, the subclassing API is for you.\n",
    "\n",
    "Simply subclass the `Model` class, create the layers you need in the constructor, & use them to perform the computations you want in the `call()` method. For example, creating an instance of the following `WideAndDeepModel` class gives us an equivalent model to the one we just built with the functional API. You can then compile it, evaluate it, & use it to make predictions, exactly like we just did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e2cc1e9-0edf-4078-85f6-82dda8e41df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units = 30, activation = \"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation = activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation = activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    def call(self, inputs):\n",
    "        input_A, inputB = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b2c2a-db68-4f8c-ac9a-d7111ee71ecf",
   "metadata": {},
   "source": [
    "This example looks very much like the functional API, except we do not need to create the inputs; we just use the `input` argument to the `call()` method, & we separate the creation of the layers in the constructor from their usage in the `call()` method. The big difference is that you can do pretty much anything you want in the `call()` method; `for`loops, `if` statements, low-level Tensorflow operations -- your imagination is the limit! This makes it great API for researchers experimenting with new ideas.\n",
    "\n",
    "This extra flexibility does come at a cost; your model's architecture is hidden within the `call()` method, so Keras cannot easily inspect it; it cannot save or clone it; & when you call the `summary()` method, you only get a list of layers. without any information on how they are connected to each other. Moreover, Keras cannot check types & shapes ahead of time, & it is easier to make mistakes. So unless you really need that extra flexibility, you should probably stick to teh sequential API or the functional API.\n",
    "\n",
    "Now that you know how to build & train neural nets using Keras, you will want to save them!\n",
    "\n",
    "## Saving & Restoring a Model\n",
    "\n",
    "When using the sequential API or the functional API, saving a trained Keras model is as simple as it gets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09007521-5a98-4d8f-a4ea-e84683a817b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.4122 - val_loss: 1.1709\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.9831 - val_loss: 0.7970\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7279 - val_loss: 0.7228\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6657 - val_loss: 0.6860\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6300 - val_loss: 0.6574\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6024 - val_loss: 0.6341\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5784 - val_loss: 0.6137\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5580 - val_loss: 0.5981\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5407 - val_loss: 0.5838\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5256 - val_loss: 0.5713\n",
      "162/162 [==============================] - 0s 897us/step - loss: 0.5351\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"relu\", input_shape = [8]),\n",
    "    keras.layers.Dense(30, activation = \"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "]) # or keras.Model([...])\n",
    "\n",
    "model.compile(loss = \"mse\", optimizer = keras.optimizers.SGD(learning_rate = 1e-3))\n",
    "history = model.fit(X_train, y_train, epochs = 10, \n",
    "                    validation_data = (X_val, y_val))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46cac9-9b7e-49a7-a64d-36dd00165284",
   "metadata": {},
   "source": [
    "Keras will use the HDF5 format to save both the model's architecture (including every layer's hyperparameters) & the values of all the model parameters for every layer (e.g., connection weights & biases). It also saves the optimiser (including its hyperparameters & any state it may have).\n",
    "\n",
    "You will typically have a script that trains a model & saves it, & one or more scripts (or web services) that load the model & use it to make predictions. Loading the model is just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ccbf55d-0f50-4262-9435-e3c7868af5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb76572-7a49-448e-97b8-159cf8ef648a",
   "metadata": {},
   "source": [
    "But what if training lasts several hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training, to avoid losing everything if your computer crashes. But how can you tell the `fit()` method to save checkpoints? Use callbacks.\n",
    "\n",
    "## Using Callbacks\n",
    "\n",
    "The `fit()` method accepts a `callbacks` argument that lets you specify a list of objects that Keras will call at the start & end of training, at the start & end of each epoch, & even before & after processing each batch. For example, the `ModelCheckpoint` callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2897c716-03a0-4313-8c3d-ab46256b2a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8411\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7426\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6439\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5991\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5678\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5454\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5282\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5137\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5012\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4907\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"relu\", input_shape = [8]),\n",
    "    keras.layers.Dense(30, activation = \"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss = \"mse\", optimizer = keras.optimizers.SGD(learning_rate = 1e-3))\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs = 10, callbacks = [checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16413d95-5077-4dad-9d66-a8e236c98625",
   "metadata": {},
   "source": [
    "Moreover, if you use a validation set during training, you can set `save_best_only = True` when creating the `ModelCheckpoint`. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long & overfitting the training set: simply restore the last model saved after training, & this will be the best model on the validation set. The following code is a simple way to implement early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09364096-a098-4159-bce3-8e9ba99d7375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4815 - val_loss: 0.5337\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4740 - val_loss: 0.5253\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4670 - val_loss: 0.5218\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4613 - val_loss: 0.5136\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4553 - val_loss: 0.5135\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4508 - val_loss: 0.5091\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4474 - val_loss: 0.4998\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4428 - val_loss: 0.4976\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4388 - val_loss: 0.4931\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4942\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only = True)\n",
    "history = model.fit(X_train, y_train, epochs = 10,\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    callbacks = [checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # roll back to best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf92135-83c7-486c-934b-114f6354eb57",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to simply use the `EarlyStopping` callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the `patience` argument), & it will optionally roll back to the best model. You can combine both callbacks to save checkpoints of your model (in case your computer crashes) & interrupt training early when there is no more progress (to avoid wasting time & resources):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc91b786-dd97-4326-83e1-631cbbe39ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4356 - val_loss: 0.4946\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.4879\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4300 - val_loss: 0.4835\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4837\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.4797\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.4787\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4212 - val_loss: 0.4806\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4206 - val_loss: 0.4743\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.4732\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4161 - val_loss: 0.4717\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4144 - val_loss: 0.4704\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4127 - val_loss: 0.4691\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4114 - val_loss: 0.4690\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4100 - val_loss: 0.4681\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4102 - val_loss: 0.4619\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4081 - val_loss: 0.4708\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4074 - val_loss: 0.4611\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4055 - val_loss: 0.4593\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4046 - val_loss: 0.4580\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4598\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.4589\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.4546\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.4603\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.4529\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3991 - val_loss: 0.4512\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3974 - val_loss: 0.4510\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3968 - val_loss: 0.4521\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3962 - val_loss: 0.4469\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3949 - val_loss: 0.4471\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3938 - val_loss: 0.4497\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.4456\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4465\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3920 - val_loss: 0.4412\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3919 - val_loss: 0.4422\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3898 - val_loss: 0.4410\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3892 - val_loss: 0.4390\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3883 - val_loss: 0.4386\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3870 - val_loss: 0.4424\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3873 - val_loss: 0.4360\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3856 - val_loss: 0.4386\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3853 - val_loss: 0.4412\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.4340\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3837 - val_loss: 0.4444\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.4324\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3830 - val_loss: 0.4312\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3811 - val_loss: 0.4361\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3810 - val_loss: 0.4287\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.4304\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3784 - val_loss: 0.4294\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3777 - val_loss: 0.4304\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3772 - val_loss: 0.4245\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.4242\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3756 - val_loss: 0.4242\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3750 - val_loss: 0.4251\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3743 - val_loss: 0.4269\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3751 - val_loss: 0.4192\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3737 - val_loss: 0.4193\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.4246\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3723 - val_loss: 0.4178\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3707 - val_loss: 0.4187\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3702 - val_loss: 0.4158\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3704 - val_loss: 0.4191\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3689 - val_loss: 0.4149\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3689 - val_loss: 0.4183\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3672 - val_loss: 0.4178\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3668 - val_loss: 0.4147\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3660 - val_loss: 0.4172\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.4151\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3647 - val_loss: 0.4126\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3639 - val_loss: 0.4139\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3640 - val_loss: 0.4102\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3629 - val_loss: 0.4145\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3622 - val_loss: 0.4087\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3614 - val_loss: 0.4104\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.4078\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.4065\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.4084\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.4067\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.4065\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.4080\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.4048\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.4058\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3554 - val_loss: 0.4034\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3547 - val_loss: 0.4035\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3545 - val_loss: 0.4031\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.4052\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3529 - val_loss: 0.4033\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.4022\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.4008\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.4018\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.4014\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.3984\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3975\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3994\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.4005\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3480 - val_loss: 0.3953\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3478 - val_loss: 0.3951\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3956\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3955\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3984\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10, \n",
    "                                                  restore_best_weights = True)\n",
    "history = model.fit(X_train, y_train, epochs = 100,\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    callbacks = [checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb4d4c-9691-4474-bfb4-234e3d1786d1",
   "metadata": {},
   "source": [
    "The number of epochs can be set to a large value since training will stop automatically when there is no more progress. In this case, there is no need to restore the best model saved because the `EarlyStopping` callback will keep track of the best weights & retore them for you at the end of training.\n",
    "\n",
    "If you need extra control, you can easily write your own custom callbacks. As an example of how to do that, the following custom callback will display the ratio between the validation loss & the training loss during training (e.g., to detect overfitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9fce413-abdd-4e70-8b3b-cf4d50b291c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"]/logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08096b76-566b-450f-a6e6-61530a6909ee",
   "metadata": {},
   "source": [
    "As you might expect, you can implement `on_train_begin()`, `on_train_end()`, `on_epoch_begin()`, `on_epoch_end()`, `on_batch_begin()`, & `on_batch_end()`. Callbacks can also be used during evaluation & predictions, should you ever need them (e.g., for debugging). For evaluation, you should implement `on_test_begin()`, `on_test_end()`, `on_test_batch_begin()`, or `on_test_batch_end()` (called by `evaluate()`), & for prediction you should implement `on_predict_begin()`, `on_predict_end()`, `on_predict_batch_begin()`, or `on_predict_batch_end()` (called by `predict()`).\n",
    "\n",
    "Now let's take a look at one more tool you should definitely have in your toolbox when using tf.keras: Tensorboard.\n",
    "\n",
    "## Using Tensorboard for Visualisation\n",
    "\n",
    "Tensorboard is a great interactive visualisation tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualise the computation graph, analyse training statistics, view images generated by your model, visualise complex multidimensional data projected down to 3D & automatically clustered for you, & more! This tool is installed automatically when you install Tensorflow, so you already have it.\n",
    "\n",
    "To use it, you must modify your program so that it outputs the data you want to visualise to special binary log files called *event files*. Each binary data record is called a *summary*. The Tensorboard server will monitor the log directory, & it will automatically pick up the changes & update the visualisations: this allows you to visualise live data (with a short delay), such as the learning curves during training. In general, you want to point the Tensorboard server to a root log directory & configure your program so that it writes to a different subdirectory every time it runs. This way, the same Tensorboard server will allow you to visualise & compare data from multiple runs of your program, without getting everything mixed up.\n",
    "\n",
    "Let's start by defining the root log directory we will use for our Tensorboard logs, plus a small function that will generate a subdirectory path based on the current data & time so that it's different at every run. You may want to include extra information in the log directory name, such as hyperparameter values that you are testing, to make it easier to know what you are looking at in Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0e8fb2a-8fcf-43e0-b739-498dce82e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(\"/Users/jiehengyu/Desktop/ML Python/Chapter 10\", \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316687a-2c1e-4119-bc74-0a5dac39e459",
   "metadata": {},
   "source": [
    "The good news is that Keras provides a nice `TensorBoard()` callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a562e7c-a48d-4671-ba7a-4336a708f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.6016 - val_loss: 0.8829\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8016 - val_loss: 0.7757\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7186 - val_loss: 0.7301\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6726 - val_loss: 0.6943\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6360 - val_loss: 0.6645\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6055 - val_loss: 0.6363\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5787 - val_loss: 0.6139\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5561 - val_loss: 0.5951\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5365 - val_loss: 0.5807\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5198 - val_loss: 0.5672\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5056 - val_loss: 0.5540\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4936 - val_loss: 0.5416\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4829 - val_loss: 0.5360\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4742 - val_loss: 0.5246\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4667 - val_loss: 0.5175\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4604 - val_loss: 0.5140\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4545 - val_loss: 0.5070\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4495 - val_loss: 0.5029\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4448 - val_loss: 0.4979\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4405 - val_loss: 0.4966\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4370 - val_loss: 0.4910\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4335 - val_loss: 0.4864\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4850\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4278 - val_loss: 0.4826\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4250 - val_loss: 0.4798\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.4767\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.4754\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4738\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.4712\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.4683\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"relu\", input_shape = [8]),\n",
    "    keras.layers.Dense(30, activation = \"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss = \"mse\", optimizer = keras.optimizers.SGD(learning_rate = 1e-3))\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs = 30,\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    callbacks = [tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2a7f95-4908-4fdf-a2aa-2c07c669970c",
   "metadata": {},
   "source": [
    "That's all there is to it! It could hardly be easier to use. If you run this code, the `TensorBoard()` callback will take care to creating the log directory for you (along with its parent directories if needed), & during training it will create event files & write summarise to them. After running the program a second time (perhaps changing some hyperparameter value), you will end up with a directory structure similar to this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929cc1ba-3f9b-46da-a65a-6313748a0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_logs/\n",
    "|-- run_2019_06_07-15_15_22\n",
    "|    |-- train\n",
    "|    |    |-- events.out.tfevents.1559891732.mycomputer.local.38511.694049.v2\n",
    "|    |    |-- events.out.tfevents.1559891732.mycomputer.local.profile-empty\n",
    "|    |    |-- plugins/profile/2019-06-07_15-15-32\n",
    "|    |        |-- local.trace\n",
    "|    |-- validation\n",
    "|         |-- events.out.tfevents.1559891733.mycomputer.local.38511.696430.v2\n",
    "|-- run_2019_06_07-15_15_49\n",
    "     |-- [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e6c4d-b542-4403-8136-3800c17e29d1",
   "metadata": {},
   "source": [
    "There is one directory per run, each containing one subdirectory for training logs & one for validation logs. Both contain event files, but the training logs also include profiling traces: this allows Tensorboard to show you exactly how much time the model spent on each part of your model, across all your devices, which is great for locating performance bottelnecks.\n",
    "\n",
    "Next, you need to start the Tensorboard server. One way to do this is by running a command in a terminal. If you installed Tensorflow within a virtualenv, you should activate it. Next, run the following command at the root of the project (or from anywhere else, as long as you point to the appropriate log directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9c757-da1a-40c3-866d-e7d4ac5debd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f211ea4-ff84-4c54-92e5-58f25d376c39",
   "metadata": {},
   "source": [
    "If your shell cannot find the *tensorboard* script, then you must update your `PATH` environment variable so that it contains the directory in which the script was installed (alternatively, you can just replace `tensorboard` in the command line with `python3 -m tensorboard.main`). Once the server is up, you can open a web browser & go to *http://localhost:6006*. \n",
    "\n",
    "Alternatively, you can use Tensorbaord directly within jupyter, by running the following commands. The first line loads the Tensorboard extension, & the second line starts a Tensorboard server on port 6006 (unless it is already started) & connects to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1077314d-7089-40e3-a314-f194e627908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4f27e19efcd46ea3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4f27e19efcd46ea3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --/Users/jiehengyu/Desktop/ML Python/Chapter 10/my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece7ae2-a3f0-4c71-b2bc-7c820db455ca",
   "metadata": {},
   "source": [
    "Either way, you should see Tensorboard's web interface. Click the SCALARS tab to view the learning curves. At the bottom left, select the logs you want to visualise (e.g., the training logs from the first & second run), & click the `epoch_loss` scalar. Notice that the training loss went down nicely during both runs, but the second run went down much faster. Indeed, we used a learning rate of 0.05 (`optimizer = keras.optimizers.SGD(lr = 0.05)`) instead of 0.001.\n",
    "\n",
    "You can also visualise the whole graph, the learned weights (projected to 3D, or the profiling traces. The `TensorBoard()` callback has options to log extra data too, such as embeddings.\n",
    "\n",
    "Additionally, Tensorflow offers a lower-level API in the `tf.summary` package. The following code creates a `SummaryWriter` using the `create_file_writer()` function, & it uses this writer as a context to log scalars, histograms, images, audio, & text, all of which can then be visualised using Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4246d25e-dd67-45a8-bc1e-874ec70f0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step = step)\n",
    "        data = (np.random.randn(100) + 2) * step / 100\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets = 50, step = step)\n",
    "        images = np.random.rand(2, 32, 32, 3)\n",
    "        tf.summary.image(\"my_images\", images * step / 1000, step = step)\n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step ** 2)]\n",
    "        tf.summary.text(\"my_text\", texts, step = step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate = 48000, step = step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576f056-374f-400f-b373-21cb386c062c",
   "metadata": {},
   "source": [
    "This is actually a useful visualisation tool to have, even beyond Tensorflow or deep learning.\n",
    "\n",
    "Let's summarise what you've learned so far in this lesson: we saw where neural nets came from, what an MLP is & how you can use it for classification & regression, how to use tf.keras's sequential API to build MLPs, & how to use the functional API or the subclassing API to build more complex model architectures. You learned how to save & restore a model & how to use callbacks for checkpointing, early stopping, & more. Finally, you learned how to use Tensorboard for visualisation. You can already go ahead & use neural networks to tackle many problems. However, you may wonder how to choose the number of hidden layers, the number of neurons in the network, & all the other hyperparameters. Let's look at this now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46edb083-0ed5-4120-91b1-a1fc39581588",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ef2a6-c45b-453b-9118-d318b1c41f09",
   "metadata": {},
   "source": [
    "# Fine-Tuning Neural Network Hyperparameters\n",
    "\n",
    "The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak. Not only can you use any imaginable network architecture, but even in a simple MLP you can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialisation logic, & much more. How do you know what combination of hyperparameters is the best for your task?\n",
    "\n",
    "One option is to simply try many combinations of hyperparameters & see which one works best on the validation set (or use K-fold cross-validation). For example, we can use `GridSearchCV` or `RandomizedSearchCV` to explore the hyperparameter space. To do this, we need to wrap our Keras models in objects that mimic regular scikit-learn regressors. The first step is to create a function that will build & compile a Keras model, given a set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe6e3137-ef0b-455f-8c73-0f3a25d72a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3, input_shape = [8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape = input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation = \"relu\"))\n",
    "    optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318d7eb-57a8-4c61-96b9-d5f67e3095c4",
   "metadata": {},
   "source": [
    "This function creates a simple `Sequential` model for univariate regression (only one output neuron), with the given input shape & the given number of hidden layers & neurons, & it compiles it using an `SGD` optimizer configured with the specified learning rate. It is good practice to provide reasonable defaults to as many hyperparameters as you can, as scikit-learn does.\n",
    "\n",
    "Next, let's create a `KerasRegressor` based on this `build_model()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d62d85ed-4211-4f19-8049-7a907df5fe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_15873/1709004121.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67e67c-4726-4820-8e5d-f89dc77359b7",
   "metadata": {},
   "source": [
    "The `KerasRegressor` object is a thin wrapper around the keras model built using `build_model()`. Since we did not specify any hyperparameters when creating it, it will use the default hyperparameters we defined in `build_model()`. Now, we can use this object like a regular scikit-learn regressor: we can train it using its `fit()` method, then evaluate it using its `score()` method, & use it to make predictions using its `predict()` method, as you can see in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b942293b-8f53-40fb-a8de-31f06e3b97dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 2ms/step - loss: 4.8802 - val_loss: 4.6707\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 4.6691 - val_loss: 4.4642\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 4.4572 - val_loss: 4.2600\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 4.2497 - val_loss: 4.0606\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 4.0481 - val_loss: 3.8668\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 3.8524 - val_loss: 3.6784\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.6625 - val_loss: 3.4959\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.4796 - val_loss: 3.3205\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 3.3035 - val_loss: 3.1517\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.1332 - val_loss: 2.9883\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.9676 - val_loss: 2.8287\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.8052 - val_loss: 2.6721\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.6456 - val_loss: 2.5191\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.4896 - val_loss: 2.3707\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.3387 - val_loss: 2.2283\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.1946 - val_loss: 2.0932\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.0576 - val_loss: 1.9649\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.9277 - val_loss: 1.8438\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.8054 - val_loss: 1.7311\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.6922 - val_loss: 1.6272\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.5880 - val_loss: 1.5318\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.4923 - val_loss: 1.4443\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.4047 - val_loss: 1.3644\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3246 - val_loss: 1.2914\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.2513 - val_loss: 1.2249\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.1843 - val_loss: 1.1643\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.1234 - val_loss: 1.1092\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0681 - val_loss: 1.0594\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.0181 - val_loss: 1.0144\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.9730 - val_loss: 0.9739\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9325 - val_loss: 0.9376\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8962 - val_loss: 0.9052\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8636 - val_loss: 0.8762\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8344 - val_loss: 0.8503\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8084 - val_loss: 0.8272\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7853 - val_loss: 0.8066\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7646 - val_loss: 0.7883\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7463 - val_loss: 0.7720\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7301 - val_loss: 0.7577\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7157 - val_loss: 0.7450\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7030 - val_loss: 0.7337\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6917 - val_loss: 0.7238\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6816 - val_loss: 0.7151\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6728 - val_loss: 0.7073\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6648 - val_loss: 0.7005\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6578 - val_loss: 0.6943\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6515 - val_loss: 0.6889\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6458 - val_loss: 0.6840\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6407 - val_loss: 0.6797\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6362 - val_loss: 0.6758\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6320 - val_loss: 0.6723\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6282 - val_loss: 0.6691\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6248 - val_loss: 0.6662\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6216 - val_loss: 0.6635\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6187 - val_loss: 0.6610\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6160 - val_loss: 0.6587\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6134 - val_loss: 0.6566\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6111 - val_loss: 0.6547\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6089 - val_loss: 0.6528\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6068 - val_loss: 0.6511\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6048 - val_loss: 0.6494\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6029 - val_loss: 0.6479\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6011 - val_loss: 0.6464\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5994 - val_loss: 0.6450\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5978 - val_loss: 0.6436\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5962 - val_loss: 0.6423\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5947 - val_loss: 0.6410\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5932 - val_loss: 0.6398\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5918 - val_loss: 0.6386\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5904 - val_loss: 0.6374\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5890 - val_loss: 0.6363\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5877 - val_loss: 0.6352\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5864 - val_loss: 0.6341\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5852 - val_loss: 0.6330\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5839 - val_loss: 0.6320\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5827 - val_loss: 0.6310\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5815 - val_loss: 0.6300\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5804 - val_loss: 0.6290\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5792 - val_loss: 0.6280\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5781 - val_loss: 0.6270\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5770 - val_loss: 0.6261\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5759 - val_loss: 0.6251\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5748 - val_loss: 0.6242\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5738 - val_loss: 0.6233\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5728 - val_loss: 0.6224\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5717 - val_loss: 0.6215\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5707 - val_loss: 0.6206\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5697 - val_loss: 0.6197\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5688 - val_loss: 0.6189\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5678 - val_loss: 0.6180\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5668 - val_loss: 0.6172\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5659 - val_loss: 0.6164\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5650 - val_loss: 0.6156\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5641 - val_loss: 0.6147\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5632 - val_loss: 0.6139\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5623 - val_loss: 0.6131\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5614 - val_loss: 0.6123\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5605 - val_loss: 0.6116\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5597 - val_loss: 0.6108\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5588 - val_loss: 0.6100\n",
      "162/162 [==============================] - 0s 886us/step - loss: 0.5540\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbe7d2f2040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs = 100,\n",
    "              validation_data = (X_val, y_val),\n",
    "              callbacks = [keras.callbacks.EarlyStopping(patience = 10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c0d5b-a18c-4369-9634-f89d5acb1158",
   "metadata": {},
   "source": [
    "Note that any extra parameter you pass to the `fit(0` method will get passed to the underlying Keras model. Also note that the score will be the opposite of the MSE because scikit-learn wants scores, not losses (i.e., higher should be better).\n",
    "\n",
    "We don't want to train & evaluate a single model like this, though we want to train hundred of variants & see which one performs best on the validation set. Since there are many hyperparameters, it is perferable to use a randomised search rater than grid search. Let's try to explore the number of hidden layers, the number of neurons, & the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17fd70fd-acb6-419f-877e-60828157dcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 61/242 [======>.......................] - ETA: 0s - loss: 6.8817 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 877us/step - loss: 6.2043\n",
      "Epoch 1/100\n",
      " 58/242 [======>.......................] - ETA: 0s - loss: 5.8591 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 819us/step - loss: 6.6216\n",
      "Epoch 1/100\n",
      " 66/242 [=======>......................] - ETA: 0s - loss: 6.3708 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 773us/step - loss: 6.4715\n",
      "Epoch 1/100\n",
      " 64/242 [======>.......................] - ETA: 0s - loss: 6.0705 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 744us/step - loss: 6.2043\n",
      "Epoch 1/100\n",
      " 64/242 [======>.......................] - ETA: 0s - loss: 6.8031 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 993us/step - loss: 6.6216\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 857us/step - loss: 6.4715\n",
      "Epoch 1/100\n",
      " 58/242 [======>.......................] - ETA: 0s - loss: 6.6576 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 6.2043\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 26s - loss: 7.3232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 970us/step - loss: 6.6216\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 24s - loss: 5.8500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 892us/step - loss: 6.4715\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 5.1751 - val_loss: 5.0099\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1656 - val_loss: 5.0007\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1561 - val_loss: 4.9915\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1464 - val_loss: 4.9821\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1368 - val_loss: 4.9728\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1270 - val_loss: 4.9633\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.1173 - val_loss: 4.9539\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1074 - val_loss: 4.9443\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0976 - val_loss: 4.9348\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0877 - val_loss: 4.9252\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0777 - val_loss: 4.9155\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0678 - val_loss: 4.9059\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0577 - val_loss: 4.8962\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0477 - val_loss: 4.8864\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0376 - val_loss: 4.8766\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0275 - val_loss: 4.8668\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0173 - val_loss: 4.8570\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0071 - val_loss: 4.8471\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9969 - val_loss: 4.8372\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9867 - val_loss: 4.8272\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9764 - val_loss: 4.8173\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9661 - val_loss: 4.8073\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9558 - val_loss: 4.7973\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9455 - val_loss: 4.7873\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9351 - val_loss: 4.7773\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9248 - val_loss: 4.7672\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9144 - val_loss: 4.7571\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9040 - val_loss: 4.7470\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8935 - val_loss: 4.7369\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8831 - val_loss: 4.7268\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8726 - val_loss: 4.7167\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8622 - val_loss: 4.7065\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8517 - val_loss: 4.6963\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8412 - val_loss: 4.6862\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8306 - val_loss: 4.6760\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8201 - val_loss: 4.6658\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8096 - val_loss: 4.6556\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7990 - val_loss: 4.6454\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7885 - val_loss: 4.6351\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7779 - val_loss: 4.6249\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7673 - val_loss: 4.6147\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7567 - val_loss: 4.6044\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7461 - val_loss: 4.5942\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7356 - val_loss: 4.5839\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7250 - val_loss: 4.5737\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7144 - val_loss: 4.5634\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7038 - val_loss: 4.5532\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.6932 - val_loss: 4.5430\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6826 - val_loss: 4.5327\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.6720 - val_loss: 4.5225\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.6614 - val_loss: 4.5122\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.6508 - val_loss: 4.5020\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6402 - val_loss: 4.4917\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6297 - val_loss: 4.4815\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6191 - val_loss: 4.4713\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6085 - val_loss: 4.4611\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5980 - val_loss: 4.4509\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5874 - val_loss: 4.4406\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5769 - val_loss: 4.4304\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5663 - val_loss: 4.4202\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5558 - val_loss: 4.4101\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5452 - val_loss: 4.3999\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5347 - val_loss: 4.3897\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5242 - val_loss: 4.3795\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5137 - val_loss: 4.3694\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5032 - val_loss: 4.3592\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4927 - val_loss: 4.3491\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4822 - val_loss: 4.3389\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4717 - val_loss: 4.3288\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.4612 - val_loss: 4.3187\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.4508 - val_loss: 4.3086\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 4.4403 - val_loss: 4.2985\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.4299 - val_loss: 4.2884\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.4195 - val_loss: 4.2783\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.4091 - val_loss: 4.2683\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3987 - val_loss: 4.2582\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3883 - val_loss: 4.2482\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3780 - val_loss: 4.2382\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3676 - val_loss: 4.2282\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3573 - val_loss: 4.2182\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3469 - val_loss: 4.2082\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3366 - val_loss: 4.1983\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3263 - val_loss: 4.1883\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3160 - val_loss: 4.1784\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3058 - val_loss: 4.1685\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2955 - val_loss: 4.1586\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2853 - val_loss: 4.1487\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2751 - val_loss: 4.1388\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2648 - val_loss: 4.1290\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2547 - val_loss: 4.1192\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2445 - val_loss: 4.1094\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2343 - val_loss: 4.0996\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2242 - val_loss: 4.0898\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2141 - val_loss: 4.0800\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2040 - val_loss: 4.0703\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1939 - val_loss: 4.0606\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1838 - val_loss: 4.0508\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1738 - val_loss: 4.0412\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1637 - val_loss: 4.0315\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1537 - val_loss: 4.0218\n",
      "121/121 [==============================] - 0s 778us/step - loss: 4.0067\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 5.0004 - val_loss: 5.0222\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9912 - val_loss: 5.0130\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9819 - val_loss: 5.0037\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9725 - val_loss: 4.9943\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9630 - val_loss: 4.9849\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9535 - val_loss: 4.9754\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9440 - val_loss: 4.9659\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9344 - val_loss: 4.9563\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9248 - val_loss: 4.9466\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9151 - val_loss: 4.9369\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9053 - val_loss: 4.9272\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8955 - val_loss: 4.9174\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8857 - val_loss: 4.9076\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8758 - val_loss: 4.8978\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8659 - val_loss: 4.8879\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8560 - val_loss: 4.8779\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8460 - val_loss: 4.8680\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8361 - val_loss: 4.8580\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8260 - val_loss: 4.8479\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8160 - val_loss: 4.8379\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8059 - val_loss: 4.8278\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7959 - val_loss: 4.8177\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7857 - val_loss: 4.8076\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7756 - val_loss: 4.7975\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7655 - val_loss: 4.7873\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7553 - val_loss: 4.7771\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7451 - val_loss: 4.7670\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7349 - val_loss: 4.7568\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7247 - val_loss: 4.7465\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7144 - val_loss: 4.7363\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7042 - val_loss: 4.7261\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6939 - val_loss: 4.7158\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6837 - val_loss: 4.7056\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6734 - val_loss: 4.6953\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6631 - val_loss: 4.6851\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6529 - val_loss: 4.6748\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6426 - val_loss: 4.6645\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6323 - val_loss: 4.6542\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6220 - val_loss: 4.6440\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6117 - val_loss: 4.6337\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6015 - val_loss: 4.6234\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5912 - val_loss: 4.6131\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5809 - val_loss: 4.6029\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5706 - val_loss: 4.5926\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5604 - val_loss: 4.5823\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5501 - val_loss: 4.5721\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5398 - val_loss: 4.5618\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5296 - val_loss: 4.5515\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5193 - val_loss: 4.5413\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5091 - val_loss: 4.5310\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4989 - val_loss: 4.5208\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4886 - val_loss: 4.5106\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4784 - val_loss: 4.5003\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4682 - val_loss: 4.4901\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4580 - val_loss: 4.4799\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4478 - val_loss: 4.4697\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4376 - val_loss: 4.4595\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4274 - val_loss: 4.4493\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4172 - val_loss: 4.4391\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4070 - val_loss: 4.4290\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3969 - val_loss: 4.4188\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3867 - val_loss: 4.4087\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3766 - val_loss: 4.3985\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3665 - val_loss: 4.3884\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3563 - val_loss: 4.3783\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3462 - val_loss: 4.3682\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3361 - val_loss: 4.3581\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3261 - val_loss: 4.3481\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3160 - val_loss: 4.3380\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3060 - val_loss: 4.3280\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2959 - val_loss: 4.3180\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2859 - val_loss: 4.3079\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2759 - val_loss: 4.2980\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2659 - val_loss: 4.2880\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2559 - val_loss: 4.2780\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2459 - val_loss: 4.2681\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2360 - val_loss: 4.2581\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2261 - val_loss: 4.2482\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2161 - val_loss: 4.2383\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2062 - val_loss: 4.2284\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1963 - val_loss: 4.2186\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1865 - val_loss: 4.2087\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1766 - val_loss: 4.1989\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1668 - val_loss: 4.1891\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1570 - val_loss: 4.1793\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1472 - val_loss: 4.1695\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1374 - val_loss: 4.1597\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1277 - val_loss: 4.1500\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1179 - val_loss: 4.1402\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1082 - val_loss: 4.1305\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.0985 - val_loss: 4.1208\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0888 - val_loss: 4.1112\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0791 - val_loss: 4.1015\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0695 - val_loss: 4.0919\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0599 - val_loss: 4.0823\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0503 - val_loss: 4.0727\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0407 - val_loss: 4.0631\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0311 - val_loss: 4.0535\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0216 - val_loss: 4.0440\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0120 - val_loss: 4.0345\n",
      "121/121 [==============================] - 0s 714us/step - loss: 4.3329\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 5.2138 - val_loss: 5.0360\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.2050 - val_loss: 5.0274\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1961 - val_loss: 5.0187\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1872 - val_loss: 5.0100\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1782 - val_loss: 5.0013\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1692 - val_loss: 4.9924\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1601 - val_loss: 4.9836\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1509 - val_loss: 4.9746\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1417 - val_loss: 4.9657\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1325 - val_loss: 4.9567\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1232 - val_loss: 4.9476\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1139 - val_loss: 4.9385\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1045 - val_loss: 4.9294\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0951 - val_loss: 4.9202\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0856 - val_loss: 4.9110\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0761 - val_loss: 4.9018\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0666 - val_loss: 4.8925\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0570 - val_loss: 4.8832\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0474 - val_loss: 4.8738\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0378 - val_loss: 4.8645\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.0282 - val_loss: 4.8551\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0185 - val_loss: 4.8457\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0087 - val_loss: 4.8362\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9990 - val_loss: 4.8267\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9892 - val_loss: 4.8173\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9794 - val_loss: 4.8078\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9696 - val_loss: 4.7982\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9598 - val_loss: 4.7887\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9500 - val_loss: 4.7791\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9401 - val_loss: 4.7696\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9303 - val_loss: 4.7600\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9204 - val_loss: 4.7504\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9105 - val_loss: 4.7407\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9006 - val_loss: 4.7311\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8906 - val_loss: 4.7215\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8807 - val_loss: 4.7118\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8708 - val_loss: 4.7022\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8609 - val_loss: 4.6925\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8509 - val_loss: 4.6829\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8410 - val_loss: 4.6732\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8310 - val_loss: 4.6635\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8210 - val_loss: 4.6539\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8111 - val_loss: 4.6442\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8011 - val_loss: 4.6345\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7912 - val_loss: 4.6249\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7812 - val_loss: 4.6152\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7713 - val_loss: 4.6056\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7613 - val_loss: 4.5959\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7514 - val_loss: 4.5862\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7415 - val_loss: 4.5766\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7315 - val_loss: 4.5669\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7216 - val_loss: 4.5573\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7116 - val_loss: 4.5476\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7017 - val_loss: 4.5380\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6918 - val_loss: 4.5283\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6818 - val_loss: 4.5187\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6719 - val_loss: 4.5091\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6620 - val_loss: 4.4994\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6521 - val_loss: 4.4898\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6422 - val_loss: 4.4802\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.6323 - val_loss: 4.4706\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6224 - val_loss: 4.4610\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6126 - val_loss: 4.4514\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6027 - val_loss: 4.4418\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5928 - val_loss: 4.4322\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5829 - val_loss: 4.4227\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5731 - val_loss: 4.4131\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5632 - val_loss: 4.4035\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5534 - val_loss: 4.3940\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5436 - val_loss: 4.3844\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5338 - val_loss: 4.3749\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5240 - val_loss: 4.3654\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5142 - val_loss: 4.3558\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5044 - val_loss: 4.3463\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.4946 - val_loss: 4.3368\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4848 - val_loss: 4.3273\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4751 - val_loss: 4.3179\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4653 - val_loss: 4.3084\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.4556 - val_loss: 4.2989\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4459 - val_loss: 4.2895\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4362 - val_loss: 4.2800\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4265 - val_loss: 4.2706\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4168 - val_loss: 4.2612\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4071 - val_loss: 4.2518\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3974 - val_loss: 4.2424\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3878 - val_loss: 4.2330\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3781 - val_loss: 4.2236\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3685 - val_loss: 4.2143\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3589 - val_loss: 4.2049\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3493 - val_loss: 4.1956\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3397 - val_loss: 4.1863\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3301 - val_loss: 4.1770\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3205 - val_loss: 4.1677\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3110 - val_loss: 4.1584\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3014 - val_loss: 4.1491\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2919 - val_loss: 4.1399\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2824 - val_loss: 4.1306\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2729 - val_loss: 4.1214\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.2634 - val_loss: 4.1121\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.2539 - val_loss: 4.1029\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 4.0758\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 24s - loss: 8.1601"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 6.2043\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 25s - loss: 7.8975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 3ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 810us/step - loss: 6.6216\n",
      "Epoch 1/100\n",
      " 67/242 [=======>......................] - ETA: 0s - loss: 6.3151 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 773us/step - loss: 6.4715\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 5.1877 - val_loss: 5.0041\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1454 - val_loss: 4.9610\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1001 - val_loss: 4.9150\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0517 - val_loss: 4.8659\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0001 - val_loss: 4.8137\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9452 - val_loss: 4.7581\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8869 - val_loss: 4.6992\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8251 - val_loss: 4.6369\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7596 - val_loss: 4.5710\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6905 - val_loss: 4.5017\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6180 - val_loss: 4.4290\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5420 - val_loss: 4.3529\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4626 - val_loss: 4.2737\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3800 - val_loss: 4.1914\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2943 - val_loss: 4.1061\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2055 - val_loss: 4.0179\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1139 - val_loss: 3.9271\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0195 - val_loss: 3.8338\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9225 - val_loss: 3.7379\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8231 - val_loss: 3.6398\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7213 - val_loss: 3.5397\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6175 - val_loss: 3.4377\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5118 - val_loss: 3.3341\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4047 - val_loss: 3.2294\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2964 - val_loss: 3.1239\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1874 - val_loss: 3.0181\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0781 - val_loss: 2.9124\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9692 - val_loss: 2.8074\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8608 - val_loss: 2.7032\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7531 - val_loss: 2.6001\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6466 - val_loss: 2.4982\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5414 - val_loss: 2.3980\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4383 - val_loss: 2.3003\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3381 - val_loss: 2.2062\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2416 - val_loss: 2.1158\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1491 - val_loss: 2.0295\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0606 - val_loss: 1.9473\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9765 - val_loss: 1.8694\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8966 - val_loss: 1.7958\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8210 - val_loss: 1.7264\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7496 - val_loss: 1.6611\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6825 - val_loss: 1.6001\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6199 - val_loss: 1.5434\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5616 - val_loss: 1.4909\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5076 - val_loss: 1.4425\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4577 - val_loss: 1.3980\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4117 - val_loss: 1.3571\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3692 - val_loss: 1.3195\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3300 - val_loss: 1.2848\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2937 - val_loss: 1.2528\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2599 - val_loss: 1.2231\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2287 - val_loss: 1.1958\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1998 - val_loss: 1.1706\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1732 - val_loss: 1.1475\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1487 - val_loss: 1.1263\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1262 - val_loss: 1.1068\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1054 - val_loss: 1.0891\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0864 - val_loss: 1.0727\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0688 - val_loss: 1.0576\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0524 - val_loss: 1.0435\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0368 - val_loss: 1.0300\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0218 - val_loss: 1.0169\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0070 - val_loss: 1.0040\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9927 - val_loss: 0.9914\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9787 - val_loss: 0.9792\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9654 - val_loss: 0.9676\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9530 - val_loss: 0.9569\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9415 - val_loss: 0.9471\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9309 - val_loss: 0.9381\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9211 - val_loss: 0.9297\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9121 - val_loss: 0.9220\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9037 - val_loss: 0.9149\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8959 - val_loss: 0.9083\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8887 - val_loss: 0.9022\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8820 - val_loss: 0.8964\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8757 - val_loss: 0.8911\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8698 - val_loss: 0.8861\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8642 - val_loss: 0.8813\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8590 - val_loss: 0.8769\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8541 - val_loss: 0.8727\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8495 - val_loss: 0.8687\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8451 - val_loss: 0.8649\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8409 - val_loss: 0.8613\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8369 - val_loss: 0.8579\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8331 - val_loss: 0.8546\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8295 - val_loss: 0.8515\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8260 - val_loss: 0.8485\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8226 - val_loss: 0.8456\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8194 - val_loss: 0.8428\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8163 - val_loss: 0.8401\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8133 - val_loss: 0.8374\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8104 - val_loss: 0.8349\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8076 - val_loss: 0.8324\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8048 - val_loss: 0.8301\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8022 - val_loss: 0.8277\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7996 - val_loss: 0.8255\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7971 - val_loss: 0.8233\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7946 - val_loss: 0.8211\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7923 - val_loss: 0.8190\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7899 - val_loss: 0.8170\n",
      "121/121 [==============================] - 0s 768us/step - loss: 0.7878\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 5.0078 - val_loss: 5.0132\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9695 - val_loss: 4.9730\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9287 - val_loss: 4.9303\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8855 - val_loss: 4.8852\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8398 - val_loss: 4.8376\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7918 - val_loss: 4.7877\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7414 - val_loss: 4.7353\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6886 - val_loss: 4.6805\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6334 - val_loss: 4.6232\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5758 - val_loss: 4.5637\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5159 - val_loss: 4.5018\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4537 - val_loss: 4.4376\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3891 - val_loss: 4.3712\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3223 - val_loss: 4.3024\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2531 - val_loss: 4.2314\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1817 - val_loss: 4.1581\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1081 - val_loss: 4.0827\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0326 - val_loss: 4.0054\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9552 - val_loss: 3.9262\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8762 - val_loss: 3.8457\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7958 - val_loss: 3.7639\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7143 - val_loss: 3.6812\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6318 - val_loss: 3.5978\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5487 - val_loss: 3.5137\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4650 - val_loss: 3.4292\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3809 - val_loss: 3.3446\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2967 - val_loss: 3.2601\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2126 - val_loss: 3.1760\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1289 - val_loss: 3.0924\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0458 - val_loss: 3.0095\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9633 - val_loss: 2.9275\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8815 - val_loss: 2.8462\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8004 - val_loss: 2.7657\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7202 - val_loss: 2.6865\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6412 - val_loss: 2.6086\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5636 - val_loss: 2.5323\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4876 - val_loss: 2.4579\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4134 - val_loss: 2.3852\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3409 - val_loss: 2.3144\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2700 - val_loss: 2.2453\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2008 - val_loss: 2.1781\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1335 - val_loss: 2.1127\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0680 - val_loss: 2.0494\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0045 - val_loss: 1.9880\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9430 - val_loss: 1.9286\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8834 - val_loss: 1.8712\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8258 - val_loss: 1.8159\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7703 - val_loss: 1.7627\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7167 - val_loss: 1.7118\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6654 - val_loss: 1.6631\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6165 - val_loss: 1.6167\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5699 - val_loss: 1.5728\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5259 - val_loss: 1.5314\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4842 - val_loss: 1.4923\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4450 - val_loss: 1.4554\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4079 - val_loss: 1.4207\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3730 - val_loss: 1.3879\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3399 - val_loss: 1.3571\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3087 - val_loss: 1.3280\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2793 - val_loss: 1.3006\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2515 - val_loss: 1.2748\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2253 - val_loss: 1.2504\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2005 - val_loss: 1.2275\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1771 - val_loss: 1.2060\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1551 - val_loss: 1.1857\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1344 - val_loss: 1.1667\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1151 - val_loss: 1.1489\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0970 - val_loss: 1.1323\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0800 - val_loss: 1.1166\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0639 - val_loss: 1.1018\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0488 - val_loss: 1.0878\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0345 - val_loss: 1.0747\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0210 - val_loss: 1.0624\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0085 - val_loss: 1.0510\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9968 - val_loss: 1.0403\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9859 - val_loss: 1.0304\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9759 - val_loss: 1.0213\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9666 - val_loss: 1.0129\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9580 - val_loss: 1.0051\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9501 - val_loss: 0.9979\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9427 - val_loss: 0.9913\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9360 - val_loss: 0.9851\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9296 - val_loss: 0.9794\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9237 - val_loss: 0.9740\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9182 - val_loss: 0.9690\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9130 - val_loss: 0.9643\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9081 - val_loss: 0.9598\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9035 - val_loss: 0.9556\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8992 - val_loss: 0.9517\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8951 - val_loss: 0.9479\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8912 - val_loss: 0.9444\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8876 - val_loss: 0.9410\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8841 - val_loss: 0.9378\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8807 - val_loss: 0.9347\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8775 - val_loss: 0.9317\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8745 - val_loss: 0.9289\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8715 - val_loss: 0.9261\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8687 - val_loss: 0.9235\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8660 - val_loss: 0.9210\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8633 - val_loss: 0.9185\n",
      "121/121 [==============================] - 0s 780us/step - loss: 0.9589\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 5.1962 - val_loss: 4.9997\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1531 - val_loss: 4.9564\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1074 - val_loss: 4.9104\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0590 - val_loss: 4.8619\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0079 - val_loss: 4.8108\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9541 - val_loss: 4.7571\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8977 - val_loss: 4.7008\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8386 - val_loss: 4.6419\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.7767 - val_loss: 4.5802\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7119 - val_loss: 4.5159\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6443 - val_loss: 4.4488\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5740 - val_loss: 4.3790\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5008 - val_loss: 4.3066\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4248 - val_loss: 4.2315\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3460 - val_loss: 4.1538\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2646 - val_loss: 4.0737\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1809 - val_loss: 3.9914\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0949 - val_loss: 3.9071\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0068 - val_loss: 3.8209\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9167 - val_loss: 3.7329\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8249 - val_loss: 3.6434\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7314 - val_loss: 3.5523\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6363 - val_loss: 3.4597\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5397 - val_loss: 3.3657\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4419 - val_loss: 3.2709\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3432 - val_loss: 3.1753\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2439 - val_loss: 3.0794\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1444 - val_loss: 2.9833\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0449 - val_loss: 2.8875\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9458 - val_loss: 2.7923\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8475 - val_loss: 2.6979\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7500 - val_loss: 2.6047\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6539 - val_loss: 2.5131\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5596 - val_loss: 2.4236\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4675 - val_loss: 2.3365\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3781 - val_loss: 2.2522\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2915 - val_loss: 2.1708\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2083 - val_loss: 2.0927\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1285 - val_loss: 2.0182\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0523 - val_loss: 1.9473\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9800 - val_loss: 1.8805\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9117 - val_loss: 1.8175\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8475 - val_loss: 1.7584\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7873 - val_loss: 1.7032\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7308 - val_loss: 1.6514\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6778 - val_loss: 1.6028\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6279 - val_loss: 1.5571\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5809 - val_loss: 1.5140\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5365 - val_loss: 1.4733\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4949 - val_loss: 1.4353\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4560 - val_loss: 1.4000\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4200 - val_loss: 1.3675\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3868 - val_loss: 1.3377\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3565 - val_loss: 1.3104\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3287 - val_loss: 1.2855\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3031 - val_loss: 1.2625\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2797 - val_loss: 1.2415\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2581 - val_loss: 1.2222\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2383 - val_loss: 1.2044\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2200 - val_loss: 1.1881\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2032 - val_loss: 1.1729\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1875 - val_loss: 1.1588\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1729 - val_loss: 1.1457\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1593 - val_loss: 1.1334\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1466 - val_loss: 1.1219\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1346 - val_loss: 1.1110\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1231 - val_loss: 1.1006\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1122 - val_loss: 1.0905\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1015 - val_loss: 1.0806\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0910 - val_loss: 1.0707\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0805 - val_loss: 1.0607\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0697 - val_loss: 1.0502\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0579 - val_loss: 1.0378\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0439 - val_loss: 1.0237\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0291 - val_loss: 1.0095\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0145 - val_loss: 0.9958\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0004 - val_loss: 0.9825\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9868 - val_loss: 0.9697\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9737 - val_loss: 0.9574\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9611 - val_loss: 0.9456\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9489 - val_loss: 0.9345\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9375 - val_loss: 0.9242\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9269 - val_loss: 0.9146\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9171 - val_loss: 0.9057\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9079 - val_loss: 0.8974\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8993 - val_loss: 0.8897\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8913 - val_loss: 0.8824\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8837 - val_loss: 0.8755\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8766 - val_loss: 0.8690\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8698 - val_loss: 0.8629\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8633 - val_loss: 0.8570\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8572 - val_loss: 0.8513\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8512 - val_loss: 0.8458\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8455 - val_loss: 0.8405\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8400 - val_loss: 0.8353\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8345 - val_loss: 0.8302\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8291 - val_loss: 0.8250\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8238 - val_loss: 0.8199\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8183 - val_loss: 0.8146\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8128 - val_loss: 0.8091\n",
      "121/121 [==============================] - 0s 772us/step - loss: 0.7490\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 4.3101 - val_loss: 3.5145\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0790 - val_loss: 2.4609\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0832 - val_loss: 1.6518\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3866 - val_loss: 1.1458\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9772 - val_loss: 0.8778\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7734 - val_loss: 0.7528\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6779 - val_loss: 0.6945\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6314 - val_loss: 0.6641\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6061 - val_loss: 0.6467\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5908 - val_loss: 0.6358\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5805 - val_loss: 0.6277\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5725 - val_loss: 0.6212\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5656 - val_loss: 0.6159\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5597 - val_loss: 0.6108\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5546 - val_loss: 0.6061\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5499 - val_loss: 0.6018\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5983\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5418 - val_loss: 0.5947\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5381 - val_loss: 0.5915\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5349 - val_loss: 0.5885\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5320 - val_loss: 0.5856\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 0.5829\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5267 - val_loss: 0.5808\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 0.5789\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5225 - val_loss: 0.5766\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5208 - val_loss: 0.5749\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5191 - val_loss: 0.5736\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5177 - val_loss: 0.5722\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5163 - val_loss: 0.5707\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5150 - val_loss: 0.5695\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5139 - val_loss: 0.5681\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.5666\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5118 - val_loss: 0.5658\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5109 - val_loss: 0.5648\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5100 - val_loss: 0.5641\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5093 - val_loss: 0.5632\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5085 - val_loss: 0.5621\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5078 - val_loss: 0.5611\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5071 - val_loss: 0.5601\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5065 - val_loss: 0.5596\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5059 - val_loss: 0.5590\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5053 - val_loss: 0.5586\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5047 - val_loss: 0.5581\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5042 - val_loss: 0.5577\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5038 - val_loss: 0.5569\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5033 - val_loss: 0.5563\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5029 - val_loss: 0.5558\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5024 - val_loss: 0.5552\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5020 - val_loss: 0.5549\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5016 - val_loss: 0.5542\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5013 - val_loss: 0.5536\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5009 - val_loss: 0.5531\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5006 - val_loss: 0.5526\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5002 - val_loss: 0.5522\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4999 - val_loss: 0.5517\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4996 - val_loss: 0.5517\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4993 - val_loss: 0.5513\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4990 - val_loss: 0.5511\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4987 - val_loss: 0.5510\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4985 - val_loss: 0.5504\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4982 - val_loss: 0.5499\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4979 - val_loss: 0.5499\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4977 - val_loss: 0.5492\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4975 - val_loss: 0.5487\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4972 - val_loss: 0.5485\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4969 - val_loss: 0.5479\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4967 - val_loss: 0.5474\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4966 - val_loss: 0.5472\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4963 - val_loss: 0.5467\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4961 - val_loss: 0.5466\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4959 - val_loss: 0.5464\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4957 - val_loss: 0.5461\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4955 - val_loss: 0.5460\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4953 - val_loss: 0.5455\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4951 - val_loss: 0.5453\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4950 - val_loss: 0.5451\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4948 - val_loss: 0.5451\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4945 - val_loss: 0.5447\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4944 - val_loss: 0.5444\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4942 - val_loss: 0.5442\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4941 - val_loss: 0.5439\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4940 - val_loss: 0.5438\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4937 - val_loss: 0.5437\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4935 - val_loss: 0.5432\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4934 - val_loss: 0.5431\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4933 - val_loss: 0.5430\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4931 - val_loss: 0.5430\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4929 - val_loss: 0.5426\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4928 - val_loss: 0.5424\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4926 - val_loss: 0.5424\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4925 - val_loss: 0.5419\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4923 - val_loss: 0.5417\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4923 - val_loss: 0.5414\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4921 - val_loss: 0.5413\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4920 - val_loss: 0.5410\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4918 - val_loss: 0.5408\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4917 - val_loss: 0.5409\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4916 - val_loss: 0.5405\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4914 - val_loss: 0.5404\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4913 - val_loss: 0.5403\n",
      "121/121 [==============================] - 0s 784us/step - loss: 0.4816\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 4.0235 - val_loss: 3.4115\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8554 - val_loss: 2.4088\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9966 - val_loss: 1.7093\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3974 - val_loss: 1.2377\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0222 - val_loss: 0.9613\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8000 - val_loss: 0.7953\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6746 - val_loss: 0.7079\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6126 - val_loss: 0.6670\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5831 - val_loss: 0.6473\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5675 - val_loss: 0.6358\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5571 - val_loss: 0.6275\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5490 - val_loss: 0.6206\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5423 - val_loss: 0.6147\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5361 - val_loss: 0.6094\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 0.6047\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5256 - val_loss: 0.6001\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5209 - val_loss: 0.5963\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5167 - val_loss: 0.5923\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.5888\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5093 - val_loss: 0.5854\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5061 - val_loss: 0.5824\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5034 - val_loss: 0.5798\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5008 - val_loss: 0.5775\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4985 - val_loss: 0.5754\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4964 - val_loss: 0.5733\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4945 - val_loss: 0.5714\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4927 - val_loss: 0.5699\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4911 - val_loss: 0.5685\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4896 - val_loss: 0.5671\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4882 - val_loss: 0.5656\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4870 - val_loss: 0.5644\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4858 - val_loss: 0.5634\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4847 - val_loss: 0.5624\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4837 - val_loss: 0.5613\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4828 - val_loss: 0.5602\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4819 - val_loss: 0.5594\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4811 - val_loss: 0.5588\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4804 - val_loss: 0.5580\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4797 - val_loss: 0.5573\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4790 - val_loss: 0.5567\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4783 - val_loss: 0.5557\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4778 - val_loss: 0.5550\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4772 - val_loss: 0.5546\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4767 - val_loss: 0.5540\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4762 - val_loss: 0.5532\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4757 - val_loss: 0.5528\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4752 - val_loss: 0.5525\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4749 - val_loss: 0.5520\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4744 - val_loss: 0.5515\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4740 - val_loss: 0.5510\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4737 - val_loss: 0.5505\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4733 - val_loss: 0.5506\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4730 - val_loss: 0.5502\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4727 - val_loss: 0.5499\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4724 - val_loss: 0.5496\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4721 - val_loss: 0.5491\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4718 - val_loss: 0.5486\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4715 - val_loss: 0.5483\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4712 - val_loss: 0.5480\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4709 - val_loss: 0.5476\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4707 - val_loss: 0.5474\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4705 - val_loss: 0.5472\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4702 - val_loss: 0.5468\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4699 - val_loss: 0.5467\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4697 - val_loss: 0.5462\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4696 - val_loss: 0.5460\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4693 - val_loss: 0.5458\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4691 - val_loss: 0.5456\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4689 - val_loss: 0.5451\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4687 - val_loss: 0.5449\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4685 - val_loss: 0.5447\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4683 - val_loss: 0.5446\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4681 - val_loss: 0.5442\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4680 - val_loss: 0.5439\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4678 - val_loss: 0.5437\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4676 - val_loss: 0.5436\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4674 - val_loss: 0.5433\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4673 - val_loss: 0.5430\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4671 - val_loss: 0.5429\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4669 - val_loss: 0.5427\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4668 - val_loss: 0.5424\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4666 - val_loss: 0.5424\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4665 - val_loss: 0.5421\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4663 - val_loss: 0.5422\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4662 - val_loss: 0.5418\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4660 - val_loss: 0.5416\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4659 - val_loss: 0.5412\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4657 - val_loss: 0.5412\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4656 - val_loss: 0.5410\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4654 - val_loss: 0.5409\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4653 - val_loss: 0.5407\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4651 - val_loss: 0.5405\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4650 - val_loss: 0.5404\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4648 - val_loss: 0.5400\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4647 - val_loss: 0.5401\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4646 - val_loss: 0.5400\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4644 - val_loss: 0.5399\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4643 - val_loss: 0.5394\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4642 - val_loss: 0.5394\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4640 - val_loss: 0.5389\n",
      "121/121 [==============================] - 0s 809us/step - loss: 0.5319\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 4.2987 - val_loss: 3.6079\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2959 - val_loss: 2.7961\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5101 - val_loss: 2.0666\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7625 - val_loss: 1.4227\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2031 - val_loss: 1.0040\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8791 - val_loss: 0.7891\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7260 - val_loss: 0.6947\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6588 - val_loss: 0.6538\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6273 - val_loss: 0.6341\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6101 - val_loss: 0.6227\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5988 - val_loss: 0.6148\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5901 - val_loss: 0.6083\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5829 - val_loss: 0.6028\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5765 - val_loss: 0.5979\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5709 - val_loss: 0.5933\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5657 - val_loss: 0.5889\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5605 - val_loss: 0.5850\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5557 - val_loss: 0.5814\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5514 - val_loss: 0.5774\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5479 - val_loss: 0.5746\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5449 - val_loss: 0.5720\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5422 - val_loss: 0.5697\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5398 - val_loss: 0.5676\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5376 - val_loss: 0.5657\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5356 - val_loss: 0.5636\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5337 - val_loss: 0.5619\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5320 - val_loss: 0.5604\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.5592\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5290 - val_loss: 0.5578\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5276 - val_loss: 0.5568\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.5557\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 0.5546\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 0.5536\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 0.5525\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5222 - val_loss: 0.5518\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5214 - val_loss: 0.5510\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5505\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5198 - val_loss: 0.5497\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5190 - val_loss: 0.5490\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.5482\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5177 - val_loss: 0.5478\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5171 - val_loss: 0.5472\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5166 - val_loss: 0.5465\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5160 - val_loss: 0.5458\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5155 - val_loss: 0.5451\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5150 - val_loss: 0.5448\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.5445\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5141 - val_loss: 0.5440\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5136 - val_loss: 0.5435\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5132 - val_loss: 0.5429\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.5426\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5125 - val_loss: 0.5421\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5121 - val_loss: 0.5419\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5117 - val_loss: 0.5414\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5115 - val_loss: 0.5413\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5112 - val_loss: 0.5412\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5108 - val_loss: 0.5410\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5105 - val_loss: 0.5405\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5103 - val_loss: 0.5401\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5100 - val_loss: 0.5398\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5097 - val_loss: 0.5394\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5094 - val_loss: 0.5392\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5091 - val_loss: 0.5392\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5089 - val_loss: 0.5389\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5086 - val_loss: 0.5387\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5085 - val_loss: 0.5384\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5082 - val_loss: 0.5381\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5080 - val_loss: 0.5380\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5077 - val_loss: 0.5377\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5075 - val_loss: 0.5375\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5073 - val_loss: 0.5371\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5070 - val_loss: 0.5369\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5068 - val_loss: 0.5368\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5066 - val_loss: 0.5365\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5064 - val_loss: 0.5363\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5062 - val_loss: 0.5362\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5060 - val_loss: 0.5358\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5058 - val_loss: 0.5355\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5056 - val_loss: 0.5353\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5054 - val_loss: 0.5351\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5052 - val_loss: 0.5350\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5050 - val_loss: 0.5347\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5049 - val_loss: 0.5345\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5047 - val_loss: 0.5343\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5045 - val_loss: 0.5341\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5043 - val_loss: 0.5340\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5041 - val_loss: 0.5338\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5040 - val_loss: 0.5336\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5038 - val_loss: 0.5334\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5037 - val_loss: 0.5332\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5035 - val_loss: 0.5330\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5033 - val_loss: 0.5328\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5032 - val_loss: 0.5326\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5029 - val_loss: 0.5329\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5028 - val_loss: 0.5328\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5027 - val_loss: 0.5322\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5026 - val_loss: 0.5322\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5024 - val_loss: 0.5321\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5022 - val_loss: 0.5321\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5021 - val_loss: 0.5319\n",
      "121/121 [==============================] - 0s 795us/step - loss: 0.4528\n",
      "Epoch 1/100\n",
      " 64/242 [======>.......................] - ETA: 0s - loss: 6.4391 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.5465 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 804us/step - loss: 6.2043\n",
      "Epoch 1/100\n",
      " 66/242 [=======>......................] - ETA: 0s - loss: 6.7101 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.3379 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 781us/step - loss: 6.6216\n",
      "Epoch 1/100\n",
      " 66/242 [=======>......................] - ETA: 0s - loss: 6.5237 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6.4129 - val_loss: 6.3409\n",
      "121/121 [==============================] - 0s 778us/step - loss: 6.4715\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 4.9974 - val_loss: 4.8073\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9585 - val_loss: 4.7694\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9191 - val_loss: 4.7313\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8795 - val_loss: 4.6929\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8397 - val_loss: 4.6543\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7998 - val_loss: 4.6156\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7598 - val_loss: 4.5768\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7197 - val_loss: 4.5380\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6794 - val_loss: 4.4990\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6391 - val_loss: 4.4601\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5988 - val_loss: 4.4212\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5585 - val_loss: 4.3823\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5182 - val_loss: 4.3435\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4781 - val_loss: 4.3049\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4380 - val_loss: 4.2663\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3981 - val_loss: 4.2279\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3584 - val_loss: 4.1897\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3188 - val_loss: 4.1517\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2794 - val_loss: 4.1138\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2402 - val_loss: 4.0762\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2013 - val_loss: 4.0388\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1626 - val_loss: 4.0017\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1241 - val_loss: 3.9647\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0859 - val_loss: 3.9280\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0478 - val_loss: 3.8915\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0100 - val_loss: 3.8553\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9725 - val_loss: 3.8194\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9353 - val_loss: 3.7837\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8983 - val_loss: 3.7483\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8617 - val_loss: 3.7132\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8252 - val_loss: 3.6784\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7891 - val_loss: 3.6439\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7533 - val_loss: 3.6097\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7178 - val_loss: 3.5757\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6826 - val_loss: 3.5421\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6476 - val_loss: 3.5087\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6130 - val_loss: 3.4756\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5786 - val_loss: 3.4428\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5446 - val_loss: 3.4103\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.5107 - val_loss: 3.3780\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4772 - val_loss: 3.3460\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4439 - val_loss: 3.3143\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4109 - val_loss: 3.2828\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3782 - val_loss: 3.2516\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3457 - val_loss: 3.2206\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3135 - val_loss: 3.1899\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2815 - val_loss: 3.1593\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2498 - val_loss: 3.1291\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2183 - val_loss: 3.0991\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1870 - val_loss: 3.0693\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1560 - val_loss: 3.0397\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1252 - val_loss: 3.0103\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0945 - val_loss: 2.9810\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0640 - val_loss: 2.9520\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0337 - val_loss: 2.9230\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0035 - val_loss: 2.8943\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9734 - val_loss: 2.8657\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9434 - val_loss: 2.8371\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9135 - val_loss: 2.8087\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8837 - val_loss: 2.7804\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8539 - val_loss: 2.7521\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8243 - val_loss: 2.7239\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7947 - val_loss: 2.6958\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7652 - val_loss: 2.6677\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7357 - val_loss: 2.6398\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7062 - val_loss: 2.6119\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6767 - val_loss: 2.5840\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6472 - val_loss: 2.5562\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6178 - val_loss: 2.5285\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5883 - val_loss: 2.5008\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.5590 - val_loss: 2.4732\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5298 - val_loss: 2.4458\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5008 - val_loss: 2.4186\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4719 - val_loss: 2.3916\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4432 - val_loss: 2.3648\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4147 - val_loss: 2.3382\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3865 - val_loss: 2.3118\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3584 - val_loss: 2.2857\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3306 - val_loss: 2.2598\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3032 - val_loss: 2.2343\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2760 - val_loss: 2.2091\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2492 - val_loss: 2.1842\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2228 - val_loss: 2.1596\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1966 - val_loss: 2.1353\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1709 - val_loss: 2.1114\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1455 - val_loss: 2.0879\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1205 - val_loss: 2.0647\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0959 - val_loss: 2.0419\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0717 - val_loss: 2.0194\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.0478 - val_loss: 1.9973\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.0244 - val_loss: 1.9756\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.0013 - val_loss: 1.9542\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.9786 - val_loss: 1.9332\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.9563 - val_loss: 1.9125\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9342 - val_loss: 1.8921\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.9126 - val_loss: 1.8721\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8913 - val_loss: 1.8524\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.8702 - val_loss: 1.8329\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8496 - val_loss: 1.8138\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.8292 - val_loss: 1.7949\n",
      "121/121 [==============================] - 0s 982us/step - loss: 1.7635\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 4.6312 - val_loss: 4.6223\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5914 - val_loss: 4.5831\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5516 - val_loss: 4.5439\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5119 - val_loss: 4.5049\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.4724 - val_loss: 4.4659\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4331 - val_loss: 4.4271\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3939 - val_loss: 4.3886\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3551 - val_loss: 4.3503\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3166 - val_loss: 4.3123\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.2783 - val_loss: 4.2746\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2403 - val_loss: 4.2371\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2026 - val_loss: 4.2000\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1653 - val_loss: 4.1632\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1282 - val_loss: 4.1266\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.0914 - val_loss: 4.0904\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.0550 - val_loss: 4.0545\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0189 - val_loss: 4.0189\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9831 - val_loss: 3.9836\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9476 - val_loss: 3.9487\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9125 - val_loss: 3.9142\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8777 - val_loss: 3.8799\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8433 - val_loss: 3.8461\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8092 - val_loss: 3.8126\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7756 - val_loss: 3.7794\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7422 - val_loss: 3.7466\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7092 - val_loss: 3.7140\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.6766 - val_loss: 3.6818\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6442 - val_loss: 3.6499\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.6121 - val_loss: 3.6183\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.5804 - val_loss: 3.5869\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.5489 - val_loss: 3.5559\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5178 - val_loss: 3.5252\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4869 - val_loss: 3.4948\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.4563 - val_loss: 3.4646\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4259 - val_loss: 3.4347\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.3958 - val_loss: 3.4051\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3659 - val_loss: 3.3757\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.3363 - val_loss: 3.3466\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3069 - val_loss: 3.3178\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.2778 - val_loss: 3.2892\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.2489 - val_loss: 3.2608\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.2203 - val_loss: 3.2326\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.1918 - val_loss: 3.2047\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.1636 - val_loss: 3.1769\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1356 - val_loss: 3.1493\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1078 - val_loss: 3.1219\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.0802 - val_loss: 3.0948\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0528 - val_loss: 3.0677\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0255 - val_loss: 3.0408\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9983 - val_loss: 3.0141\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9713 - val_loss: 2.9875\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9445 - val_loss: 2.9611\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9178 - val_loss: 2.9348\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8912 - val_loss: 2.9086\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8647 - val_loss: 2.8825\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8384 - val_loss: 2.8566\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.8121 - val_loss: 2.8308\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7861 - val_loss: 2.8052\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7602 - val_loss: 2.7797\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7344 - val_loss: 2.7543\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7088 - val_loss: 2.7291\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6833 - val_loss: 2.7040\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6580 - val_loss: 2.6791\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6327 - val_loss: 2.6544\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6077 - val_loss: 2.6297\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5827 - val_loss: 2.6052\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.5578 - val_loss: 2.5809\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.5330 - val_loss: 2.5566\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.5084 - val_loss: 2.5325\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.4838 - val_loss: 2.5085\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.4594 - val_loss: 2.4846\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4352 - val_loss: 2.4609\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4110 - val_loss: 2.4373\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3870 - val_loss: 2.4139\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3631 - val_loss: 2.3905\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3392 - val_loss: 2.3673\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3155 - val_loss: 2.3443\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2918 - val_loss: 2.3214\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2683 - val_loss: 2.2986\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2449 - val_loss: 2.2759\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2216 - val_loss: 2.2533\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1984 - val_loss: 2.2309\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1754 - val_loss: 2.2087\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1525 - val_loss: 2.1866\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1298 - val_loss: 2.1647\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1071 - val_loss: 2.1428\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0846 - val_loss: 2.1211\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0623 - val_loss: 2.0995\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0400 - val_loss: 2.0780\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0180 - val_loss: 2.0566\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9960 - val_loss: 2.0353\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9742 - val_loss: 2.0142\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9526 - val_loss: 1.9932\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9312 - val_loss: 1.9724\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.9098 - val_loss: 1.9518\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8887 - val_loss: 1.9313\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8677 - val_loss: 1.9109\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8469 - val_loss: 1.8907\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.8263 - val_loss: 1.8707\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.8058 - val_loss: 1.8508\n",
      "121/121 [==============================] - 0s 772us/step - loss: 1.9640\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 4.9462 - val_loss: 4.7742\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9051 - val_loss: 4.7339\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8637 - val_loss: 4.6934\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.8220 - val_loss: 4.6528\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7802 - val_loss: 4.6121\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7383 - val_loss: 4.5713\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6963 - val_loss: 4.5305\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6544 - val_loss: 4.4898\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6125 - val_loss: 4.4492\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5707 - val_loss: 4.4087\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.5290 - val_loss: 4.3683\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4875 - val_loss: 4.3281\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4462 - val_loss: 4.2882\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.4052 - val_loss: 4.2485\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3643 - val_loss: 4.2089\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3237 - val_loss: 4.1696\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2833 - val_loss: 4.1305\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2432 - val_loss: 4.0917\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2034 - val_loss: 4.0531\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1638 - val_loss: 4.0149\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.1246 - val_loss: 3.9769\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.0857 - val_loss: 3.9392\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0471 - val_loss: 3.9018\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0087 - val_loss: 3.8647\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9706 - val_loss: 3.8278\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9328 - val_loss: 3.7913\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8954 - val_loss: 3.7550\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8581 - val_loss: 3.7190\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8212 - val_loss: 3.6834\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7847 - val_loss: 3.6480\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7484 - val_loss: 3.6129\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7124 - val_loss: 3.5781\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6767 - val_loss: 3.5436\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6413 - val_loss: 3.5094\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6062 - val_loss: 3.4755\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5714 - val_loss: 3.4418\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5369 - val_loss: 3.4084\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5026 - val_loss: 3.3752\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4686 - val_loss: 3.3423\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4349 - val_loss: 3.3097\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4015 - val_loss: 3.2773\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3683 - val_loss: 3.2452\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3353 - val_loss: 3.2132\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3026 - val_loss: 3.1815\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2700 - val_loss: 3.1500\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2377 - val_loss: 3.1187\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2056 - val_loss: 3.0876\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1736 - val_loss: 3.0568\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1419 - val_loss: 3.0261\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1103 - val_loss: 2.9957\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0788 - val_loss: 2.9654\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0475 - val_loss: 2.9352\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0163 - val_loss: 2.9052\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9852 - val_loss: 2.8752\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9541 - val_loss: 2.8453\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.9232 - val_loss: 2.8155\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8922 - val_loss: 2.7857\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8614 - val_loss: 2.7560\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8306 - val_loss: 2.7263\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7999 - val_loss: 2.6968\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7692 - val_loss: 2.6673\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7385 - val_loss: 2.6378\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7079 - val_loss: 2.6085\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6774 - val_loss: 2.5792\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6469 - val_loss: 2.5501\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6165 - val_loss: 2.5210\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5862 - val_loss: 2.4920\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5560 - val_loss: 2.4631\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.5259 - val_loss: 2.4344\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4960 - val_loss: 2.4059\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4663 - val_loss: 2.3774\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4367 - val_loss: 2.3491\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4073 - val_loss: 2.3211\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3781 - val_loss: 2.2932\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3491 - val_loss: 2.2655\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3202 - val_loss: 2.2380\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2915 - val_loss: 2.2107\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2630 - val_loss: 2.1836\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2346 - val_loss: 2.1566\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2064 - val_loss: 2.1298\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1785 - val_loss: 2.1033\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1508 - val_loss: 2.0769\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1233 - val_loss: 2.0508\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0960 - val_loss: 2.0248\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0689 - val_loss: 1.9990\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0420 - val_loss: 1.9734\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0154 - val_loss: 1.9481\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9889 - val_loss: 1.9229\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9627 - val_loss: 1.8980\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9366 - val_loss: 1.8732\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9108 - val_loss: 1.8487\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8852 - val_loss: 1.8245\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8599 - val_loss: 1.8007\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8349 - val_loss: 1.7771\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.8103 - val_loss: 1.7539\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7860 - val_loss: 1.7310\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7621 - val_loss: 1.7085\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7386 - val_loss: 1.6864\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7156 - val_loss: 1.6647\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6929 - val_loss: 1.6433\n",
      "121/121 [==============================] - 0s 801us/step - loss: 1.5954\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 3ms/step - loss: 5.2142 - val_loss: 5.0462\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.2002 - val_loss: 5.0326\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1861 - val_loss: 5.0188\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1719 - val_loss: 5.0049\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1576 - val_loss: 4.9909\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1431 - val_loss: 4.9769\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.1286 - val_loss: 4.9627\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1140 - val_loss: 4.9484\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0992 - val_loss: 4.9341\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0844 - val_loss: 4.9197\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0696 - val_loss: 4.9052\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0546 - val_loss: 4.8906\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0396 - val_loss: 4.8760\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0245 - val_loss: 4.8613\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0094 - val_loss: 4.8466\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9942 - val_loss: 4.8318\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9790 - val_loss: 4.8170\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9637 - val_loss: 4.8022\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9484 - val_loss: 4.7873\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9331 - val_loss: 4.7724\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9177 - val_loss: 4.7575\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9023 - val_loss: 4.7425\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8869 - val_loss: 4.7276\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8714 - val_loss: 4.7126\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8560 - val_loss: 4.6976\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8405 - val_loss: 4.6826\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8251 - val_loss: 4.6676\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8096 - val_loss: 4.6525\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7941 - val_loss: 4.6375\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7786 - val_loss: 4.6225\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7631 - val_loss: 4.6075\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7476 - val_loss: 4.5925\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7322 - val_loss: 4.5775\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7167 - val_loss: 4.5625\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7013 - val_loss: 4.5475\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6858 - val_loss: 4.5325\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6704 - val_loss: 4.5176\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6550 - val_loss: 4.5026\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6396 - val_loss: 4.4877\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6242 - val_loss: 4.4728\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6088 - val_loss: 4.4579\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5935 - val_loss: 4.4431\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5782 - val_loss: 4.4283\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5629 - val_loss: 4.4134\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5476 - val_loss: 4.3987\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5324 - val_loss: 4.3839\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5172 - val_loss: 4.3692\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5020 - val_loss: 4.3544\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4868 - val_loss: 4.3397\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4716 - val_loss: 4.3251\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4565 - val_loss: 4.3104\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4414 - val_loss: 4.2958\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4263 - val_loss: 4.2812\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4112 - val_loss: 4.2666\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3962 - val_loss: 4.2521\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3811 - val_loss: 4.2376\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3662 - val_loss: 4.2231\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3512 - val_loss: 4.2086\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3363 - val_loss: 4.1942\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3214 - val_loss: 4.1798\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3065 - val_loss: 4.1654\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2917 - val_loss: 4.1511\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2769 - val_loss: 4.1367\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2621 - val_loss: 4.1225\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2474 - val_loss: 4.1082\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2326 - val_loss: 4.0940\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2180 - val_loss: 4.0798\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2033 - val_loss: 4.0656\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1887 - val_loss: 4.0515\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1741 - val_loss: 4.0374\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1595 - val_loss: 4.0234\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1450 - val_loss: 4.0093\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1305 - val_loss: 3.9953\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1161 - val_loss: 3.9814\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1017 - val_loss: 3.9675\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0873 - val_loss: 3.9536\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0729 - val_loss: 3.9397\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0585 - val_loss: 3.9258\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0442 - val_loss: 3.9120\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.0299 - val_loss: 3.8983\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0157 - val_loss: 3.8845\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0015 - val_loss: 3.8708\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9873 - val_loss: 3.8572\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9732 - val_loss: 3.8435\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9591 - val_loss: 3.8299\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9450 - val_loss: 3.8164\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9309 - val_loss: 3.8028\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9169 - val_loss: 3.7893\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9029 - val_loss: 3.7759\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8890 - val_loss: 3.7624\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8750 - val_loss: 3.7490\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8611 - val_loss: 3.7356\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8473 - val_loss: 3.7223\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8334 - val_loss: 3.7089\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8196 - val_loss: 3.6956\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8058 - val_loss: 3.6824\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7921 - val_loss: 3.6691\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7783 - val_loss: 3.6559\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7646 - val_loss: 3.6427\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7509 - val_loss: 3.6296\n",
      "121/121 [==============================] - 0s 776us/step - loss: 3.6150\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 5.0239 - val_loss: 5.0427\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0102 - val_loss: 5.0289\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9965 - val_loss: 5.0151\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9825 - val_loss: 5.0012\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9685 - val_loss: 4.9871\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9544 - val_loss: 4.9730\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9402 - val_loss: 4.9587\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9259 - val_loss: 4.9444\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9115 - val_loss: 4.9300\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8971 - val_loss: 4.9156\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8826 - val_loss: 4.9010\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8680 - val_loss: 4.8865\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8534 - val_loss: 4.8718\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8387 - val_loss: 4.8572\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8240 - val_loss: 4.8424\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8092 - val_loss: 4.8277\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7943 - val_loss: 4.8129\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7795 - val_loss: 4.7980\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7646 - val_loss: 4.7831\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7497 - val_loss: 4.7682\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7347 - val_loss: 4.7533\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7198 - val_loss: 4.7384\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7048 - val_loss: 4.7234\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6898 - val_loss: 4.7084\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6748 - val_loss: 4.6934\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6597 - val_loss: 4.6784\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6447 - val_loss: 4.6634\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6297 - val_loss: 4.6484\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6146 - val_loss: 4.6334\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5996 - val_loss: 4.6184\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5846 - val_loss: 4.6034\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5695 - val_loss: 4.5884\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5545 - val_loss: 4.5734\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5395 - val_loss: 4.5584\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5245 - val_loss: 4.5434\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5095 - val_loss: 4.5284\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4945 - val_loss: 4.5135\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4795 - val_loss: 4.4985\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4646 - val_loss: 4.4836\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4496 - val_loss: 4.4687\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4347 - val_loss: 4.4538\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4198 - val_loss: 4.4390\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4049 - val_loss: 4.4241\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3900 - val_loss: 4.4093\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3752 - val_loss: 4.3945\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3604 - val_loss: 4.3797\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3456 - val_loss: 4.3650\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3308 - val_loss: 4.3503\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3161 - val_loss: 4.3356\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3014 - val_loss: 4.3209\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2867 - val_loss: 4.3062\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2720 - val_loss: 4.2916\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2573 - val_loss: 4.2770\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2427 - val_loss: 4.2624\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2281 - val_loss: 4.2479\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2136 - val_loss: 4.2334\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1990 - val_loss: 4.2189\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1845 - val_loss: 4.2044\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1701 - val_loss: 4.1900\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1556 - val_loss: 4.1756\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1412 - val_loss: 4.1612\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1269 - val_loss: 4.1469\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1125 - val_loss: 4.1326\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0982 - val_loss: 4.1183\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0839 - val_loss: 4.1041\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.0697 - val_loss: 4.0899\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0554 - val_loss: 4.0757\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0413 - val_loss: 4.0616\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0271 - val_loss: 4.0475\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0130 - val_loss: 4.0334\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9989 - val_loss: 4.0193\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9848 - val_loss: 4.0053\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9708 - val_loss: 3.9913\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9568 - val_loss: 3.9774\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9429 - val_loss: 3.9634\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9290 - val_loss: 3.9496\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9151 - val_loss: 3.9357\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9012 - val_loss: 3.9219\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8873 - val_loss: 3.9081\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8735 - val_loss: 3.8943\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8598 - val_loss: 3.8806\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8460 - val_loss: 3.8669\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8323 - val_loss: 3.8532\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8186 - val_loss: 3.8396\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8050 - val_loss: 3.8260\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7913 - val_loss: 3.8124\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7777 - val_loss: 3.7988\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7642 - val_loss: 3.7853\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7506 - val_loss: 3.7718\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.7371 - val_loss: 3.7584\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7236 - val_loss: 3.7449\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7102 - val_loss: 3.7315\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6967 - val_loss: 3.7182\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6833 - val_loss: 3.7048\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6699 - val_loss: 3.6915\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6566 - val_loss: 3.6782\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6433 - val_loss: 3.6649\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6299 - val_loss: 3.6517\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6167 - val_loss: 3.6385\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6034 - val_loss: 3.6253\n",
      "121/121 [==============================] - 0s 804us/step - loss: 3.9064\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiehengyu/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 5.2090 - val_loss: 5.0275\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1944 - val_loss: 5.0133\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1796 - val_loss: 4.9989\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1648 - val_loss: 4.9845\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1498 - val_loss: 4.9699\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1347 - val_loss: 4.9552\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1195 - val_loss: 4.9405\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.1043 - val_loss: 4.9256\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0889 - val_loss: 4.9107\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0734 - val_loss: 4.8957\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0579 - val_loss: 4.8806\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0423 - val_loss: 4.8654\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0266 - val_loss: 4.8502\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.0108 - val_loss: 4.8349\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9950 - val_loss: 4.8196\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9792 - val_loss: 4.8042\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9633 - val_loss: 4.7888\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9474 - val_loss: 4.7733\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9315 - val_loss: 4.7578\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.9155 - val_loss: 4.7423\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8995 - val_loss: 4.7268\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8834 - val_loss: 4.7112\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8674 - val_loss: 4.6956\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8513 - val_loss: 4.6800\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8352 - val_loss: 4.6644\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8191 - val_loss: 4.6487\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8030 - val_loss: 4.6331\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7868 - val_loss: 4.6174\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7707 - val_loss: 4.6017\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7545 - val_loss: 4.5861\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7384 - val_loss: 4.5704\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7222 - val_loss: 4.5548\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.7061 - val_loss: 4.5391\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6899 - val_loss: 4.5235\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6738 - val_loss: 4.5078\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6577 - val_loss: 4.4922\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6416 - val_loss: 4.4766\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6255 - val_loss: 4.4610\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6094 - val_loss: 4.4455\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5933 - val_loss: 4.4299\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5773 - val_loss: 4.4144\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5613 - val_loss: 4.3989\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5453 - val_loss: 4.3834\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5294 - val_loss: 4.3680\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.5134 - val_loss: 4.3526\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4976 - val_loss: 4.3372\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4817 - val_loss: 4.3218\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4658 - val_loss: 4.3065\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4500 - val_loss: 4.2912\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4342 - val_loss: 4.2759\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4185 - val_loss: 4.2607\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.4028 - val_loss: 4.2455\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3871 - val_loss: 4.2303\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3714 - val_loss: 4.2152\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3558 - val_loss: 4.2001\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3402 - val_loss: 4.1850\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3247 - val_loss: 4.1700\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.3092 - val_loss: 4.1550\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2937 - val_loss: 4.1400\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.2782 - val_loss: 4.1251\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2628 - val_loss: 4.1102\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2474 - val_loss: 4.0953\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2321 - val_loss: 4.0805\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2168 - val_loss: 4.0657\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2015 - val_loss: 4.0510\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1863 - val_loss: 4.0363\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1711 - val_loss: 4.0216\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1560 - val_loss: 4.0070\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1409 - val_loss: 3.9924\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1258 - val_loss: 3.9778\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1107 - val_loss: 3.9633\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0957 - val_loss: 3.9488\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0808 - val_loss: 3.9343\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0659 - val_loss: 3.9199\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0510 - val_loss: 3.9055\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0361 - val_loss: 3.8912\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0213 - val_loss: 3.8769\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0065 - val_loss: 3.8626\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9918 - val_loss: 3.8484\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9770 - val_loss: 3.8341\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9623 - val_loss: 3.8200\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9477 - val_loss: 3.8058\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9330 - val_loss: 3.7917\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9185 - val_loss: 3.7777\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.9039 - val_loss: 3.7636\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.8894 - val_loss: 3.7496\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.8749 - val_loss: 3.7357\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8605 - val_loss: 3.7217\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8461 - val_loss: 3.7079\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8317 - val_loss: 3.6940\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8174 - val_loss: 3.6802\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8031 - val_loss: 3.6664\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7888 - val_loss: 3.6527\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7746 - val_loss: 3.6390\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7604 - val_loss: 3.6253\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7463 - val_loss: 3.6117\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7321 - val_loss: 3.5981\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7181 - val_loss: 3.5845\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.7040 - val_loss: 3.5710\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.6900 - val_loss: 3.5575\n",
      "121/121 [==============================] - 0s 785us/step - loss: 3.5151\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 4.0185 - val_loss: 3.0584\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.4119 - val_loss: 1.8174\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.4390 - val_loss: 1.1567\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.9744 - val_loss: 0.8921\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7681 - val_loss: 0.7385\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6514 - val_loss: 0.6689\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6069 - val_loss: 0.6449\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5883 - val_loss: 0.6322\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5761 - val_loss: 0.6229\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5662 - val_loss: 0.6146\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5576 - val_loss: 0.6072\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 0.6005\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5434 - val_loss: 0.5944\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5377 - val_loss: 0.5895\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5327 - val_loss: 0.5847\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5285 - val_loss: 0.5808\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5247 - val_loss: 0.5777\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5214 - val_loss: 0.5745\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.5718\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5161 - val_loss: 0.5692\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5139 - val_loss: 0.5670\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5119 - val_loss: 0.5645\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5101 - val_loss: 0.5635\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5084 - val_loss: 0.5617\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5070 - val_loss: 0.5602\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5057 - val_loss: 0.5588\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5045 - val_loss: 0.5569\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5034 - val_loss: 0.5562\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5024 - val_loss: 0.5556\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5015 - val_loss: 0.5544\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5006 - val_loss: 0.5529\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4998 - val_loss: 0.5521\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4991 - val_loss: 0.5517\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4984 - val_loss: 0.5510\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4977 - val_loss: 0.5501\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4972 - val_loss: 0.5498\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4966 - val_loss: 0.5489\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4960 - val_loss: 0.5478\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4956 - val_loss: 0.5472\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4950 - val_loss: 0.5465\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4945 - val_loss: 0.5462\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4942 - val_loss: 0.5456\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4937 - val_loss: 0.5449\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4933 - val_loss: 0.5445\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4929 - val_loss: 0.5442\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4925 - val_loss: 0.5436\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4921 - val_loss: 0.5435\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4918 - val_loss: 0.5427\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4914 - val_loss: 0.5422\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4911 - val_loss: 0.5419\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4908 - val_loss: 0.5412\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4905 - val_loss: 0.5409\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4902 - val_loss: 0.5408\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4899 - val_loss: 0.5408\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4896 - val_loss: 0.5401\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4893 - val_loss: 0.5395\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4890 - val_loss: 0.5390\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4889 - val_loss: 0.5389\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4885 - val_loss: 0.5392\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4883 - val_loss: 0.5388\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4881 - val_loss: 0.5385\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4878 - val_loss: 0.5381\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4876 - val_loss: 0.5376\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4873 - val_loss: 0.5373\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4871 - val_loss: 0.5370\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4869 - val_loss: 0.5366\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4866 - val_loss: 0.5362\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4865 - val_loss: 0.5363\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4863 - val_loss: 0.5362\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4861 - val_loss: 0.5361\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4858 - val_loss: 0.5360\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4856 - val_loss: 0.5359\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4854 - val_loss: 0.5354\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4852 - val_loss: 0.5349\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4851 - val_loss: 0.5344\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4849 - val_loss: 0.5346\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4847 - val_loss: 0.5345\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4845 - val_loss: 0.5343\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4843 - val_loss: 0.5341\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4840 - val_loss: 0.5333\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4839 - val_loss: 0.5332\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4837 - val_loss: 0.5333\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4835 - val_loss: 0.5331\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4834 - val_loss: 0.5328\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4832 - val_loss: 0.5326\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4830 - val_loss: 0.5329\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4828 - val_loss: 0.5324\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4826 - val_loss: 0.5323\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4825 - val_loss: 0.5322\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4823 - val_loss: 0.5320\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4821 - val_loss: 0.5318\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4819 - val_loss: 0.5319\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4818 - val_loss: 0.5319\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4816 - val_loss: 0.5317\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4814 - val_loss: 0.5315\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4813 - val_loss: 0.5311\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4811 - val_loss: 0.5310\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4810 - val_loss: 0.5302\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4807 - val_loss: 0.5299\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4807 - val_loss: 0.5298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7fbe68a311c0>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fbe87bcfdc0>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\"n_hidden\":[0, 1, 2, 3],\n",
    "                  \"n_neurons\": np.arange(1, 100),\n",
    "                  \"learning_rate\": reciprocal(3e-4, 3e-2)}\n",
    "rand_search = RandomizedSearchCV(keras_reg, param_distribs, n_iter = 10, cv = 3)\n",
    "rand_search.fit(X_train, y_train, epochs = 100, \n",
    "                validation_data = (X_val, y_val),\n",
    "                callbacks = [keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae0af1-637e-42be-b7ee-7868068b177e",
   "metadata": {},
   "source": [
    "This is identical to what we have done before in previous lessons, but here we pass parameters to the `fit()` method, & they get relayed to the underlying Keras models. Note that `RandomizedSearchCV` uses K-fold cross-validation, so it does not use `X_val` & `y_val`, which are only used for early stopping.\n",
    "\n",
    "The exploration may last many hours, depending on the hardware, the size of the dataset, the complexity of the model, & the values of `n_iter` & `cv`. When it's over, you can access the best parameters found, the best score, & the trained Keras model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62e05ae3-8c53-4e7a-9f76-b94545529b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.017335062093214396, 'n_hidden': 1, 'n_neurons': 17}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7031ed02-40d0-4c3b-bb95-1d4af4546b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4888018071651459"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12da5531-6f21-4683-9758-17b4acf07f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rand_search.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7117835-5f20-44a8-aa5f-33ca74747e3e",
   "metadata": {},
   "source": [
    "You can now save this model, evaluate it on the test set, & if you are satisfied with its performance, deploy it to production. Using randomised search is not too hard, & it works well for many fairly simple problems. When training is slow, however (e.g., for more complex problems with larger datasets), this approach will only explore a tiny portion of the hyperparameter space. You can partially alleviate this problem by assisting the search process manually: first run a quick random search using wide ranges of hyperparameter values, then run another search using smaller ranages of values centered on the best ones found during the first run, & so on. This approach will hopefully zoom in on a good set of hyperparameters. However, it's very time consuming, & probably not the best use of your time.\n",
    "\n",
    "Fortunately, there are many techniques to explore a search space much more efficiently than randomly. Their core idea is simple: when a region of the space turns out to be good, it should be explored more. Such techniques take care of the \"zooming\" process for you & lead to much better solutions in much less time. Here are some python libraries you can use to optimise hyperparameters.\n",
    "\n",
    "* *Hyperopt*\n",
    "   * A popular library for optimising over all sorts of complex search spaces (including real values, such as the learning rate, & discrete values, such as the number of layers).\n",
    "* *Hypera*, *kopt*, or *Talos*\n",
    "   * Useful libraries for optimising hyperparameters for Keras models (the first two are based on Hyperopt).\n",
    "* *Keras Tuner*\n",
    "   * An easy-to-use hyperparameter optimisation library by Google for Keras models, with a hosted service for visualisation & analysis.\n",
    "* *Scikit-Optimise* (skopt)\n",
    "   * A general-purpose optimisation library. The `BayesSearchCV` class performs Bayesian optimisation using an interface similar to `GridSearchCV`.\n",
    "* *Spearmint*\n",
    "   * A Bayesian optimisation library.\n",
    "* *Hyperband*\n",
    "   * A fast hyperparameter tuning library based on the recent Hyperband paper by Lisha Li.\n",
    "* *Sklearn-Deap*\n",
    "   * A hyperparameter optimisation library based on evolutionary algorithms, with a `GridSearchCV`-like interface.\n",
    "   \n",
    "Moreover, many companies offer services for hyperparameter optimisations, such as Google cloud AI platform's hyperparameter tuning service. Other options include services by Arimo, SigOpt, & CallDesk's Oscar.\n",
    "\n",
    "Hyperparameter tuning is still an active area of research, & evolutionary algorithms are making a comeback. For example, check out DeepMind's excellent 2017 paper, where the authors jointly optimise a population of models & their hyperparameters. Google has also use an evolutionary approach, not just to search for hyperparameters but also to look for the best neural network architecture for the problem; their AutoML suite is already available as a cloud service. Perhaps the days of building neural networks will soon be over. Check out Google's post on this topic. In fact, evolutionary algorithms have been used successfully to train individual neural networks, replacing the ubiquitous gradient descent. For an example, see the 2017 post by Uber, where the authors introduce their *deep neuroevolution* technique.\n",
    "\n",
    "But despite all this exciting progress & all these tools & services, it still helps to have an idea of what values are reasonable for each hyperparameter so that you can build a quick prototype & restrict the search space. The following sections provide guidelines for choosing the number of hidden layers & neurons in an MLP & for selecting good values for some of the main hyperparameters.\n",
    "\n",
    "## Number of Hidden Layers\n",
    "\n",
    "For many problems, you can begin with a single hidden layer & get reasonable results. A MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons. But for complex problems, deep networks have a much higher *parameter efficiency* than shallow ones: they can model complex function using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data. \n",
    "\n",
    "To understand why, suppose you are asked to draw a forest using some drawing software, but you are forbidden to copy & paste anything. It would take an enormous amount of time: you would have to draw each tree individually, branch by branch, leaf by leaf. If you could instead draw one leaf, copy & paste it to draw a branch, then copy & paste that branch to create a tree, & finally copy & paste this tree to make a forest, you would be finished in no time. Real-world data is often structured in such a hierarchical way, & deep neural networks automatically take advantage of this fact: lower hidden layers model low-level structures (e.g., line segments of various shapes & orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squared, circles), & the highest hidden layers & the output layer combine these intermediate structures to model high-level structures (e.g., faces).\n",
    "\n",
    "Not only does this hierarchical architecture help DNNs converge faster to a good solution, but it also improves their ability to generalise to new datasets. For example, if you have already trained a model to recognise faces in pictures & you now want to train a new neural network to recognise hairstyles, you can kickstart the training by reusing the lower layers of the first network. Instead of randomly initialising the weights & biases of the first few layers of the new neural network, you can initialise them to the values of the weights & biases of the lower layers of the first network. This way, the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures (e.g., hairstyles). This is called *transfer learning*.\n",
    "\n",
    "In summary, for many problems you can start with just one or two hidden layers & the neural network will work just fine. For instance, you can easily reach above 97% accuracy on the MNIST dataset using just one hidden layer with a few hundred neurons, in roughly the same amount of training time. For more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds, but not fully connected ones), & they need a huge amount of training data. You will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will be a lot faster & require much less data.\n",
    "\n",
    "## Number of Neurons per Hidden Layer\n",
    "\n",
    "The number of neurons in the input & output layers is determined by the type of input & output your task requires. For example, the MNIST task requires 28 x 28 = 784 input neurons & 10 output neurons.\n",
    "\n",
    "As for the hidden layers, it used to be common to size them to form a pyramid, with fewer & fewer neurons at each layer -- the rationale being that many low-level features can coalesce into far fewer high-level features. A typical neural network for MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200, & the third with 100. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperprameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\n",
    "\n",
    "Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. But in practice, it's often simpler & more efficient to pick a model with more layers & neurons than you actually need, then use early stopping & other regularisation techniques to prevent it from overfitting. Vincent Vanhoucke, a scientist at Google, has dubbed this the \"stretch pants\" approach: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size. With this approach, you avoid bottleneck layers that could ruin your model. On the flip size, if a layer has too few neurons, it will not have enough representational power to preserve all the useful information from the inputs (e.g., a layer with two neurons can only output 2D data, so if it processes 3D data, some information will be lost). No matter how big & powerful the rest of the network is, that information will never be recovered.\n",
    "\n",
    "## Learning Rate, Batch Size, & Other Hyperparameters\n",
    "\n",
    "The numbers of hidden layers & neurons are not the only hyperparameters you can tweak in a MLP. Here are some of the most important ones, as well as tips on how to set them:\n",
    "\n",
    "* *Learning rate*\n",
    "   * The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., $10^{-5}$) & gradually increasing it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration (e.g., by $e^{\\frac{log(10^6)}{500}}$ to go from $10^{-5}$ to 10 in 500 iterations). If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point). You can the reinitialise your model & train it normally using this good learning rate.\n",
    "* *Optimizer*\n",
    "   * Choosing a better optimiser than plain old mini-batch gradient descent (& tuning its hyperparameters) is also quite important.\n",
    "* *Batch size*\n",
    "   * The batch size can have a significant impact on your model's performance & training time. The main benefit of using large batch sizes is that hardware accelerators like GPUs can process them efficiently, so the training algorithm will see more instances per second. Therefore, many researchers & practitioners recommend using the largest batch size that can fit in GPU RAM. There's a catch, though: in practice, large batch sizes often lead to training instabilities, especially at the beginning of training, & the resulting model may not generalise as well as model trained with a small batch size. In April 2018, Yann LeCun even tweeted, \"Friends don't let friends use mini-batches larger than 32,\" citing a 2018 paper by Dominic Masters & Carlo Luschi which concluded that using small batches (from 2 to 32) was preferable because small batches led to better models in less training time. Other papers point in the opposite direction, however; in 2017, papers by Elad Hoffer & Priya Goyal showed that it was possible to use very large batch sizes (up to 8,192) using various techniques such as warming up the learning rate (i.e., starting training with a small learning rate, then ramping it up). This led to a very short training time, without any generalisation gap. So, one strategy is to try to use a large batch size, using learning rate warmup, & if training is unstable or the final performance is disappointing, then try using a small batch size instead.\n",
    "* *Activation function*\n",
    "   * We discussed how to choose the activation function earlier in this chapter: in general, the ReLU activation function will be a good default for all hidden layers. For the output layer, it really depends on your task.\n",
    "* *Number of iterations*\n",
    "   * In most cases, the number of training iterations does not actually need to be tweaked: just use early stopping instead.\n",
    "   \n",
    "For more best practices regarding tuning neural network hyperparameters, check out the 2018 paper by Leslie Smith.\n",
    "\n",
    "This concludes our introduction to artificial neural netowkrs & their implementation with Keras. In the next lessons, we will discuss techniques to train very deep nets. We will also explore how to customise models using Tensorflow's lower-level API & how to load & preprocess data efficiently using the data API. We will dive into other popular neural network architectures: convolutional neural networks for image processing, recurrent neural networks for sequential data, autoencoders for representation learning, & generative adversarial networks to model & generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946997d-a4a8-452f-bf64-0dbea48e71ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
