{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bd80d0-206c-4f96-bdc6-0cedbbbb90e3",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. The Tensorflow playground is a handy neural network simulator built by the Tensorflow team. In this exercise, you will train several binary classifiers in just a few clicks, & tweak the model's architecture & its hyperparameters to gain some intuition on how neural networks work & what their hyperparameters do. Take some time to explore the following:\n",
    "   a. The patterns learned by a neural net. Try training the default neural network by clicking the Run button (top left). Notice how it quickly finds a good solution for the classification task. The neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns. In general, the more layers there are, the more complex the patterns can be.\n",
    "   b. Activation functions. Try replacing the tanh activation with a ReLU activation function, & train the network again. Notice that it finds a solution even faster, but this time the boundaries are linear. This is due to the shape of the ReLU function.\n",
    "   c. The risk of local minima. Modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, click the Reset button next to the Play button). Notice that the training time varies a lot, & sometimes it even gets stuck in a local minimum.\n",
    "   d. What happens when neural nets are too small? Remove one neuron to keep just two. Notice that the neural network is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters & systematically underfits the training set.\n",
    "   e. What happens when neural nets are large enough? Set the number of neurons to eight, & train the network several times. Notice that it is now consistently fast & never gets stuck. This highlights an important finding in neural network theory: large neural networks almost never get stuck in local minima, & even when they do these local optima are almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.\n",
    "   f. The risk of vanishing gradients in deep networks. Select the spiral dataset (the bottom-right dataset under \"DATA\"), & change the network architecture to have four hidden layers with eight neurons each. Notice that training takes much longer & often gets stuck on plateaus for long periods of time. Also notice that the neurons in the highest layers (on the right) tend to evolve faster than neurons in the lowest layers (on the left). This problem, called the \"vanishing gradients\" problem, can be alleviated with better weight initialisation & other techniques, better optimisers (such as AdaGrad or Adam), or batch normalisation.\n",
    "   g. Go further. Take an hour or so to play around with other parameters & get a feel for what they do, to build an intuitive understanding about neural networks.\n",
    "2. Draw an ANN using the original artificial neurons that computes $A \\oplus B$ (where $\\oplus$ represents the XOR operation). Hint: $A \\oplus B = (A \\land \\neg B) \\lor (\\neg A \\land B)$.\n",
    "3. Why is it generally preferrable to use a logistic regression classifier rather than a classical perceptron (i.e., a single layer of threshold logic units trained using the perceptron training algorithm)? How can you tweak a perceptron to make it equivalent to a logistic regression classifier?\n",
    "4. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "5. Name three popular activation functions. Can you draw them?\n",
    "6. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, & finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "   * What is the shape of the input matrix $X$?\n",
    "   * What are the shapes of the hidden layer's weight vector $W_j$ & its bias vector $b_h$?\n",
    "   * What are the shapes of the output layer's weight vector $W_o$ & its bias vector $b_o$?\n",
    "   * What is the shape of the network's output matrix $Y$?\n",
    "   * Write the equation that computes the network's output matrix $Y$ as a function of $X$, $W_h$, $b_h$, $W_o$, & $b_o$.\n",
    "7. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, & which activation function should you use? What about for getting your network to predict housing prices?\n",
    "8. What is backpropagation & how does it work? What is the difference between backpropagation & reverse-mode autodiff?\n",
    "9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "10. Train a deep MLP on the MNIST dataset (you can load it using the `keras.datasets.mnist.load_data()`. See if you can get over 98% precision. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the loss, & finding the point where the loss shoots up). Try adding all the bells & whistles -- save checkpoints, use early stopping, & plot learning curves using Tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36104382-ca17-4eea-a2da-6c567c5222c7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0945dc6-3288-4516-8a9b-afff5df55318",
   "metadata": {},
   "source": [
    "1. [Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.27089&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "2. <img src = \"Images/Xor Operation.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "3. Perceptrons do not output a class probability; they make predictions based on a hard threshold, unlike logistic regression classifiers, which output class probabilities. Also, perceptrons can only converge to a solution if the training instances are linearly separable (*perceptron convergence theorem*). You can make a perceptron equivalent to a logistic regression classifier by changing the activation function to the  logistic activation function (softmax activation function for multiple outputs). Then you train it with gradient descent, similar to logistic regression.\n",
    "4. It's derivative is never zero, the MLP could always converge to a solution.\n",
    "5. <img src = \"Images/Activation Functions & Their Derivatives.png\" widht = \"600\" style = \"margin:auto\"/>\n",
    "6. The input matrix has 10 input neurons, so its size would be 10 rows x $m$ columns, where $m$ is the mini-batch size (# of instances). The shape of the hidden layer's weight vector $W_j$ & its bias vector $b_h$ is (10, 50) & 50, respectively. The shape of the outputer layer's weight vector $W_o$ & its bias vector $b_o$ is (50, 3) & 3, respectively. The shape of the network's output matrix $Y$ is 3 * $m$; once again, $m$ is the mini-batch size.\n",
    "7. The equation is $\\phi((\\phi(XW_h + b_h))W_o + b_o)$ where $\\phi$ is ReLU activation function.\n",
    "8. Backpropagation is a technique used to find out each connection weight & each bias term in a neural network with regard to every single model parameter. It handles one mini-batch at a time & goes through the full training set multiple times. Each mini-batch is passed through the network's input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this hidden layer for each instance in the mini-batch, & the output is passed on to the next layer. This process is repeaeted until we get the output of the last layer, the output layer. This is the *forward pass*; it is exactly like making a prediction, but the intermediate results are preserved for the *backward pass*. The algorithm then measure the network's output error using a loss function that compares the desired output & the actual output of the network, returning some measure of the error. Then it computes how much each output connection contributed to the error. The algorithm then computes how much of these error contributions came from the connection in the layer below, & so on, until it reaches the first hidden layer. Finally, the algorithm performs gradient descent to tweak all the connection weights in the network using the error gradients it computed. Reverse-mode auto diff performs forward pass through a network & then performs a reverse pass, computing the error gradients. In essence, backpropagation is the entire process of training a neural network using backpropagation steps, computing the error gradient & performing gradient descent on each mini-batch, until its gone through the training set multiple times. Reverse-mode auto diff is the technique of computing the gradients efficiently, which backpropagation happens to use.\n",
    "9. Number of hidden layers, number of neurons in hidden layers, learning rate, optimiser, activation function, number of training iterations. If a MLP overfits the training data, you can reduce the number hidden layers & reduce the number of neurons in your hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a339698-f953-4db1-8d20-0a7569befe01",
   "metadata": {},
   "source": [
    "# 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaecd7f-6f3f-4941-9660-06d8f565cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_val, X_train = X_train[:5000] / 255.0, X_train[5000:] / 255.0\n",
    "y_val, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad704fe7-22bb-40e2-af92-0dd4c22e195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 18:01:02.818909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5879 - accuracy: 0.8465 - val_loss: 0.3084 - val_accuracy: 0.9164\n",
      "Epoch 2/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2897 - accuracy: 0.9179 - val_loss: 0.2523 - val_accuracy: 0.9326\n",
      "Epoch 3/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2391 - accuracy: 0.9323 - val_loss: 0.2109 - val_accuracy: 0.9408\n",
      "Epoch 4/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2050 - accuracy: 0.9425 - val_loss: 0.1828 - val_accuracy: 0.9496\n",
      "Epoch 5/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1790 - accuracy: 0.9491 - val_loss: 0.1667 - val_accuracy: 0.9544\n",
      "Epoch 6/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1588 - accuracy: 0.9550 - val_loss: 0.1477 - val_accuracy: 0.9606\n",
      "Epoch 7/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1417 - accuracy: 0.9604 - val_loss: 0.1374 - val_accuracy: 0.9606\n",
      "Epoch 8/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1277 - accuracy: 0.9639 - val_loss: 0.1257 - val_accuracy: 0.9662\n",
      "Epoch 9/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1161 - accuracy: 0.9671 - val_loss: 0.1182 - val_accuracy: 0.9692\n",
      "Epoch 10/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1061 - accuracy: 0.9707 - val_loss: 0.1103 - val_accuracy: 0.9688\n",
      "Epoch 11/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0972 - accuracy: 0.9728 - val_loss: 0.1070 - val_accuracy: 0.9698\n",
      "Epoch 12/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0894 - accuracy: 0.9756 - val_loss: 0.1004 - val_accuracy: 0.9698\n",
      "Epoch 13/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0828 - accuracy: 0.9766 - val_loss: 0.0975 - val_accuracy: 0.9710\n",
      "Epoch 14/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0767 - accuracy: 0.9786 - val_loss: 0.0958 - val_accuracy: 0.9732\n",
      "Epoch 15/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0717 - accuracy: 0.9804 - val_loss: 0.0902 - val_accuracy: 0.9748\n",
      "Epoch 16/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0667 - accuracy: 0.9815 - val_loss: 0.0873 - val_accuracy: 0.9748\n",
      "Epoch 17/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0621 - accuracy: 0.9825 - val_loss: 0.0872 - val_accuracy: 0.9744\n",
      "Epoch 18/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0580 - accuracy: 0.9840 - val_loss: 0.0829 - val_accuracy: 0.9756\n",
      "Epoch 19/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0544 - accuracy: 0.9853 - val_loss: 0.0819 - val_accuracy: 0.9764\n",
      "Epoch 20/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0513 - accuracy: 0.9861 - val_loss: 0.0789 - val_accuracy: 0.9768\n",
      "Epoch 21/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0479 - accuracy: 0.9877 - val_loss: 0.0783 - val_accuracy: 0.9764\n",
      "Epoch 22/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0450 - accuracy: 0.9884 - val_loss: 0.0761 - val_accuracy: 0.9770\n",
      "Epoch 23/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0421 - accuracy: 0.9893 - val_loss: 0.0766 - val_accuracy: 0.9776\n",
      "Epoch 24/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0398 - accuracy: 0.9903 - val_loss: 0.0737 - val_accuracy: 0.9790\n",
      "Epoch 25/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0374 - accuracy: 0.9909 - val_loss: 0.0764 - val_accuracy: 0.9774\n",
      "Epoch 26/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0352 - accuracy: 0.9915 - val_loss: 0.0722 - val_accuracy: 0.9808\n",
      "Epoch 27/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0332 - accuracy: 0.9924 - val_loss: 0.0726 - val_accuracy: 0.9780\n",
      "Epoch 28/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0311 - accuracy: 0.9929 - val_loss: 0.0732 - val_accuracy: 0.9776\n",
      "Epoch 29/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0293 - accuracy: 0.9933 - val_loss: 0.0738 - val_accuracy: 0.9774\n",
      "Epoch 30/50\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0278 - accuracy: 0.9941 - val_loss: 0.0704 - val_accuracy: 0.9794\n",
      "Epoch 31/50\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0261 - accuracy: 0.9945 - val_loss: 0.0714 - val_accuracy: 0.9790\n",
      "Epoch 32/50\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0244 - accuracy: 0.9948 - val_loss: 0.0709 - val_accuracy: 0.9794\n",
      "Epoch 33/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0232 - accuracy: 0.9951 - val_loss: 0.0706 - val_accuracy: 0.9792\n",
      "Epoch 34/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0218 - accuracy: 0.9956 - val_loss: 0.0716 - val_accuracy: 0.9784\n",
      "Epoch 35/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0207 - accuracy: 0.9961 - val_loss: 0.0694 - val_accuracy: 0.9792\n",
      "Epoch 36/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0195 - accuracy: 0.9965 - val_loss: 0.0699 - val_accuracy: 0.9796\n",
      "Epoch 37/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0185 - accuracy: 0.9968 - val_loss: 0.0697 - val_accuracy: 0.9786\n",
      "Epoch 38/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 0.0703 - val_accuracy: 0.9792\n",
      "Epoch 39/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0165 - accuracy: 0.9972 - val_loss: 0.0690 - val_accuracy: 0.9790\n",
      "Epoch 40/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0156 - accuracy: 0.9975 - val_loss: 0.0695 - val_accuracy: 0.9788\n",
      "Epoch 41/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0147 - accuracy: 0.9980 - val_loss: 0.0723 - val_accuracy: 0.9790\n",
      "Epoch 42/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0140 - accuracy: 0.9981 - val_loss: 0.0698 - val_accuracy: 0.9796\n",
      "Epoch 43/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0133 - accuracy: 0.9983 - val_loss: 0.0695 - val_accuracy: 0.9800\n",
      "Epoch 44/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0126 - accuracy: 0.9984 - val_loss: 0.0703 - val_accuracy: 0.9784\n",
      "Epoch 45/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0120 - accuracy: 0.9988 - val_loss: 0.0707 - val_accuracy: 0.9788\n",
      "Epoch 46/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0114 - accuracy: 0.9989 - val_loss: 0.0709 - val_accuracy: 0.9794\n",
      "Epoch 47/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0109 - accuracy: 0.9990 - val_loss: 0.0710 - val_accuracy: 0.9780\n",
      "Epoch 48/50\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0104 - accuracy: 0.9991 - val_loss: 0.0693 - val_accuracy: 0.9796\n",
      "Epoch 49/50\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0098 - accuracy: 0.9991 - val_loss: 0.0705 - val_accuracy: 0.9806\n",
      "Epoch 50/50\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0095 - accuracy: 0.9991 - val_loss: 0.0719 - val_accuracy: 0.9782\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.Dense(300, activation = \"relu\"),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "])\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = \"sgd\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 50,\n",
    "                    validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c08905-2688-4fae-8b1d-6714afe26fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 13.1555 - accuracy: 0.9793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13.155512809753418, 0.9793000221252441]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63cad35-d356-4027-bd3a-9e805ed9c15f",
   "metadata": {},
   "source": [
    "Close enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5578e-101a-4617-be6c-da40ef564f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
