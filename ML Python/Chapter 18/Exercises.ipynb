{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a568087a-6700-4ba7-b037-ff5da7ac65be",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. How would you define reinforcement learning? How is it different from regular supervised or unsupervised learning?\n",
    "2. Can you think of three possible applications of RL that were not mentioned in this lesson? For each of them, what is the environment? What is the agent? What are some possible actions? What are the possible rewards?\n",
    "3. What is the discount factor? Can the optimal policy change if you modify the discount factor?\n",
    "4. How do you measure the performance of a reinforcement learning agent?\n",
    "5. What is the credit assignment problem? When does it occur? How can you alleviate it?\n",
    "6. What is the point of using a replay buffer?\n",
    "7. What is an off-policy RL algorithm?\n",
    "8. Use policy gradients to solve OpenAI gym's LunarLander-v2 environment. You will need to install the Box2D dependencies (`pip install --user gym[box2d]`).\n",
    "9. Use tf-agents to train an agent that can achieve a superhuman level at SpaceInvaders-v4 using any of the available algorithms.\n",
    "10. If you have $100 to spare, you can purchase a Raspberry Pi 3 plus some cheap robotics components, install TensorFlow on theh Pi, & go wild! For an example, check out the posts by Lukas Biewald, or take a look a GoPiGo or BrickPi. Start with simple goals like making the robot turn around to find the brightest angle (if it has a light sensor) or the closest object (if it has a sonar sensor), & move in that direction. Then you can start using deep learning: for example, if the robot has a camera, you can try to implement an object detection algorithm so it detects people & moves toward them. You can also try to use RL to make the agent learn on its own how to use the motors to achieve that goal. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d70e7-fcb6-4b34-8cc7-ce63b03be3bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74587ee7-147e-447b-ac39-aa800a17006d",
   "metadata": {},
   "source": [
    "1. Reinforcement learning is the oldest field of machine learning where an agent makes observations & takes actions within an environment, & in return it receives rewards. It learns to act in a way that maximises its expected reward over time. Reinforcement learning does have some form of supervision, through rewards. Although we do not directly tell the agent how to perform a task, the rewards let the model know when it is making progress or when it is failing. Unsupervised & supervised learning generally find patterns in the data to make predictions, while in reinforcement learning, the goal is to find a good policy. Reinforcement learning is more complex: it has to find the right balance between exploring the environment, looking for new ways to get rewards, & exploiting sources of rewards it already knows, while supervised & unsupervised learning systems don't worry about exploration. Training instances are also independent for supervised & unsupervised learning while in reinforcement learning, consecutive observations are not independent. Consecutive observations are very correlated. However, sampling from the replay memory (buffer) can get you independent observations.\n",
    "2. (*a*) Self-driving cars: the environment is the real world, the agent is the car, the actions are the external capabilities of the car, forward & reverse acceleration, steering, etc. The rewards could be negative incrementers: you want to go from point A to B in as little time as possible, while obeying traffic laws & avoiding/preventing accidents. (*b*) Probably Youtube's ads or any ads or any recommender system: the environment is Youtube. The actions would be the recommendations. The rewards are views or traffic. (*c*) drug discovery & design: the environment would be a simulated chemical environment used to analyse behavior between molecules & atoms in a human body, actions could be moving the developed chemical compound through its chemical pathway, the reward would be if the compound performs what it is intended to do.\n",
    "3. One problem that reinforcement learning algorithms face is the credit assignment problem. When an agent gets an reward, it is hard for it to know which actions should get credited (or blamed) for it. To tackle this problem, you can evaluate an action based on the sum of all the rewards that come after it, usually applying a discount factor $\\gamma$ at each step: $\\gamma * R_n + \\gamma^2 * R_{n + 1} + \\gamma^3 * R_{n + 2} + ...$ where R is the earned reward at its corresponding time step. The discount factor is any value between 0 & 1. Typical discount factors vary from 0.9 to 0.99. Changing the discount factor changes the optimal policy, because the weight of the future rewards change as well.\n",
    "4. Look at the rewards it gets. If there are multiple episodes, then look at the total rewards it gets on average.\n",
    "5. Answered already.\n",
    "6. Since consecutive observations are correlated, using a replay buffer can help to reduce the correlations in the training batch. All experiences are stored in the replay buffer & sampled at random at each training iteration, helping gradient descent perform optimally.\n",
    "7. An off-policy algorithm is an algorithm where the policy being trained is not necessarily the one being executed. For example, the Q-learning example is an off-policy algorithm: the policy being executed (the exploration policy) is completely random, while the policy being trained will always choose the action with the highest Q-value. This is the opposite of an on-policy algorithm, which explore the world using the policy being trained (e.g., policy gradients algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a770605-2326-4ca2-a61b-dd8bca0f3538",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f501fa-bcb2-452c-9bfe-7342918603ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make(\"LunarLander-v2\", render_mode = \"human\")\n",
    "env.observation_space\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bde6b-5094-4665-bd5e-19bbd9dc7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1969a-3f59-4d18-846a-1225272b3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape = [n_inputs]),\n",
    "    keras.layers.Dense(32, activation = \"relu\"),\n",
    "    keras.layers.Dense(32, activation = \"relu\"),\n",
    "    keras.layers.Dense(n_outputs, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b1f82-a893-4b1d-80c7-27b54c94401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lander_play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probas = model(obs[np.newaxis])\n",
    "        logits = tf.math.log(probas + keras.backend.epsilon())\n",
    "        action = tf.random.categorical(logits, num_samples = 1)\n",
    "        loss = tf.reduce_mean(loss_fn(action, probas))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(action[0, 0].numpy())\n",
    "    return obs, reward, done, grads\n",
    "\n",
    "def lander_play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = lander_play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done: \n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5ff7f-00c2-4038-8ec6-24011cc34ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_rate\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalise_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ee030-ea3d-4493-a7b5-a45b468e8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 200\n",
    "n_episodes_per_update = 16\n",
    "n_max_steps = 1000\n",
    "discount_rate = 0.99\n",
    "\n",
    "optimiser = keras.optimizers.Nadam(learning_rate = 0.005)\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e0a3a-5d0c-4140-b099-4255cb4568e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = lander_play_multiple_episodes(env, n_episodes_per_update,\n",
    "                                                           n_max_steps, model, loss_fn)\n",
    "    mean_reward = sum(map(sum, all_rewards)) / n_episodes_per_update\n",
    "    print(\"\\rIteration: {}/{}, mean reward: {:.1f} \".format(iteration + 1,\n",
    "                                                            n_iterations,\n",
    "                                                            mean_reward), end = \"\")\n",
    "    mean_rewards.append(mean_reward)\n",
    "    all_final_rewards = discount_and_normalise_rewards(all_rewards, discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean([final_reward * all_grads[episode_index][step][var_index]\n",
    "                                     for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                                     for step, final_reward in enumerate(final_rewards)], axis = 0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimiser.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726a409-842c-46f4-8dbd-bbbb62cc5f9d",
   "metadata": {},
   "source": [
    "# 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66f951-4531-4e1b-8982-8cce6353c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"SpaceInvaders-v0\", render_mode = \"human\")\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "env.unwrapped_get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de653f8-08ec-4e28-a8e8-4b930256f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape = [3, height, width, channels]),\n",
    "    keras.layers.Conv2D(32, (8, 8), 4, activation = \"relu\"),\n",
    "    keras.layers.Conv2D(64, (4, 4), 2, activation = \"relu\"),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation = \"relu\"),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation = \"relu\"),\n",
    "    keras.layers.Dense(256, activation = \"relu\"),\n",
    "    keras.layers.Dense(actions, activations = \"linear\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a0e7d-e713-4307-9e4e-5b593c800bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr = \"eps\", value_max = 1.,\n",
    "                                  value_min = 0.1, value_test = 0.2, nb_steps = 10000)\n",
    "    memory = SequentialMemory(limit = 1000, window_length = 3)\n",
    "    dqn = DQNAgent(model = model, memory = memory, policy = policy,\n",
    "                   enable_dueling_network = True, dueling_type = \"avg\",\n",
    "                   nb_actions = actions, nb_steps_warmup = 1000)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c130b5-715d-4f02-84fd-0532c777126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(keras.optimizers.Adam(learning_rate = 1e-4))\n",
    "dqn.fit(env, nb_steps = 10000, verbose = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
