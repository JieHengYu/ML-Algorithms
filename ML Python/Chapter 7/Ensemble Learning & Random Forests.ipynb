{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703d7d32-1b6d-413f-b9c0-f8cfa1c42993",
   "metadata": {},
   "source": [
    "# Ensemble Learning & Random Forests\n",
    "\n",
    "Suppose you pose a complex question to thousands of random people, then aggregate their answers. In many cases, you will find that this aggregated answer is better than an expert's answer. This is called the *wisdom of the crowd*. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an *ensemble*; thus, this technique is called *ensemble learning*, & an ensemble learning algorithm is called an *ensemble method*.\n",
    "\n",
    "As an example of an ensemble method, you can train a group of decision tree classifiers, each on a different random subset of the training set. To make predictions, you obtain the predictions of all the individual trees, then predict the class that gets the most votes. Such an ensemble of decision trees is called a *random forest*, & despite its simplicity, it is one of the most powerful machine learning algorithms available today.\n",
    "\n",
    "As discussed before, you will often use ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor. In fact, the winning solutions in machine learning competitions often involve several ensemble methods.\n",
    "\n",
    "In this lesson, we will discuss the most popular ensemble methods, including *bagging*, *boosting*, & *stacking*. We will also explore random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc02bd-4729-4f90-8267-e3f7563b3723",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116ab51-9d28-428e-8a5f-89bc89707b1c",
   "metadata": {},
   "source": [
    "# Voting Classifiers\n",
    "\n",
    "Suppose you have trained a few classifiers, each on achieving about 80% accuracy. You may have a logistic regression classifier, an SVM classifier, a random forest classifier, a K-nearest neighbors classifier, & perhaps a few more.\n",
    "\n",
    "<img src = \"Images/Diverse Classifiers.png\" width = \"450\" style = \"margin:auto\"/>\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier & predict the class that gets the most votes. This majority-vote classifier is called a *hard-voting* classifier.\n",
    "\n",
    "<img src = \"Images/Hard Voting Classifier.png\" width = \"450\" style = \"margin:auto\"/>\n",
    "\n",
    "Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a *weak learner* (meaning it does only slightly better than random guessing), the ensemble can still be a *strong learner* (achieving high accuracy), provided there are a sufficient number of weak learners & they are sufficiently diverse.\n",
    "\n",
    "How is this possible? The following analogy can help shed some light on this mystery. Suppose you have a slightly biased coin that has 51% chance of comming up heads & 49% chance of coming up tails. If you toss it 1,000 times, you will generally get more or less 510 heads & 490 tails, & hence a majority of heads. If you do the math, you will find that the probability of obtaining a majority of heads after 1,000 tosses is close to 75%. The more you toss the coin, the higher the probability (e.g., the probability climbs over 97%). This is due to the *law of large numbers*: as you keep tossing the coin, the ratio of heads gets closer & closer to the probability of heads (51%). The below figure shows 10 series of biased coin tosses. You can see that as the number of tosses increases, the ratio of heads approaches 51%. Eventually all 10 series end up so close to 51% that they are consistently above 50%.\n",
    "\n",
    "<img src = \"Images/Law of Large Numbers.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Similarly, suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (barely better than random guessing). If you predict the majority voted class, you can hope for up to 75% accuracy! However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data. They are likely to make the same errors, so there will be many majority votes for the wrong class, reducing the ensemble's accuracy.\n",
    "\n",
    "The following code creates & trains a voting classifier in scikit-learn, composed of three diverse classifiers (the training set is the moons dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c0eb1d-a307-45b2-af93-93e98d3798dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples = 500, noise = 0.30, random_state = 42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state =42)\n",
    "\n",
    "log_classifier = LogisticRegression()\n",
    "forest_classifier = RandomForestClassifier()\n",
    "svm_classifier = SVC()\n",
    "voting_classifier = VotingClassifier(estimators = [(\"lr\", log_classifier),\n",
    "                                                   (\"rf\", forest_classifier),\n",
    "                                                   (\"svc\", svm_classifier)],\n",
    "                                    voting = \"hard\")\n",
    "voting_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a14cd6-2b71-4f50-9d66-e0d0c8275488",
   "metadata": {},
   "source": [
    "Let's look at each classifier's accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d56808d-5f7e-46d7-8167-3148e34e9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.912\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for classifier in (log_classifier, forest_classifier, svm_classifier, voting_classifier):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(classifier.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f3542-0eb8-4071-be7d-94f2ce77a096",
   "metadata": {},
   "source": [
    "There you have it! The voting classifier slightly outperforms all the individual classifiers.\n",
    "\n",
    "If all classifiers are able to estimate class probabilities (i.e., they all have a `predict_proba()` method), then you can tell scikit-learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called *soft voting*. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do it replace `voting = \"hard\"` with `voting = \"soft\"` & ensure that all classifiers can estimate class probabilities. This is not the case for the `SVC` class by default, so you need to set its `probability` hyperparameter to `True` (this will make the `SVC` class use cross-validation to estimate class probabilities, slow down training, & it will add a `predict_proba()` method). If you modify the preceding code to use soft voting, you will find that the voting classifier achieves even higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb04f82-1c8e-491f-9022-105630b0eff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()),\n",
       "                             ('svc', SVC(probability=True))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_classifier = LogisticRegression()\n",
    "forest_classifier = RandomForestClassifier()\n",
    "svm_classifier = SVC(probability = True)\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators = [(\"lr\", log_classifier),\n",
    "                                                   (\"rf\", forest_classifier), \n",
    "                                                   (\"svc\", svm_classifier)],\n",
    "                                    voting = \"soft\")\n",
    "voting_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73dd06ff-29ba-4160-aedb-fce310304f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.896\n",
      "VotingClassifier 0.928\n"
     ]
    }
   ],
   "source": [
    "for classifier in (log_classifier, forest_classifier, svm_classifier, voting_classifier):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(classifier.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfee87b-7482-4d8b-a623-eef12483c75a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20607778-fb67-4fd9-bd14-c63500c9400e",
   "metadata": {},
   "source": [
    "# Bagging & Pasting\n",
    "\n",
    "One way to get a diverse set of classifier is to use very different training algorithms, as just discessed. Another approach is to use the same training algorithm for every predictor & train random subsets of the training set. When sampling with replacement, this method is called *bagging* (short for *bootstrap aggregating*). When sampling is performed *without* replacement, it is called *pasting*.\n",
    "\n",
    "In other words, both bagging & pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor. This sampling & training process is represented below.\n",
    "\n",
    "<img src = \"Images/Bagging.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the *statistical mode* (i.e., the most frequent predictions, just like a hard voting classifier) for classifier, or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias & variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "As you can see, predictors can all be trained in parallel, via differnt CPU cores or even different servers. Similarly, predictions can be made in parallel. This is one of the reasons bagging & pasting are such popular methods: they scale very well.\n",
    "\n",
    "## Bagging & Pasting in Scikit-Learn\n",
    "\n",
    "Scikit-learn offers a simple API for both bagging & pasting with the `BaggingClassifier` class (or `BaggingRegressor` for regression). The following code trains an ensemble of 500 decision tree classifiers: each is trained on 100 training instances randomly sampled from the training set with replacement (this is an example of bagging, but if you want to use pasting, just set `bootstrap = False`). The `n_jobs` parameter tells scikit-learn the number of CPU cores to use for training & predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "215c0b39-3c32-4c94-b0b7-ff861ba046fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_classifier = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500,\n",
    "                                   max_samples = 100, bootstrap = True, n_jobs = 7)\n",
    "bag_classifier.fit(X_train, y_train)\n",
    "y_pred = bag_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51baff-62c7-42d7-aa78-398fbba82cb7",
   "metadata": {},
   "source": [
    "The below figure compares the decision boundary of a single decision tree with a decision boundary of a bagging ensemble of 500 trees (from the above code), both trained on the moons dataset. As you can see, the ensemble's predictions will likely generalise much better than the single decision tree's predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular).\n",
    "\n",
    "<img src = \"Images/Effect of Bagging.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slgihtly higher bias than pasting; but the extra diversity also means that the predictors end up beling less correlated, so the ensemble's variance is reduced. Overall, bagging often results in better models, which explains why it is generally preferred. However, if you have spare time & CPU power, you can use cross-validation to evaluate both bagging & pasting & select the one that works best.\n",
    "\n",
    "## Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default, a `BaggingClassifier` samples *m* training instances with replacement (`bootstrap = True`), where *m* is the size of the training set. This means that only about 63% of the training instances are sampled on average for each predictor. The remaining 37% of the training instances that are not sampled are called *out-of-bag* (oob) instances. Note that they are not the same 37% for all predictors.\n",
    "\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set. You can evaluate the ensemble itself by averaging out the oob evalusations of each predictor.\n",
    "\n",
    "In scikit-learn, you can set the `oob_score = True` when creating a `BaggingClassifier` to request an automatic oob evaluation after training. The following code demonstrates this. The resulting evaluation score is available through the `oob_score_` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20dff3f4-c4ec-46f1-add9-5d1d7e76e5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_classifier = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500,\n",
    "                                   bootstrap = True, n_jobs = 7, oob_score = True)\n",
    "bag_classifier.fit(X_train, y_train)\n",
    "bag_classifier.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ae49d-5625-49b6-9eb0-08af79e85887",
   "metadata": {},
   "source": [
    "According to this oob evaluation, the `BaggingClassifier` is likely to achieve about 90% accuracy on the test set. Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54171d99-6cfe-4ba5-bdda-4288879252b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_classifier.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9007389-362f-4bf1-b39f-fb990592116d",
   "metadata": {},
   "source": [
    "Yup, close enough!\n",
    "\n",
    "The oob decision function for each training instance is also available through the `oob_decision_function_` variable. In this case (since the base estimator has a `predict_proba()` method), the decision function returns the class probabilities for each training instance. For example, the oob evaluation estimates that the first training instance has a 61.2% probability of belonging to the positive class (& 38.8% of belonging to the negative class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "664c76c3-4fd3-43f4-9433-de2179849469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38797814, 0.61202186],\n",
       "       [0.32984293, 0.67015707],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06896552, 0.93103448],\n",
       "       [0.3       , 0.7       ],\n",
       "       [0.03141361, 0.96858639],\n",
       "       [0.99494949, 0.00505051],\n",
       "       [0.98421053, 0.01578947],\n",
       "       [0.78235294, 0.21764706],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70744681, 0.29255319],\n",
       "       [0.84090909, 0.15909091],\n",
       "       [0.98469388, 0.01530612],\n",
       "       [0.04371585, 0.95628415],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96774194, 0.03225806],\n",
       "       [0.9558011 , 0.0441989 ],\n",
       "       [0.98918919, 0.01081081],\n",
       "       [0.01612903, 0.98387097],\n",
       "       [0.28333333, 0.71666667],\n",
       "       [0.94478528, 0.05521472],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9787234 , 0.0212766 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.65625   , 0.34375   ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12849162, 0.87150838],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.3627451 , 0.6372549 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26256983, 0.73743017],\n",
       "       [0.38383838, 0.61616162],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02173913, 0.97826087],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01639344, 0.98360656],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9039548 , 0.0960452 ],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.95979899, 0.04020101],\n",
       "       [0.00497512, 0.99502488],\n",
       "       [0.03867403, 0.96132597],\n",
       "       [0.98351648, 0.01648352],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98395722, 0.01604278],\n",
       "       [0.80628272, 0.19371728],\n",
       "       [0.40883978, 0.59116022],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70857143, 0.29142857],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84126984, 0.15873016],\n",
       "       [1.        , 0.        ],\n",
       "       [0.62745098, 0.37254902],\n",
       "       [0.12087912, 0.87912088],\n",
       "       [0.69142857, 0.30857143],\n",
       "       [0.91      , 0.09      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16201117, 0.83798883],\n",
       "       [0.87777778, 0.12222222],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05319149, 0.94680851],\n",
       "       [0.05128205, 0.94871795],\n",
       "       [0.31638418, 0.68361582],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84916201, 0.15083799],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22727273, 0.77272727],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95      , 0.05      ],\n",
       "       [0.74857143, 0.25142857],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [1.        , 0.        ],\n",
       "       [0.20467836, 0.79532164],\n",
       "       [0.62352941, 0.37647059],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04324324, 0.95675676],\n",
       "       [0.45454545, 0.54545455],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21465969, 0.78534031],\n",
       "       [0.45901639, 0.54098361],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01764706, 0.98235294],\n",
       "       [0.99456522, 0.00543478],\n",
       "       [0.28645833, 0.71354167],\n",
       "       [0.91139241, 0.08860759],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.78735632, 0.21264368],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99473684, 0.00526316],\n",
       "       [0.9939759 , 0.0060241 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94767442, 0.05232558],\n",
       "       [0.99390244, 0.00609756],\n",
       "       [0.01648352, 0.98351648],\n",
       "       [0.21578947, 0.78421053],\n",
       "       [0.95294118, 0.04705882],\n",
       "       [0.23913043, 0.76086957],\n",
       "       [0.98924731, 0.01075269],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01530612, 0.98469388],\n",
       "       [0.72      , 0.28      ],\n",
       "       [0.4213198 , 0.5786802 ],\n",
       "       [0.41212121, 0.58787879],\n",
       "       [0.85393258, 0.14606742],\n",
       "       [0.93193717, 0.06806283],\n",
       "       [0.03723404, 0.96276596],\n",
       "       [0.78061224, 0.21938776],\n",
       "       [0.01005025, 0.98994975],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01081081, 0.98918919],\n",
       "       [0.97487437, 0.02512563],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01058201, 0.98941799],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95679012, 0.04320988],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.        , 1.        ],\n",
       "       [0.36666667, 0.63333333],\n",
       "       [0.2311828 , 0.7688172 ],\n",
       "       [0.00561798, 0.99438202],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27      , 0.73      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99514563, 0.00485437],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00526316, 0.99473684],\n",
       "       [0.68156425, 0.31843575],\n",
       "       [0.91061453, 0.08938547],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98823529, 0.01176471],\n",
       "       [0.99404762, 0.00595238],\n",
       "       [0.99421965, 0.00578035],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0726257 , 0.9273743 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03243243, 0.96756757],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04210526, 0.95789474],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9516129 , 0.0483871 ],\n",
       "       [0.80346821, 0.19653179],\n",
       "       [0.54450262, 0.45549738],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11640212, 0.88359788],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93023256, 0.06976744],\n",
       "       [0.98477157, 0.01522843],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [0.        , 1.        ],\n",
       "       [0.43169399, 0.56830601],\n",
       "       [0.84705882, 0.15294118],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95652174, 0.04347826],\n",
       "       [0.        , 1.        ],\n",
       "       [0.23529412, 0.76470588],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99507389, 0.00492611],\n",
       "       [0.7989418 , 0.2010582 ],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0625    , 0.9375    ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02777778, 0.97222222],\n",
       "       [0.        , 1.        ],\n",
       "       [0.046875  , 0.953125  ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82513661, 0.17486339],\n",
       "       [0.        , 1.        ],\n",
       "       [0.859375  , 0.140625  ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.25555556, 0.74444444],\n",
       "       [0.2617801 , 0.7382199 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25149701, 0.74850299],\n",
       "       [0.95604396, 0.04395604],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97512438, 0.02487562],\n",
       "       [0.        , 1.        ],\n",
       "       [0.43786982, 0.56213018],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11464968, 0.88535032],\n",
       "       [0.09042553, 0.90957447],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.37      , 0.63      ],\n",
       "       [0.10285714, 0.89714286],\n",
       "       [0.47802198, 0.52197802],\n",
       "       [0.56      , 0.44      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.63687151, 0.36312849],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26767677, 0.73232323],\n",
       "       [0.82010582, 0.17989418],\n",
       "       [0.09444444, 0.90555556],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77604167, 0.22395833],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00653595, 0.99346405],\n",
       "       [0.11111111, 0.88888889],\n",
       "       [0.01104972, 0.98895028],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99438202, 0.00561798],\n",
       "       [0.91623037, 0.08376963],\n",
       "       [0.22033898, 0.77966102],\n",
       "       [0.96907216, 0.03092784],\n",
       "       [0.00588235, 0.99411765],\n",
       "       [0.61650485, 0.38349515],\n",
       "       [0.12626263, 0.87373737],\n",
       "       [0.98924731, 0.01075269],\n",
       "       [0.85714286, 0.14285714],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95187166, 0.04812834],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.29834254, 0.70165746],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87192118, 0.12807882],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7591623 , 0.2408377 ],\n",
       "       [0.95402299, 0.04597701],\n",
       "       [1.        , 0.        ],\n",
       "       [0.67836257, 0.32163743],\n",
       "       [0.4950495 , 0.5049505 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89444444, 0.10555556],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.86363636, 0.13636364],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.78306878, 0.21693122],\n",
       "       [0.03092784, 0.96907216],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.23560209, 0.76439791],\n",
       "       [0.        , 1.        ],\n",
       "       [0.91794872, 0.08205128],\n",
       "       [0.87647059, 0.12352941],\n",
       "       [0.01104972, 0.98895028],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99492386, 0.00507614],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01675978, 0.98324022],\n",
       "       [0.97596154, 0.02403846],\n",
       "       [0.95811518, 0.04188482],\n",
       "       [1.        , 0.        ],\n",
       "       [0.54301075, 0.45698925],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98930481, 0.01069519],\n",
       "       [0.02824859, 0.97175141],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97093023, 0.02906977],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0755814 , 0.9244186 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01041667, 0.98958333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.14197531, 0.85802469],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [0.        , 1.        ],\n",
       "       [0.45555556, 0.54444444],\n",
       "       [0.09659091, 0.90340909],\n",
       "       [0.2       , 0.8       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97740113, 0.02259887],\n",
       "       [0.18934911, 0.81065089],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.975     , 0.025     ],\n",
       "       [0.30769231, 0.69230769],\n",
       "       [0.98974359, 0.01025641],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98924731, 0.01075269],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05813953, 0.94186047],\n",
       "       [0.97826087, 0.02173913],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03888889, 0.96111111],\n",
       "       [0.63106796, 0.36893204]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_classifier.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ab02b-1b46-4f6d-9eef-2b8a37038076",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f4617-2bbc-4b0c-8bb8-9c22c1a561a0",
   "metadata": {},
   "source": [
    "# Random Patches & Random Subspaces\n",
    "\n",
    "The `BaggingClassifier` class supoorts sampling the features as well. Sampling is controlled by two hyperparameters: `max_features` & `bootstrap_features`. They work the same way as `max_samples` & `bootstrap`, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. \n",
    "\n",
    "This technique is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances & features is called the *random patches method*. Keeping all training instances (by setting `bootstrap = False` & `max_samples = 1.0`) but sampling features (by setting `bootstrap_features` to `True` &/or `max_features` to a value smaller to `1.0`) is called the *random subspaces method*.\n",
    "\n",
    "Sampling features results in even more predictor diversity, training a bit more bias for a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec0ba8-eeab-4521-85d6-21c074af8e22",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d29b9-97f0-4c83-b767-4898144925a4",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "As we have discussed, a random forest is ensemble of decision trees, generally trained via the bagging method (or sometimes pasting), typically with `max_samples` set to the size of the training set. Instead of building a `BaggingClassifier` & passing it a `DecisionTreeClassifier` you can instead use the `RandomForestClassifier` class, which is more convenient & optimised for decision trees (similarly, there is a `RandomForestsRegressor` class for regression tasks). The following code uses all available CPU cores to train a random forest classifier with 500 trees (each limited to maximum 16 nodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36966b7e-6272-4125-b0ae-e2ab8df2a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_classifier = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = 7)\n",
    "forest_classifier.fit(X_train, y_train)\n",
    "y_pred = forest_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bddafa-e402-4276-aabf-f33a37ef230a",
   "metadata": {},
   "source": [
    "With a few exceptions, a `RandomForestClassifier` has all the hyperparameters of a `DecisionTreeClassifier` (to control how trees are grown), plus all the hyperparameters of a `BaggingClassifier` to control the ensemble itself.\n",
    "\n",
    "The random forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. The algorithm results in a greater tree diversity, which (again) trades higher bias for a lower variance, generally yielding an overall better model. The following `BaggingClassifier` is roughly equivalent to the previous `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcb3253d-8ef7-4c28-b0d0-7cb60b16063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_classifier = BaggingClassifier(DecisionTreeClassifier(splitter = \"random\", max_leaf_nodes = 16),\n",
    "                                   n_estimators = 500, max_samples = 1.0, bootstrap = True, n_jobs = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aeac29-076f-4396-b3e8-e4674271105b",
   "metadata": {},
   "source": [
    "## Extra-Trees\n",
    "\n",
    "When you are growing a tree in a random forest, at each node, only a random subset of the features is considered for splitting. It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular decision trees do). \n",
    "\n",
    "A forest of such extremely random trees is called an *extremely randomised trees ensemble* (or *extra-trees* for short). Once again, this technique trades more bias for a lower variance. It also makes extra-trees much faster to train than regular random forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.\n",
    "\n",
    "You can create an extra-trees classifier using scikit-learn's `ExtraTreesClassifier` class. Its API is identical to the `RandomForestClassifier` class. Similarly, the `ExtraTreesRegressor` class has the same API as the `RandomForestRegressor` class.\n",
    "\n",
    "## Feature Importance\n",
    "\n",
    "Yet another great quality of random forests is that they make it easy to measure the relative importance of each feature. Scikit-learn measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node's weight is equal to the number of training samples that are associated with it.\n",
    "\n",
    "Scikit-learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. You can acces the result using the `feature_importances_` variable. For example, the following code trains a `RandomForestClassifier` on the iris dataset & outputs each features importance. It seems that the most important features are the petal length (44%) & width (44%), while sepal length & width are rather unimportant in comparison (10% & 2%, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f337f2-258a-4856-bdb0-7a010dff2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10081274691188823\n",
      "sepal width (cm) 0.023193493440751854\n",
      "petal length (cm) 0.4376400648172749\n",
      "petal width (cm) 0.43835369483008496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "forest_classifier = RandomForestClassifier(n_estimators = 500, n_jobs = 7)\n",
    "forest_classifier.fit(iris[\"data\"], iris[\"target\"])\n",
    "\n",
    "for name, score in zip(iris[\"feature_names\"], forest_classifier.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf28c74-7d0f-49a8-9248-8f71be366c72",
   "metadata": {},
   "source": [
    "Similarly, if you train a random forest classifier on the MNIST dataset & plot each pixel's importance, you get the image represented below.\n",
    "\n",
    "<img src = \"Images/MNIST Pixel Importance.png\" width = \"550\" style = \"margin:auto\"/>\n",
    "\n",
    "Random forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5a601-ce40-452d-8c78-7ea854fc99fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00a9c7-5d76-4e60-9220-e18604b3db4a",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "*Boosting* (originally called *hypothesis boosting*) refers to any ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are *AdaBoost* (short for *adaptive boosting*) & *gradient boosting*. \n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more & more on the hard cases. This is the technique used by AdaBoost.\n",
    "\n",
    "For example, when training an AdaBoost classifier, the algorithm first trains a base classifier (such as decision tree) & uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, & again makes predictions on the training set, updates the instance weights & so on.\n",
    "\n",
    "<img src = \"Images/AdaBoost.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "The below figure shows the decision boundaries of five consecutive predictors on the moons dataset (in this example, each predictor  is a highly regularised SVM classifier with a RBF kernel). The first classifier gets many instances wrong, so their weights get boosted. The second classifier therefore does a better job on these instances, & so on. The plot on the right represents the same sequence of predictors, except that the learning rate is halved (i.e., the misclassified instance weights are boosted half as much at every iteration). As you can see, this sequential learning technique has some similarities with gradient descent, except that instead of tweaking a single predictor's parameters to minimise a cost function, AdaBoost adds predictors to the ensemble, gradually making it better.\n",
    "\n",
    "<img src = \"Images/Decision Boundaries Consecutive Predictors.png\" width = \"550\" style = \"margin:auto\"/>\n",
    "\n",
    "Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.\n",
    "\n",
    "Let's take a closer look at the AdaBoost algorithm. Each instance weight $w^{(i)}$ is initially set to 1/*m*. A first predictor is trained, & its weighted error rate $r_1$ is computed on the training set.\n",
    "\n",
    "$$r_j = \\frac{\\begin{split}\n",
    "\\sum^{m}_{i = 1} w^{(i)} \\\\\n",
    "\\hat{y}^{(i)}_j \\neq y^{(i)}\n",
    "\\end{split}}{\\sum^{m}_{i = 1} w^{(i)}} \\quad where\\ \\hat{y}^{(i)}_j\\ is\\ the\\ j^{th}\\ predictor's\\ prediction\\ for\\ the\\ i^{th}\\ instance.$$\n",
    "\n",
    "The predictor's weight $\\alpha_j$ is then computed using the below equations, where $\\eta$ is the learning rate hyperparameter (defaults to 1). The more accurate the predictor is,the higher its weight will be. if it is just guessing randomly, then its wieght will be close to zero. However, if it is most often wrong (i.e., less accurate than random guessing), then its weight will be negative.\n",
    "\n",
    "$$\\alpha_j = \\eta\\ log \\frac{1 - r_j}{r_j}$$\n",
    "\n",
    "Next, the AdaBoost algorithm updates the instance weights using the below equation, which boosts the weights of the misclassified instances.\n",
    "\n",
    "$$\\begin{split}\n",
    "for\\ i = 1, 2, ..., m \\\\\n",
    "w^{(i)} \\leftarrow \\Biggl\\{ \\begin{split} \n",
    "w^{(i)} \\quad if \\hat{y_j}^{(i)} = y^{(i)} \\\\\n",
    "w^{(i)} e^{\\alpha_j} \\quad if \\hat{y_j}^{(i)} \\neq y^{(i)}\n",
    "\\end{split}\n",
    "\\end{split}$$\n",
    "\n",
    "Then all the instance weights are normalised (i.e., divided by $\\sum^{m}_{i = 1} w^{(i)}$.\n",
    "\n",
    "Finally, a new predictor is trained using the updated weights, & the whole process is repeated (the new predictor's weight is computed, the instance weights are updated, then another predictor is trained, & so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n",
    "\n",
    "To make predictions, AdaBoost simply computes the predictions of all the predictors & weighs them using the predictor weights $\\alpha_j$. The predicted class is the one that receives the majority of the weighted votes.\n",
    "\n",
    "$$\\hat{y}(x) = \\underset{k}{argmax} \\quad \\underset{\\hat{y}_j(x) = k}{\\sum^{N}_{j = 1}} \\alpha_j \\quad where\\ N\\ is\\ the\\ number\\ of\\ predictors.$$  \n",
    "\n",
    "Scikit-learn uses a multiclass version of AdaBoost called *SAMME* (which stands for *stagewise additive modeling using a multiclass exponential loss function*). When there are just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate class probabilities (i.e., if they have a `predict_proba()` method), scikit-learn can use a variant of SAMME called *SAMME.R* (the *R* stands for \"Real\"), which relies on class probabilities rather than predictions & generally performs better.\n",
    "\n",
    "The following code trains an AdaBoost classifier based on 200 *decision stumps* using scikit-learn's `AdaBoostClassifier` class (as you might expect, there is also an `AdaBoostRegressor` class). A decision stump is a decision tree with `max_depth = 1` -- in other words, a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the `AdaBoostClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "729b9920-71df-4934-bd8e-061cb9299dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_classifier = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), n_estimators = 200,\n",
    "                                    algorithm = \"SAMME.R\", learning_rate = 0.5)\n",
    "ada_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689db9d8-0acd-4ee6-b1c8-e7d251c3b0ab",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
