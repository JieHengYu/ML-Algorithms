{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6837b4a0-2e57-4ccf-8c5c-32fffe059a99",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. What is the approximate depth of a decision tree trained (without restrictions) on a training set with one million instances?\n",
    "2. Is a node's gini impurity generally lower or greater than its parent's? Is it *generally* lower/greater or *always* lower/greater?\n",
    "3. If a decision tree is overfitting the training set, is it a good idea to try decreasing `max_depth`?\n",
    "4. If a decision tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
    "5. If it takes one hour to train a decision tree on a training set containing 1 million instances, roughly how much time will it take to train another decision tree on a training set containing 10 million instances?\n",
    "6. If your training set contains 100,000 instances, will setting `presort = True` speed up training?\n",
    "7. Train & fine-tune a decision tree for the moons dataset by following these steps:\n",
    "   * Use `make_moons(n_samples = 10000, noise = 0.4)` to generate a moons dataset.\n",
    "   * Use `train_test_split()` to split the dataset into a training set & a test set.\n",
    "   * Use grid search with cross-validation (with the help of `GridSearchCV()` class) to find good hyperparameter values for a `DecisionTreeClassifier()`. Hint: try various values for `max_leaf_nodes`.\n",
    "   * Train it on the full training set using these hyperparameters, & measure your model's performance on the test set. You should get roughly 85% to 87% accuracy.\n",
    "8. Grow a forest by following these steps:\n",
    "   * Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: use scikit-learn's `ShuffleSplit()` class for this.\n",
    "   * Train one decision tree on each subset, using the best hyperparameter values found in the previous exercise. Evaluate these 1,000 decision trees on the test set. Since they were trained on smaller sets, these decision trees will likely perform worse than the first decision tree, achieving only about 80% accuracy. \n",
    "   * Now here comes the magic. For each test instance, generate the predictions of the 1,000 decision trees, & keep only the most frequent predictions (you can use scipy's `mode()` function for this). This approach gives you *majority-vote predictions* over the test set.\n",
    "   * Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a random forest classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b0fef-8039-4c5d-acf0-bd5a38134558",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c135da-ef2d-4df7-b46f-b04a4437ca5c",
   "metadata": {},
   "source": [
    "1. The default (unrestricted) depth of a decision tree on any training set is unlimited. You can change a maximum depth by setting the `max_depth` hyperparameter to some positive integer value.\n",
    "2. I think a node's gini impurity is generally lower than it's parents. The CART algorithm doesn't try to minimise an individual node's gini impurity. It tries to find the optimal split by minimising the weighted sum of the gini impurities of the left & right subset of the split. Because of this, I think it's possible that: for example, the gini impurity of a right subset could be higher than the gini impurity of the parent, as long as the gini impurity of the left subset can be low enough to compensate for the right subset.\n",
    "3. Yes. If a decision tree is overfitting the training set, you should decrease max_depth.\n",
    "4. The decision tree won't be affected by scaling the training set. Instead, to fix an underfitting decision tree, you should decrease the `min_*` & increase the `max_*` hyperparameters.\n",
    "5. If a decision tree containing 1 million instances takes 1 hour to train, with a training complexity of $O(n\\ *\\ m\\ log_2(m))$; then a decision tree containing 10 million instances should have a training complexity of $O(n\\ *\\ 10m\\ log_2(10m))$. Then the amount of time to train a decision tree with 10 million instances could be written like this: $\\frac{n\\ *\\ 10m\\ log_2(10m)}{n\\ *\\ m\\ log_2(m)}$, which can be reduced to $\\frac{10\\ log_2(10m)}{log_2(m)}$. If we substitute m with 100,000, then we would get $\\frac{232.53}{19.93} \\approx$ 11.67 times longer than it takes for a decision tree to train 1 million instances, so approximately 11 hours & 40 minutes.\n",
    "6. Setting `presort = True` only speeds up training for small training sets (less than a few thousand instances), so no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec7b0d-5f32-435e-b73e-73d0f2633ff2",
   "metadata": {},
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5b3db3-b942-4c1e-9bf7-dbffdcbe6e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65815584, -0.97254898],\n",
       "       [-0.51110222, -0.03089614],\n",
       "       [ 0.20462693,  1.31207084],\n",
       "       ...,\n",
       "       [-0.23060328,  1.0514219 ],\n",
       "       [ 0.67051093,  0.83189349],\n",
       "       [ 1.8084936 , -0.16454155]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples = 1000, noise = 0.4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 32)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b6ea7b-aeb5-444d-a2b4-774fa17d506e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'max_leaf_nodes': 5, 'min_samples_leaf': 20}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "param_search_space = {\"max_depth\": [x for x in range(1, 6)], \n",
    "                      \"min_samples_leaf\": [x for x in range(20, 31)], \n",
    "                      \"max_leaf_nodes\": [x for x in range(2, 7)]}\n",
    "grid_search = GridSearchCV(tree_classifier, param_search_space, cv = 5, n_jobs = 7,\n",
    "                           scoring = \"accuracy\", return_train_score = True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5191fc6f-3606-4674-8ca2-4e3b93652b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores:  [0.8125 0.85   0.9375 0.8875 0.85   0.75   0.85   0.8875 0.7875 0.8125]\n",
      "Mean:  0.8424999999999999\n",
      "StdDev:  0.05159941860137572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, \n",
    "                         scoring = \"accuracy\", cv = 10, n_jobs = 7)\n",
    "print(\"Accuracy Scores: \", scores)\n",
    "print(\"Mean: \", scores.mean())\n",
    "print(\"StdDev: \", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0f6188-0eb1-4120-a318-1fe00ad4e75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c755b51-4d70-4931-8380-339c5fcbfab5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e24e3-4b8a-4d5e-8b2c-6bbb86623ce6",
   "metadata": {},
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3429a1f1-d95b-44c0-8e02-2b9dc20368a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "all_subsets = []\n",
    "\n",
    "randomsplit = ShuffleSplit(n_splits = 1000, train_size = 100, random_state = 32)\n",
    "for train_index, test_index in randomsplit.split(X_train):\n",
    "    X_subset_train, y_subset_train = X_train[train_index], y_train[train_index]\n",
    "    all_subsets.append((X_subset_train, y_subset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af36c5da-3a0f-4614-afb4-d5e6264b645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.769685"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "forest = [clone(grid_search.best_estimator_) for _ in range(1000)]\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for tree, (X_mini_train, y_mini_train) in zip(forest, all_subsets):\n",
    "    tree.fit(X_mini_train, y_mini_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b909b0-ef7f-4787-a911-f08fbd6cf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.empty([1000, len(X_test)], dtype = np.uint8)\n",
    "\n",
    "for tree_index, tree in enumerate(forest):\n",
    "    y_pred[tree_index] = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec22865-082b-4dd8-8826-e43c398320b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.785"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "y_pred_majority, n_votes = mode(y_pred, axis = 0)\n",
    "accuracy_score(y_test, y_pred_majority.reshape([-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95011afd-cae3-4596-a8e0-b519ea85809c",
   "metadata": {},
   "source": [
    "Where is the magic?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
