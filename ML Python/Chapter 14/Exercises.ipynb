{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed16dfa-9da8-4c46-b2dd-6f18ba96b724",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "2. Consider a CNN composed of three convolutional layers, each with 3 x 3 kernels, a stride of 2, & `\"SAME\"` padding. The lowest layer outputs 100 feature maps, the middle on outputs 200, & the top one outputs 400. The input images are RGB images of 200 x 300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n",
    "3. If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n",
    "4. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "5. When would you want to add a local response normalisation layer?\n",
    "6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, & Xception?\n",
    "7. What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n",
    "8. What is the main technical difficulty of semantic segmentation?\n",
    "9. Build your own CNN from scratch & try to achieve the highest possible accuracy on MNIST.\n",
    "10. Use transfer learning for large image classification, going through these steps:\n",
    "   - Create a training set containing at least 100 images per class. For example, you could classify your own images based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow datasets)\n",
    "   - Split it into a training set, a validation set, & a test set.\n",
    "   - Build the input pipeline, including the appropriate preprocessing operations & optionally add data augmentation.\n",
    "   - Fine-tune a pretrained model on this dataset.\n",
    "11. Go through TensorFlow's style transfer tutorial. It is a fun way to generate art using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ce6a3-9c8b-493b-a00c-de3543611d2c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e821fbc-400c-497b-b243-602bdc11c29d",
   "metadata": {},
   "source": [
    "1. A fully connected DNN would work fine for small image classification, but it would become problematic as larger images. For example, a 100 x 100 pixel image has 10,000 pixels, & if the first layer of the network has 1000 neurons, then that is 10,000,000 connections, only for the first layer. You can kinda see how this might lead to an enormous number of parameters. CNNs can solve this problem by using partially connected layers & weight sharing. By na\n",
    "2. $$\\begin{split}\n",
    "(3 * 3 * 3 + 1) * 100 = 2700 \\\\\n",
    "(3 * 3 * 3 + 1) * 200 = 5400 \\\\ \n",
    "(3 * 3 * 3 + 1) * 400 = 10800 \\\\\n",
    "2700 + 5400 + 10800 = 18900\n",
    "\\end{split}$$\n",
    "The total number of parameters is 18900. During inference (i.e., when making a prediction for a new instance) the RAM occupied by one layer can be released as soon as the next layer has been computed, so you only need as much RAM as required by two consecutive layers. Since, the question asks \"at least\", we'll compute the amount of RAM used by the two layers that output the most feature maps.\n",
    "$$\\begin{split}\n",
    "200 * 200 * 300 * 32 = 384,000,000 \\\\\n",
    "400 * 200 * 300 * 32 = 768,000,000 \\\\\n",
    "384,000,000 + 768,000,000 = 1,152,000,000 \\\\\n",
    "1,152,000,000 * \\frac{1}{8,000,000} = 144\n",
    "\\end{split}$$\n",
    "If we are using 32-bit floats, the network will require at least 144MB of RAM when making a prediction for a single instance. During training, everything computed during the forward pass is preserved for the reverse pass, so the amount of RAM needed is (at least) the total amount of RAM required by all layers.\n",
    "$$\\begin{split}\n",
    "100 * 200 * 300 * 32 = 192,000,000 \\\\\n",
    "200 * 200 * 300 * 32 = 384,000,000 \\\\\n",
    "400 * 200 * 300 * 32 = 768,000,000 \\\\\n",
    "192,000,000 + 384,000,000 + 768,000,000 = 1,344,000,000 \\\\\n",
    "1,344,000,000 * \\frac{1}{8,000,000} = 168 \\\\\n",
    "168 * 50 = 8,400\n",
    "\\end{split}$$\n",
    "When we are training on a mini-batch of 50 images & using 32-bit floats, the network will require at least 8,400MB or 8.4GB of RAM.\n",
    "3. If training crashes due to out-of-memroy error, there are several ways to solve the problem. You can reduce the mini-batch size. Alternatively, you can reduce dimensionality by increasing stride. You can even remove some layers from your network. Instead of using 32-bit floats, you can use 16-bit floats. Or you can even distribute your CNN across multiple devices.\n",
    "4. You might want to add a max pooling layer rather than a convolutional layer with the same stride because a max pooling layer can reduce computational load, memory usage, & the number of parameters. For example, if we use a 2 x 2 kernel, with a stride of 2 & no padding, then only the max input value in each receptive fields will be passed as output to the next layers, while other inputs are dropped. Other than the aforementioned benefits, a max pooling layer introduces some level of invariance to small translations. Moreover, max pooling offers a small amount of rotational invariance & slight scale invariance as well, depending on the stride & kernel. Although there are many benefits of having max pooling layers, they are very destructive. They drop a huge percentage of the input variables. Also, sometimes, invariance isn't desirable, depending on the goal.\n",
    "5. You may want to add a local response normalisation layer to improve generalisation of your network. In a local response normalisation layer, the most strongly activated neurons inhibit other neurons located at the same position in neighbouring feature maps, encouraging different feature maps to specialise, pushing them apart & forcing them to explore a wider range of features, ultimately improving generalisation.\n",
    "6. Compared to LeNet-5, AlexNet was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer. AlexNet was also a much deeper network than LeNet-5, & included multiple regularisation techniques: dropout, data augmentation, & local response normalisation. GoogLeNet's network was much deeper than the previously mentioned networks, made possible by subnetworks called inception modules, which allowed GoogLeNet to use parameters more efficiently than previous architectures. These inception modules are configured to output fewer feature maps than their inputs, so they serve as bottleneck layers, meaning they reduce dimensionality, cutting the computation cost & the number of parameters, speeding up training & improving generalisation. ResNet was an extremely deep network that included skip connections. Its skip connections allowed for the network to make progress even if the layer has not started learning yet. The network can be seen as a stack of residual units (RUs), where each residual unit is a small neural network with a skip connection. Within the deep stack of residual units, the number of feature maps are doubled every few residual units; at the same time, their height & width are halved using a convolutional layer with stride 2. When this happenes, the inputs cannot be added directly to the output sof the residual unit because they don't have the same shape. To solve this problem, the inputs as passed through a 1 x 1 convolutional layer with stride 2 & the right number of output feature maps. Xception was a variant of the GoogleNet architecture, but it replaced the inception modules with a depthwise separable convolution layer (separable convolution layer for short). Whilea regular convolutional layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) & cross-channel patterns (e.g., mouth + nose + eyes = face), a separable convolution layer assumes that spatial patterns & cross-channel patterns can be modeled separately. Thus, it is composed of two parts: the first part applies a single spatial filter for each input filter map, while the second part looks exclusively for cross-channel patterns. SENet extends existing architectures such as inception moduels & residual units, but boosting their performance. The boost comes from the fact that a SENet adds a small neural network called an SE block to every unit in the original architecture (to every inception module & every residual unit). The SE block is composed of just three layers, a global average pooling layer, a hidden dense layer using the ReLU activation function & a dense output layer using the sigmoid activation function. The SE block analyses the output of the unit it is attached to, focusing exclusively on the depth dimension (it doesn't look for any spatial pattern), & it learns which features are usually most active together. For example, an SE Block may learn that mouths, noses, & eyes usually appear together in pictures. So if the block see a strong activation in the mouth & nose feature maps, but only mild activation in the eye feature map, it will boost the eye feature map by reducing the irrelevant feature map. If the eyes were somewhat confused with something else, this feature map recalibration will help resolve the ambiguity. The global average pooling layer computes the mean activation for each feature map: for example, if its inputs contains 256 feature maps, it will output 256 numbers representing the overall level of response for each filter. The next layer has significantly fewer -- typically 16 times less -- fewer than the number of feature maps -- so the 256 numbers get compressed into a small vector. This is a low dimensional vector representation (i.e., an embedding) of the distribution of feautre responses. The bottleneck stops the SE block to learn a general representation of the feature combinations. Finally, the output layer takes the low-dimensional representation (embedding) & outputs a recalibration vector containing one number per feature map (e.g., 256), each between 0 & 1. The feature maps are then multiplied by the recalibration vector, so irrelevant features (with a low recalibration score) get scaled down while relevant feature (with a recalibration score closer to 1) are left alone.\n",
    "7. A fully convolutional network (FCN) is a network that replaces the dense layers at the top of a regular CNN with convolutional layers. To convert a dense layer to a convolutional layer, the number of filters in the convolutional layer must be equal to the number of units in the dense layer, the filter size must be equal to the size of the input feature maps, & you must use `\"VALID\"` padding. The stride may be set to 1 or more.\n",
    "8. The main difficulty of semantic segmentation is that when images go through a regular CNN, they gradually lose their spatial resolution (due to the layers with strides greater than 1); so, a regular CNN may be able to recognise an object is located somewhere within an image, but it will not be much more precise than that. There are many different approaches to tackle this problem. A fairly simple solution would be to turn the CNN into a FCN & adding upsampling layers & skip connections. Upsampling is equivalent to stretching the image by inserting empty rows & columns (full of zeros), then performing regular convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df99aa2-7566-4b7d-8a9b-74a194f040db",
   "metadata": {},
   "source": [
    "# 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d030b27b-e6b8-4c05-ac05-8c6ab0d70d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_train, X_val = X_train[:-5000], X_train[-5000:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b15769-5908-45a6-a029-303d27c169f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 117ms/step - accuracy: 0.8716 - loss: 0.4111 - val_accuracy: 0.9904 - val_loss: 0.0398\n",
      "Epoch 2/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 117ms/step - accuracy: 0.9807 - loss: 0.0721 - val_accuracy: 0.9898 - val_loss: 0.0351\n",
      "Epoch 3/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 127ms/step - accuracy: 0.9858 - loss: 0.0495 - val_accuracy: 0.9922 - val_loss: 0.0324\n",
      "Epoch 4/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 123ms/step - accuracy: 0.9896 - loss: 0.0366 - val_accuracy: 0.9932 - val_loss: 0.0268\n",
      "Epoch 5/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 121ms/step - accuracy: 0.9904 - loss: 0.0337 - val_accuracy: 0.9922 - val_loss: 0.0317\n",
      "Epoch 6/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 121ms/step - accuracy: 0.9923 - loss: 0.0308 - val_accuracy: 0.9930 - val_loss: 0.0280\n",
      "Epoch 7/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 120ms/step - accuracy: 0.9930 - loss: 0.0263 - val_accuracy: 0.9914 - val_loss: 0.0437\n",
      "Epoch 8/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 118ms/step - accuracy: 0.9945 - loss: 0.0197 - val_accuracy: 0.9926 - val_loss: 0.0399\n",
      "Epoch 9/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 116ms/step - accuracy: 0.9941 - loss: 0.0219 - val_accuracy: 0.9944 - val_loss: 0.0288\n",
      "Epoch 10/10\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 123ms/step - accuracy: 0.9947 - loss: 0.0190 - val_accuracy: 0.9936 - val_loss: 0.0343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14c351e20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size = 3, padding = \"same\", activation = \"relu\",\n",
    "                        kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.Conv2D(64, kernel_size = 3, padding = \"same\", activation = \"relu\",\n",
    "                        kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.Conv2D(128, kernel_size = 3, padding = \"same\", activation = \"relu\",\n",
    "                        kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.Conv2D(128, kernel_size = 3, padding = \"same\", activation = \"relu\",\n",
    "                        kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(128, activation = \"relu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(64, activation = \"relu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 10, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9786fe-e6c5-4327-8808-bf192db4eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9925 - loss: 0.0301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.023824412375688553, 0.9940999746322632]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc615d-dadf-4b6e-bcf9-b10ad7bc2cef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2708fc2b-0b3c-4e75-8b32-5b81ec28c00f",
   "metadata": {},
   "source": [
    "# 10.\n",
    "Split the training data into training, validation, & testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "809a98a5-af3f-4d23-a42c-1d3ae786284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "test_set, val_set, train_set = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split = [\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05833a-1cfb-4d06-a6e4-7bb4281322f6",
   "metadata": {},
   "source": [
    "Write a preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb1b3d8-8ae2-4ae3-9869-c31d09dc14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491f1eb-9240-48ae-b258-535583d599f4",
   "metadata": {},
   "source": [
    "Apply the preprocessing function to all datasets, shuffle the training set, add batching, & prefetching to all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7369b23-04e4-43ce-8f55-a715fdb680cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_set = train_set.shuffle(1000)\n",
    "train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "val_set = val_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c66477-4946-49e0-9713-6cb2ada4f94f",
   "metadata": {},
   "source": [
    "Load an Xception model that is pretrained on image net, excluding the top of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d84599-9cbb-4f7a-9c53-a46b195ba49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.xception.Xception(weights = \"imagenet\",\n",
    "                                                  include_top = False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(5, activation = \"softmax\")(avg)\n",
    "model = keras.Model(inputs = base_model.input, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f601adbc-9696-4046-ac02-c62b778b2a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m998s\u001b[0m 11s/step - accuracy: 0.5854 - loss: 1.1498 - val_accuracy: 0.2341 - val_loss: 8212.8975\n",
      "Epoch 2/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m978s\u001b[0m 11s/step - accuracy: 0.6842 - loss: 0.8852 - val_accuracy: 0.3956 - val_loss: 4.1313\n",
      "Epoch 3/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1038s\u001b[0m 12s/step - accuracy: 0.7861 - loss: 0.6142 - val_accuracy: 0.5880 - val_loss: 1.7458\n",
      "Epoch 4/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1136s\u001b[0m 13s/step - accuracy: 0.8618 - loss: 0.3893 - val_accuracy: 0.7731 - val_loss: 0.7086\n",
      "Epoch 5/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1082s\u001b[0m 13s/step - accuracy: 0.8713 - loss: 0.3751 - val_accuracy: 0.4156 - val_loss: 5.9274\n",
      "Epoch 6/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1109s\u001b[0m 13s/step - accuracy: 0.8864 - loss: 0.3650 - val_accuracy: 0.8221 - val_loss: 0.6647\n",
      "Epoch 7/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1117s\u001b[0m 13s/step - accuracy: 0.9230 - loss: 0.2204 - val_accuracy: 0.8457 - val_loss: 0.4706\n",
      "Epoch 8/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1057s\u001b[0m 12s/step - accuracy: 0.9362 - loss: 0.1800 - val_accuracy: 0.7858 - val_loss: 0.6210\n",
      "Epoch 9/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1088s\u001b[0m 13s/step - accuracy: 0.9419 - loss: 0.1749 - val_accuracy: 0.8512 - val_loss: 0.5038\n",
      "Epoch 10/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1025s\u001b[0m 12s/step - accuracy: 0.9599 - loss: 0.1200 - val_accuracy: 0.7895 - val_loss: 0.7282\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate = 0.2, momentum = 0.9, decay = 0.01)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "history = model.fit(train_set, epochs = 10, validation_data = val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f629ac4-5677-48df-bd34-605bcfbfc0a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ba7f7-90a0-4c44-8835-edf2ba08f2b3",
   "metadata": {},
   "source": [
    "# 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056188d9-d54a-4dd2-9347-f1ff289431db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
