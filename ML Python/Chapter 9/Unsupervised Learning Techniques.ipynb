{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a491f6b-b8f9-4469-a0d9-753497f30431",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Techniques\n",
    "\n",
    "Although most of the applications of machine learning today are based on supervised learning (& as a result, this is where most of the investments go to), the vast majority of the available data is unlabeled: we have the input features $X$, but we do not have the labels $y$. The computer scientist Yann LeCun famously said that \"if intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, & reinforcement learning would be the cherry on the cake.\" In other words, there is a huge potential in unsupervised learning that we have only barely started to sink our teeth into.\n",
    "\n",
    "Say you want to create a system that will take a few pictures of each item on a manufacturing production line & detect which items are defective. You can fairly easily create a system that will take pictures automatically, & this might give you thousands of pictures every day. you can then build a reasonably large dataset in just a few weeks. But wait, there is no labels! If you want to train a regular binary classifier that will predict whether an item is defective or not, you will need to label every single picture as 'defective\" or \"normal\". This will generally require human experts to sit down & manually go through all the pictures. This is a long, costly, & tedious task, so it will usually only be done ona small subset of available pictures. As a result, the labeled dataset will be quite small, & the classifier's performance will be disappointing. Moreover, every time the company makes any change to its products, the whole process will need to be started over from scratch. Wouldn't it be great if the algorithm could just exploit the unlabeled data without needing humans to label every picture? Enter unsupervised learning.\n",
    "\n",
    "In this lesson, we will look at a few unsupervised learning tasks & algorithms:\n",
    "\n",
    "* *Clustering*\n",
    "   - The goal is to group similar instance together into *clusters*. Clustering is a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, & more.\n",
    "* *Anomaly detection*\n",
    "   - The objective is to learn what \"normal\" data looks like, & then use that to detect abnormal instances, such as defective items on a production line or a new trend in a time series.\n",
    "* *Density estimation*\n",
    "   - This is the task of estimating the *probability density function* (PDF) of the random process that generated the dataset. Density estimation of commonly used for anomaly detection: instances located in very low-density regions are likely to be anomalies. It is also useful for data analysis & visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44afcdb-911b-4a41-8e4c-302022926908",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8e3c2-5c10-44af-bdc6-10478180ea83",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "As you enjoy a hike in the mountains, you stumble upon a plant you have never seen before. You look around & you notice a few more. They are not identical, yet they are sufficiently similar for you to know that they most likely belong to the same species (or at least the same genus). You may need a botanist to tell you what species that is, but you certainly don't need an expert to identify groups of similar-looking objects. This is called *clustering*: it is the task of identifying similar instances & assigning them to *clusters*, or groups of similar instances.\n",
    "\n",
    "Just like in classification, each instance gets assigned to a group. However, unlike classification, clustering is an unsupervised task. \n",
    "\n",
    "<img src = \"Images/Classification vs Clustering.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Consider the left diagram in the figure: on the left is the iris dataset, where each instance's species is represented with a different marker. It is a labeled dataset, for which classification algorithms such as logistic regression, SVMs, or random forest classifiers are well suited. On the right is the same dataset, but without labels, so you cannot use classification algorithm anymore. This is where clustering algorithms step in: many of them can easily detect the lower-left cluster. It is also quite easy to see with your own eyes, but it is not so obvious that the upper-right cluster is composed of two distinct sub-clusters. That siad, the dataset has two additional features (sepal length & width), not represented here, & clustering algorithms can make good use of all features, so in fact they identify the three clusters fairly well (e.g., using a Gaussian mixture model, only 5 instances out of 150 are assigned to the wrong cluster).\n",
    "\n",
    "Clustering is used in a wide variety of applications, including these:\n",
    "\n",
    "* *For customer segmentation*\n",
    "   - You can cluster your customers based on their purchases & their activity on your website. This is useful to understand who your customers are & what they need, so you can adapt your products & marketing campaigns to each segment. For example, customer segmentation can be useful in *recommender systems* to suggest content that other users in the same cluster enjoyed.\n",
    "* *For data analysis*\n",
    "   - When you analyse a new dataset, it can be helpful to run a clustering algorithm, & then analyse each cluster separately.\n",
    "* *As a dimensionality reduction technique*\n",
    "   - Once a dataset has been clustered, it is usually possible to measure each instance's *affinity* with each cluster (affinity is a measure of how well an instance fits into a cluster). Each instance's feature vector $x$ can then be replaced with the vector of its cluster affinities. If there are *k* clusters,then this vector is *k*-dimensional. This vector is typically much lower-dimensional than the original feature vector, but it can preserve enough information for further processing.\n",
    "* *For anomaly detection (also called outlier detection)*\n",
    "   - Any instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users of your website based on their behaviour, you can detect users with unusual behaviour, such as an unusual number of requests per second. Anomaly detection is particularly useful in detecting defects in manufacturing, or for *fraud detection*.\n",
    "* *For semi-supervised learning*\n",
    "   - If you only have a few labels, you could perform clustering & propagate the labels to all the instances in the same cluster. This technique can greatly increase the number of labels available for a subsequent supervised learning algorithm, & thus improve its performance.\n",
    "* *For search engines*\n",
    "   - Some search engines let you search for images that are similar to a reference image. To build such a system, you would first apply a clustering algorithm to all the images in your database; similar images would end up in the same cluster. Then when a user provides a reference image, all you need to do is use the trained clustering model to find this image's cluster, & you can then simply return all the images from this cluster.\n",
    "* *To segment an image*\n",
    "   - By clustering pixels according to their colour, then replacing each pixel's colour with the mean colour of its cluster, it is possible to considerably reduce the number of different colors in the image. Image segmentation is used in many object detection & tracking systems, as it makes it easier to detect the contour of each object.\n",
    "  \n",
    "There is no universal definition of what a cluster is: it really depends on the context & different algorithms will capture different kinds of clusters. Some algorithms look for instance centered around a particular point, called a *centroid*. Others look for continuous regions of densely packed instances: these clusters can take on any shape. Some algorithms are hierarchical, looking for clusters of clusters. & the list goes on.\n",
    "\n",
    "In this section, we'll look at two popular clustering algorithms, K-means & DBSCAN, & explore some of their applications, such as nonlinear dimensionality reduction, semi-supervised learning, & anomaly detection.\n",
    "\n",
    "## K-Means\n",
    "\n",
    "Consider the unlabeled dataset represented in this figure, you can clearly see five blobs of instances.\n",
    "\n",
    "<img src = \"Images/Unlabeled Blobs.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "The K-means algorithm is a simple algorithm capable of clustering this kind of dataset very quickly & efficiently, often in just a few iterations. It as proposed by Stuart Lloyd at Bell Labs in 1957 as a technique for pulse-code modulation, but it was only published outside of the company in 1982. In 1965, Edward W. Forgy had published virtually the same algorithm, so K-means is sometimes referred to as Lloyd-Forgy.\n",
    "\n",
    "Let's train a K-means clusterer on this dataset. It will try to find each blob's center & assign each instance to the closest blob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f1076d-4b15-4c93-90df-618e1568e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "blob_centers = np.array([[ 0.2,  2.3],\n",
    "                         [-1.5 ,  2.3],\n",
    "                         [-2.8,  1.8],\n",
    "                         [-2.8,  2.8],\n",
    "                         [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "X, y = make_blobs(n_samples = 2000, centers = blob_centers,\n",
    "                  cluster_std = blob_std, random_state = 32)\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1f628-97ea-40ec-bdda-8bb8d59f762c",
   "metadata": {},
   "source": [
    "Note that you have to specify the number of clusters *k* that the algorithm must find. In this example, it is pretty obvious from looking at the data that *k* should be set to 5, but it general it is not that easy.\n",
    "\n",
    "Each instance was assigned to one of the five clusters. In the context of clustering, an instance's label is the index of the cluster that this instance gets assigned to by the algorithm: this is ot to be confused with the class labels in classification (remember that clustering is an unsupervised learning task). The `KMeans` instance preserves a copy of the labels of the instances it was trained on, available via the `labels_`instance variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f79ffd01-00cd-4c85-b2d6-74955b79072f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 0, ..., 0, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad58f6f1-50aa-421c-857c-f91257e83619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred is kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89fda5-c3ee-4fae-b031-cfe349cefd88",
   "metadata": {},
   "source": [
    "We can also take a look at the five centroids that the algorithm found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4054e4-70d8-4833-b3a9-65583a0091d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.48628063,  2.26054806],\n",
       "       [-2.78301719,  2.80833515],\n",
       "       [ 0.21050882,  2.34183312],\n",
       "       [-2.80254262,  1.29573614],\n",
       "       [-2.80720418,  1.80874278]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a95e57-8305-4861-bb23-f637586e4a2d",
   "metadata": {},
   "source": [
    "You can easily assign new instances to the cluster whose centroid is closest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa55b31-b69d-47bc-a221-62f6a3c56f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6454db10-8e4e-41e8-8da0-4cb38bc17d41",
   "metadata": {},
   "source": [
    "If you predict the cluster's decision boundaries, you get a Voronoi tesselation, where each centroid is represented with an X.\n",
    "\n",
    "<img src = \"Images/K-Means Decision Boundaries.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "The vast majority of the instances were clearly assigned to the appropriate cluster, but a few instances were probably mislabeled (especially near the boundary between the top-left cluster & the central clsuter). Indeed, the K-means algorithm does not behave very well then the blobs have very different diameters because all it cares about when assigning an instance to a cluster is the distance to the centroid.\n",
    "\n",
    "Instead of assigning each instance to a single cluster, which is called *hard clustering*, it can be useful to give each instance a score per cluster, which is called *soft clustering*. The score can be the distance between the instance & the centroid; conversely, it can be a similarity score (or affinity), such as the gaussian radial basis function. In the `KMeans` class, the `transform()` method measures the distance from each instance to every centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2396bc08-1d51-4112-8d9b-d80d8cd179ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.50894513, 2.89803216, 0.40145218, 2.88967693, 2.8137119 ],\n",
       "       [4.49384014, 5.83923741, 2.81035779, 5.84512519, 5.8103528 ],\n",
       "       [1.68467667, 0.28951158, 3.27727792, 1.71566451, 1.20675763],\n",
       "       [1.53254153, 0.37703064, 3.21440254, 1.22034457, 0.71763972]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4813cb-8b39-477c-a53b-634d6c3a1c9d",
   "metadata": {},
   "source": [
    "In this example, theh first example in `X_new` is located at a distance of 1.51 from the first centroid, 2.90 from the second centroid, 0.40 from the third centroid, 2.89 from the fourth centroid, & 2.81 from the fifth centroid. If you have a high-dimensional dataset & you transform it this way, you end up with a *k*-dimensional dataset: this transformation can be a very efficient nonlinear dimensionality reduction technique.\n",
    "\n",
    "### The K-Means Algorithm\n",
    "\n",
    "So, how does the algorithm work? Well, suppose you were given the centroids. You could easily label all the instances in the dataset by assigning each of them to the cluster whose centroid is closest. Conversely, if you were given all the instance labels, you could easily locate all teh centroids by computing the mean of the instances for each cluster. But you are given neither the labels nor the centroids, so how can you proceed? Well, just start by placing the centroids randomly (e.g., by picking the *k* instances at random & using their locations as centroids). Then label the instances, update the centroids, label the instances, update the centroids, & so on until the centroids stop moving. The algorithm is guaranteed to converge in a finite number of steps (usually quite small); it will not oscillate forever.\n",
    "\n",
    "You can see the algorithm in action here: the centroids are initialised randomly (top left), then the instances are labeled (top right), then the centroids are updated (center left), the instances are relabeled (center right), & so on. As you can see, in just three iterations, the algorithm has reached a clustering that seems close to optimal.\n",
    "\n",
    "<img src = \"Images/K-Means Algorithm.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Although the algorithm is guaranteed to converge, it may not converge to the right solution (i.e., it may converge to a local optimum): whether it does or not depends on the centroid initialisation. The below figure shows two suboptimal solutions that the algorithm can converge to if you are unlucky with the random initialisation step.\n",
    "\n",
    "<img src = \"Images/Suboptimal Solution K-Means.png\" width = \"600\" style = \"margin:auto\"/>\n",
    "\n",
    "Let's look at a few ways you can mitigate this risk by improving the centroid intialisation.\n",
    "\n",
    "### Centroid Initialisation Methods\n",
    "\n",
    "If you happen to know approximately where the centroids should be (e.g., if you ran another clustering algorithm earlier), then you can set the `init` hyperparameter to a numpy array containing the list of centroids, & set `n_init` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aae90e5-fce1-4f31-8c78-045e7880a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans = KMeans(n_clusters = 5, init = good_init, n_init = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3b6a9-cfa2-456e-907d-1ccf892de316",
   "metadata": {},
   "source": [
    "Another solution is to run the algorithm multiple times with different random intialisations & keep the best solution. The number of random intialisations is controlled by the `n_init` hyperparameter: by default, it is equal to 10, which means that the whole algorithm decribed earlier runs 10 times when you call `fit()`, & scikit-learn keeps the best solution. But how exactly does it know which solution is the best? It uses a performance metric. That metrics is called the model's *inertia*, which is the mean squared distance between each instance & its cloest centroid. It is roughly equal to 223.3 & 237.5 on the left & right of the above figure, respectively. The `KMeans` class runs the algorithm `n_init` times & keeps the model with the lowest inertia. In this example,the model will be selected (unless we are very unlucky with `n_init` consecutive random initialisations). If you are curious, a model's inertia is accessible via the `inertia_` instance variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aecf95ce-10de-4af5-9a85-8745d23afa62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216.0712975215635"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(X)\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cfa513-93f4-4974-8ed7-d797aef574e8",
   "metadata": {},
   "source": [
    "The `score()` method returns the negative inertia. Why negative? Because a predictor's `score()` method must always respect scikit-learn's \"greater is better\" rule: if a predictor is better than another, its `score()` method should return a greater score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1689a1a-88b1-40c2-a603-7927fec8c631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-216.0712975215635"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e58c9-e730-4af5-b719-79591e9e448c",
   "metadata": {},
   "source": [
    "An important improvement to the K-means algorithm, *K-Means++*, was proposed in a 2006 paper by David Arthur & Sergei Vassilvitskii. They introduced a smarter initialisation step that tends to select centroids that are distant from one another, & this improvement makes the K-means algorithm much less likely to converge to a suboptimal solution. They showed that the additional computation required for the smarter initialisation step is well worth it because it makes it possiblel to drastically reduce the number of times the algorithm needs to be run to find the optimal solution. here is the K-Means++ initialisation algorithm.\n",
    "\n",
    "1. Take one centroid $c^{(i)}$, chosen uniformly at random from the dataset.\n",
    "2. Take a new centroid $c^{(i)}$, choosing an instance $x^{(i)}$ with probability $D(x^{(i)})^2/\\sum^{m}_{j = 1}D(x^{(j)})^2$, where $D(x^{(i)})$ is the distance between the instance $x^{(i)}$ & the closest centroid that has already chosen. This probability distribution ensures that instances farther away from already chosen centroids are much more likely to be selected as centroids.\n",
    "3. Repeat the previous step until all *k* centroids have been chosen.\n",
    "\n",
    "The *KMeans* class uses this utilisation method by default. If you want to force it to use the original method (i.e., picking *k* instances randomly to define the intial centroids), then you can set the `init` hyperparameter to \"random\". You will rarely need to do this.\n",
    "\n",
    "### Accelerated K-means & Mini-Batch K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fcdd3d-41d7-42ff-8817-4f7aff58205e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
