{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74cfdd3c-49ac-4eb5-ae11-ed525058cc77",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. How would you define clustering? Can you name a few clustering algorithms?\n",
    "2. What are some of the main applications of clustering algorithms?\n",
    "3. Describe two techniques to select the right number of clusters when using K-means.\n",
    "4. What is label propagation? Why would you implement it & how?\n",
    "5. Can you name two clustering algorithms that can scale to large datasets? & two that look for regions of high density?\n",
    "6. Can you think of a use case where active learning would be useful? How would you implement it?\n",
    "7. What is the difference between anomaly detection & novelty detection?\n",
    "8. What is a gaussian mixture? What tasks can you use it for?\n",
    "9. Can you name two techniques to find the right number of clusters when using a gaussian mixture model?\n",
    "10. The classic Olivetti faces dataset contains 400 grayscale 64x64 pixel images of faces. Each image is flattened to a 1D vector of size 4096. 40 different people were photographed (10 times each), & the usual task is to train a model that can predict which persion is represented in each picture. Load the dataset using the `sklearn.datasets.fetch_olivetti_faces()` function, the split it into a training set, a validation set, & a test set (note that the dataset is already scaled between 0 & 1). Since the dataset is quite small, you probably want ot use stratified sampling to ensure that there are the same number of images per person in each set. Next, cluster the images using K-means, & ensure that you have a good number of clusters (using one of the techniques discuessed in this lesson). Visualise theclusters: do you see similar faces in each cluster?\n",
    "11. Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, & evaluate it on the validation set. Next, use K-means as a dimensionality reduction tool 7 train a classifier on the reduced set. Search for the number of clusters that allows the classifier to get the best performance: waht performance can you reach? What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?\n",
    "12. Train a gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the dataset's dimensionality (e.g., use PCA, preserving 99% of the variance). Use the model to generate some new faces (using the `sample()` method), & visualise them (if you used PCA, you will need to use it `inverse_transform()` method). Try to modify some images (e.g., rotate flip, darken) & see if the model can detect the anomalies (i.e., compare the output of the `score_samples()` method for normal images & for anomalies).\n",
    "13. Some dimensionality reduction techniques can be used for anomaly detection. For example, take the Olivetti faces dataset & reduce it with PCA, preserving 99% of the variance. Then compute the reconstruction error for each image. Next, take some of the modified images you built in the previous exercise, & look at their reconstruction error: notice how much larger the reconstruction error is. If you plot a reconstructed image, you will see why: it tries to reconstruct a normal face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4dd5e-b029-4453-b6ad-6001b3c83d22",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ae549-77a5-478d-813e-72f0e8136995",
   "metadata": {},
   "source": [
    "1. Clustering is identifying similar instances or regions of high density, & assigning them to clusters, or groups of similar instances. Some clustering algorithms include K-means, DBSCAN, agglomerative clustering, Birch, mean-shift, etc.\n",
    "2. Clustering algorithms are used in a wide variety of tasks, including: *(a)* customer segmentation: you can group customers based on their purchases or activity, which is useful for targeted advertisements; *(b)* data analysis: it can be helpful to run a clustering algorithm on a new dataset & then analyse each cluster separately; *(c)* dimensionality reduction: after clustering, it's possible to replace each instance's feature vector $x$ with a vector of its cluster affinities, which is typically much lower-dimensional than the original feature vector $x$; *(d)* anomaly detection: instances with low affinity to all clusters are likely to be anomalies -- this is useful for detecting unusual behaviour, fraud, & defects in manufacturing; *(e)* semi-supervised learning: if you only have a few labels, you can use clustering & propagate the labels to all instances in the same cluster; greatly increasing the number of labels available for subsequent supervised learning & improving performance; *(f)* image segmentation: you can cluster pixels of an image accourding to their colour & replace each pixel's colour with the mean colour of its cluster; considerably reducing the number of collours in an images; this can be useful for object detection & tracking systems.\n",
    "3. Inertia & silhouette score. Inertia is the mean squared distance between each instance & its closest centroid. You can then plot the inertia against different values of *k* clusters & select the value of *k* that is closest to the inflection point of the curve. The silhouette score is the mean silhouette coefficient over all instances. An instance's silhouette coefficient is equal to (b - a)/max(a, b) where a is the mean distance to the other instances in the same cluster (mean intra-cluster distance) & b is the mean distance to the instances in the next closest cluster (mean nearest-cluster distance). The silhouette coefficient is any value between -1 & 1: 1 means that the instance is well inside its own cluster, 0 means it is close to the cluster boundary, -1 means it may be assigned to the wrong cluster. We take the mean of the silhouette coefficients to get the silhouette score. We can plot this silhouette score against different values of *k* clusters & pick the value of *k* where the silhouette score is the greatest.\n",
    "4. In semi-supervised learning, since only a few instances are labeled, we can propagate the labels to instances of the same cluster, thereby creating a fully labeled dataset. You would implement label propagation to hopefully increase the performance of the clustering algorithm. You can perform label propagation by taking a representative sample of instances from the training set, & manually labeling them. Then, propagate these labels to instances of the same cluster. If you compared the clustering algorithms performance trained on the labeled representative sample vs after the propagation, you should see an increase in performance, assuming the labels that were propagated were accurate. The way we propagated the labels applies to all instances in the same cluster, including the instances closer to the cluster boundaries, which are more likely to be mislabeled. If we only propagate the labels to the n% of the instances that are closest to the cluster centroids, then we would see even higher clustering performance.\n",
    "5. With a connectivity matrix, agglomerative clustering or birch can effective clustering algorithms for large datasets. DBSCAN & mean-shift are two clustering algorithms that look for regions of high density.\n",
    "6. Active semi-supervised learning can be great for computer vision or image recognition. Using a small amount of labeled data, we can propagate the labels to instances of the same cluster, particularly those who are closer to the cluster centroids. For the instances that are closer to the cluster borders, the algorithm can be more uncertain about which clusters the instance belongs to. The algorithm can then pose the task to the user for the instances it is uncertain of. The user would then identify which cluster the instance belongs to, & the algorithm would take that information for its training. This process will then iterate until all instances are labeled or until the algorithm's performance increase doesn't justify the effort of labeling anymore.\n",
    "7. Anomaly detection is the task of detecting instances that deviate strongly from the norm, or anomalies, so it is trained on a dataset with outliers. Novelty detection assumes that it is trained on a dataset without outliers, so that when it comes time for predictions, it can identify new or unknown patterns that the algorithm was not exposed to during training.\n",
    "8. A gaussian mixture is a probabilities model that assumes that the instances were generated from a mixture of several gaussian distributions whose parameters are unknown. All instances generated from a gaussian distribution form a cluster that is typically ellipsoidal in shape, but can be different shapes, sizes, densities, & orientations. Gaussian mixture models can be used for density estimation, clustering, & anomaly detection.\n",
    "9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc32f6-f35c-441f-97cc-f18a7ce3d7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
