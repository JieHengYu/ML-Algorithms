{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b0790e-813d-4b79-9e29-7adb283a1670",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. What are the main motivations for reducing a dataset's dimensionality? What are the main drawbacks?\n",
    "2. What is the curse of dimensionality?\n",
    "3. Once a dataset's dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?\n",
    "4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
    "5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
    "6. In what cases would you use vanilla PCA, incremental PCA, randomised PCA, or kernel PCA?\n",
    "7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?\n",
    "8. Does it make any sense to chain two different dimensionality reduction algorithms?\n",
    "9. Load the MNIST dataset & split it into a training set & a test set (take the first 60,000 instances for training, & the remaining 10,000 for testing). Train a random forest classifier on the dataset & time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%. Train a new random forest classifier on the reduced dataset & see how long it takes. Was training much faster? Next, evaluate the classifier on the test set: how does it compare to the previous classifier?\n",
    "10. Use t-SNE to reduce the MNIST dataset down to two dimensions & plot the results using matplotlib. You can use a scatterplot using 10 different colors to represent each image's target class. alternatively, you can write colored digits at the location of each instance, or even plot a scaled-down versions of the digit images themselves (if you plot all the digits, the visualisation will be too clusttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nie visualisation with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS & compare the resulting visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1737b0b-c9ef-4878-8efa-f3fcc312fa36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0571db-16da-4ca8-a2be-f477b7e95d17",
   "metadata": {},
   "source": [
    "1. Sometimes when a dataset is super large, it can take a long time to train a model. By reducing a dataset's dimensionality, we can speed up the amount of time taken to train a model & possibly remove noise from the training set. Dimensionality reduction does have its drawbacks, such as losing some information, & making your system more complex/harder to maintain.\n",
    "2. The curse of dimensionality refers to the phenomena that arise when data has many dimensions: high-dimensional datasets are at risk of being very sparse, so your trianing instances are likely very far away from one another. This also means that new instances will be far away from any training instances, making predictions less reliable than in lower dimensions. As such, it is easier to overfit a model to the training set.\n",
    "3. Yes, but you do lose some of the information when you reverse the operation. Use the `inverse_transform()` method for PCA. It's called reconstruction. Once you reconstruct your dataset from your reduced-dimensionality dataset, then you can find the reconstruction error, which compares the reconstruction to the original.\n",
    "4. Depends. For something like the Swiss roll dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f95eb-ca5e-4e28-9fd3-6dbfd1b01840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
