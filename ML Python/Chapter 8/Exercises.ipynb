{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b0790e-813d-4b79-9e29-7adb283a1670",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. What are the main motivations for reducing a dataset's dimensionality? What are the main drawbacks?\n",
    "2. What is the curse of dimensionality?\n",
    "3. Once a dataset's dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?\n",
    "4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
    "5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
    "6. In what cases would you use vanilla PCA, incremental PCA, randomised PCA, or kernel PCA?\n",
    "7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?\n",
    "8. Does it make any sense to chain two different dimensionality reduction algorithms?\n",
    "9. Load the MNIST dataset & split it into a training set & a test set (take the first 60,000 instances for training, & the remaining 10,000 for testing). Train a random forest classifier on the dataset & time how long it takes, then evaluate the resulting model on the test set. Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%. Train a new random forest classifier on the reduced dataset & see how long it takes. Was training much faster? Next, evaluate the classifier on the test set: how does it compare to the previous classifier?\n",
    "10. Use t-SNE to reduce the MNIST dataset down to two dimensions & plot the results using matplotlib. You can use a scatterplot using 10 different colors to represent each image's target class. alternatively, you can write colored digits at the location of each instance, or even plot a scaled-down versions of the digit images themselves (if you plot all the digits, the visualisation will be too clusttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nie visualisation with well-separated clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS & compare the resulting visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1737b0b-c9ef-4878-8efa-f3fcc312fa36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee2de1-1b23-4757-bcfe-1d4513ad5583",
   "metadata": {},
   "source": [
    "1. The main benefit of reducing a dataset's dimensionality is that it significantly speeds up training, but it makes the data easier for us to visualise. By reducing the dimensionality, you are guaranteed to lose some information. It also makes your pipelines more complex & harder to maintain. Also, while reducing the dimensionality could potentially filter out noise & unnecessary details, thus resulting in a higher performance, it could also not & just speed up training.\n",
    "2. The curse of dimensionality is a phenomena that you see with high-dimensional datasets. There is plenty of space in high dimensions, so high-dimensional datasets tend to be very sparse; most training instances are far away from one another. This also means that new instances that we want to predict on are also far away from the training instances, making predictions less reliable than in lower dimensions, since they will be based on much larger extrapolations. Our model is also more likely to overfit the training set, because or model will be more complex & less generalizable.\n",
    "3. Once a dataset's dimensionality is reduced, it isn't possible to reverse the operation entirely, because it's already lost information. You can only reverse the operation up to a certain point & the result is a dataset that is similar, but not the same, to the original dataset. This is reconstruction. You can get a measure of the difference between the reconstructed dataset & the original dataset with the reconstruction error.\n",
    "4. Of course PCA can be used, though it will depend on the shape of the dataset. If the training instances lie close to a lower-dimensional subspace of our higher-dimensional dataset, then PCA will perform well compressing the dataset. However, if the shape of the dataset can be modeled with a manifold, then a manifold learning algorithm, like LLE, will perform better than our PCA projection. Using a manifold learning algorithm assumes that a lower-dimensional manifold is part of the higher dimensional data & the manifold resembles a hyperplane. It also assumes that the training instances lie close to the lower-dimensional manifold. Regardless of the algorithm used, it just depends on the shape of the data. Dimensionality reduction might not even help, depending on the end goal. It all just depends.\n",
    "5. It depends on the proportion of the variance each principal component explains. Whatever number of principal components where the summed proportion of variances is greater or equal to 95% is the number of dimensions the resulting dataset will have.\n",
    "6. Vanilla PCA is for for dataset compression. It reduces the nubmer of dimensions while trying to preserve as much infromation as possible. Randomised PCA is an approximation of the first *d* principal components (the first principal component always contains the most information (variability), then the second, then the third, & so on. You choose *d*.), so its faster than vanilla PCA. It is automatically used if the number of instances or features is greater than 500 & *d* is less than 80% of the number of instances or features. Basically, randomised PCA is used when the training set is large. Incremental PCA doesn't require that the whole training set be fit into memory for the algorithm to run, unlike the aforementioned PCA methods. The training set is split into mini-batches & fed to the algorithm one mini-batch at a time, making this particularly useful for large training sets & online training. Kernel PCA uses the kernel trick to reduce the dimensionality of a very high-dimensional feature space.\n",
    "7. You can look at how much the data is compressed. Take the number of dimensions (features) in your original training set & take the number of dimensions in your compressed data set. Put the former over the latter.\n",
    "8. It could. Maybe you use PCA first to reduce a dataset into 4 principal components, & within those 4 principle components exists a manifold. Then you would want to use manifold learning. Though I think it would highly depend on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbf81a-dd49-4659-baaa-ee1927065ab5",
   "metadata": {},
   "source": [
    "# 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b4ba88-8ea9-454b-a87b-98be485feb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version = 1, as_frame = False)\n",
    "mnist.keys()\n",
    "X, y = mnist[\"data\"].astype(np.intc), mnist[\"target\"].astype(np.intc)\n",
    "\n",
    "strat_split = StratifiedShuffleSplit(n_splits = 1, test_size = 10000, random_state = 32)\n",
    "for train_index, test_index in strat_split.split(X, y):\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_test, y_test = X[test_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "660ed1f8-7798-405f-bbbd-de9fabe68756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.87036204338074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "forest_classifier = RandomForestClassifier()\n",
    "start = time.time()\n",
    "forest_classifier.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a40575-ba3d-4cbd-be2f-5ebaf18a76d7",
   "metadata": {},
   "source": [
    "It took about 45 seconds to train our random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc59e726-c9c3-44bf-8571-d4cd529f5a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9692"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = forest_classifier.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a4b84c7-8d3c-46d4-8f08-cc3ca2a4dabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 0.95)\n",
    "X_train_PC = pca.fit_transform(X_train)\n",
    "len(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eaa586-7833-4221-bcc4-3e6f6ee4f3d0",
   "metadata": {},
   "source": [
    "We still keep over 95% of the information in our training set when we reduce it from 784 to 154 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e29613c-0159-4d05-ac18-fadfde697360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.6408200263977\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "forest_classifier.fit(X_train_PC, y_train)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0d2b7-e609-4966-b66d-a513915694cd",
   "metadata": {},
   "source": [
    "Wtf, it took longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac0ce7b7-1916-48f0-8cb5-d2b7019b647b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 151 features, but DecisionTreeClassifier is expecting 154 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/mwqj58td1tlf_3mrmgghsykh0000gp/T/ipykernel_54219/4124720930.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_test_PC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_PC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\",\n\u001b[0m\u001b[1;32m    408\u001b[0m                                     reset=False)\n\u001b[1;32m    409\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure_2d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
      "\u001b[0;31mValueError\u001b[0m: X has 151 features, but DecisionTreeClassifier is expecting 154 features as input."
     ]
    }
   ],
   "source": [
    "X_test_PC = pca.transform(X_test)\n",
    "y_pred = forest_classifier.predict(X_test_PC)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a8cd3-25ad-45d2-80cf-6c57b05cad41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
