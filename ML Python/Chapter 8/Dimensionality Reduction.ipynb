{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4caf6156-b83f-4d8c-8d3c-c6b587d8b97e",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Many machine learning problems involve thousands or even millions of features for each training instance. Not only does this make training extremely slow, it can also make it much harder to find a good solution. This problem is often referred to as the *curse of dimensionality*.\n",
    "\n",
    "Fortunately, in real-world problems, it is often possible to reduce the number of features considerably, turning an intractable problem into a tractable one. For example, consider the MNIST images: the pixels on the image borders are almost always white, so you could completely drop these pixels from the training set without losing much information. Moreover, two neighboring pixels are often highly correlated: if you merge them into a single pixel (e.g., by taking the mean of the two pixel intensities), you will not lose much information.\n",
    "\n",
    "Apart from speeding up training, dimensionality reduction is also extremely useful for data visualisation. Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of a high-dimensional training set on a graph & often gain some important insights by visually detecting patterns, such as clusters. Moreover, data visualisation is essential to communicate your conclusions to people who are not data scientists, in particular, decison makers who will use your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a15283-e9c5-467e-89a8-4994083f7f3d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e123bd-a249-449f-bfc1-34069974e6e3",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "\n",
    "We are so used to living in three dimensions that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind, let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.\n",
    "\n",
    "<img src = \"Images/Multi-Dimensions.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "It turns out that many things behave very differently in a high-dimensional space. For example, if you pick a random point in a unit square (1 x 1 square), it will have a 0.4% chance of being located less than 0.001 from a border (in other words, it will be very unlikely that a random point will be \"extreme\" along any dimension). But in a 10,000-dimensional unit hypercube (a 1 x 1 x ... x 1 cube, with ten thousand 1s), this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close to the border.\n",
    "\n",
    "Here is a more troublesome difference: if you pick two points randomly on a unit square, the distance between these two points will be, on average, roughly 0.52. If you pick two random points in a unit 3D cube, the average distance will be roughly 0.66. But what about two points picked randomly in a 1,000,000-dimensional hypercube? Well the average distance, believe it or not, will be about 408.25 (roughly $\\sqrt{1,000,000/6}$ )! This is quite counterintuitive: how can two points be so far apart when they both lie within the same unit hypercube? This fact implies that high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. Of course, this also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations. In short, the more dimensions the training set has, the greater the risk of overfitting it.\n",
    "\n",
    "In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density of training instances. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features (much less than in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacae544-0ff8-4c85-8f3e-80cd233d5621",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f453faf-5c29-4d06-8591-fc6d205f0b49",
   "metadata": {},
   "source": [
    "# Main Approches for Dimensionality Reduction\n",
    "\n",
    "Let's take a look at the two main approaches to reducing dimensionality: projection & manifold learning.\n",
    "\n",
    "## Projection\n",
    "\n",
    "In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated (as discussed earlier for MNIST). As a result, all training instances actually lie within (or close to) a much lower-dimensional *subspace* of the high-dimensional space. See the 3D dataset represented below.\n",
    "\n",
    "<img src = \"Images/3D Dataset in 2D Subspace.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Notice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of a high-dimensional (3D) space. Now if we project every training instance perpendicularly onto this subspace (as represented by the short lines connecting the instances to the plane), we get the new 2D dataset show in the below figure. Ta-da! We have just reduced the dataset's dimensionality from 3D to 2D. Note that the axes correpsond to new features $z_1$ & $z_2$ (the coordinates of the projections on the plane).\n",
    "\n",
    "<img src = \"Images/2D Dataset After Projection.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "However, projection is not always the best approach to dimensionality reduction. In many cases, the subspace may twist & turn, such as in the famous *Swiss roll* toy dataset represented below.\n",
    "\n",
    "<img src = \"Images/Swiss Roll Dataset.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "Simply projecting onto a plane (e.g., by dropping $x_3$) would squash different layers of the Swiss roll together, as shown below on the left. However, what you really want is to unroll the Swiss roll to obtain the 2D dataset on the right.\n",
    "\n",
    "<img src = \"Images/Squashing vs Unrolling Swiss Roll.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "## Manifold Learning\n",
    "\n",
    "The Swiss roll is an example of a 2D *manifold*. Put simply, a 2D manifold is a 2D shape that can be bent & twisted in a higher-dimensional space. More generally, a *d*-dimensional manifold is a part of an *n*-dimensional space (where *d* < *n*) that locally resembles a *d*-dimensional hyperplane. In the case of the Swiss roll, *d* = 2 & *n* = 3: it locally represents a 2D plane, but it is rolled in the third dimension.\n",
    "\n",
    "Many dimensionality reduction algroithms work by modeling the *manifold* on which the training instances lie; this is called *Manifold Learning*. It relies on the *manifold assumption*, also called the *manifold hypothesis*, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed.\n",
    "\n",
    "Once again, think about the MNIST dataset: all handwritten digit images have some similarities. They are mde of connected lines, the borders are white, they are more or less centered, & so on. If you randomly generated images, only a ridiculously tiny fraction of them would look like handwritten digits. In other words, the degrees of freedom available to you if you try to create a digit image are dramatically lower than the degrees of freedom you would have if you were allowed to generate any image you wanted. These constraints tend to squeeze the dataset into a lower-dimensional manifold.\n",
    "\n",
    "The manifold assumption is often accompanied by another implicit assumption: that the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold. For example, in the top row fo the below figure, the Swiss roll is split into two classes: in the 3D space (on the left), the decision boundary would be farily complex, but in the 2D unrolled manifold space (on the right), the decision boundary is a simple straight line.\n",
    "\n",
    "<img src = \"Images/Swiss Roll Decision Boundary.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "However, this assumption does not always hold. For example, in the bottom row, the decision boundary is located at $x_1 = 5$. This decision boundary looks very simple in the original 3D space (a vertical plane), but it looks more complex in the unrolled manifold (a collection of four independent line segments).\n",
    "\n",
    "In short, if you reduce the dimensionality of your training set before training a model, it will usually speed up training, but it may not always lead to a better or simpler solution; it all depends on the dataset. \n",
    "\n",
    "Hopefully, you now have a good sense of what the curse of dimensionality is & how dimensionality reduction algorithms can fight it, especially when the manifold assumption holds. The rest of this chapter will go through some of the most popular algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb0f89-8a30-439a-a8e4-c3ba6590a29e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ef2de-65ae-49d2-a0cf-89df50c60bac",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "*Principal Component Analysis* (PCA) is by far the most popular dimensionality reduction algorithm. First, it identifies the hyperplane that lies closest to the data, & then it projects the data onto it.\n",
    "\n",
    "## Preserving the Variance\n",
    "\n",
    "Before you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane. For example, a simple 2D dataset is represented on the left of the below figure, along with three different axes (i.e., one-dimensional hyperplanes). On the right is the result of the projection of the dataset onto each of these axes. As you can see, the projection onto the solid line preserves the maximum variance, while the projection onto the dotted line preserves very little variance, & the projection onto the dashed line preserves an intermediate amount of variance.\n",
    "\n",
    "<img src = \"Images/Projecting Onto A Subspace.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "It seems reasonable to select the axis that preserves the maximum amount of variance, as it will most likely lose less information than the other projections. Another way to justify this choice is that it is the axis that minimises the mean squared distance between the original dataset & its projection onto that axis. This is the idea behind *PCA*.\n",
    "\n",
    "## Principal Components\n",
    "\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the training set. In the previous figure, it is a solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example, there is no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA would also find a third axis, orthogonal to both previous axes, & a fourth, a fifth, & so on -- as many axes as the number of dimensions in the dataset.\n",
    "\n",
    "The unit vector that defines the $i^{th}$ axis is called the *$i^{th}$ principal component* (PC). In the previous figure, the $1^{st}$ PC is $c_1$, & the $2^{nd}$ PC is $c_2$. \n",
    "\n",
    "So how can you find the principal components of a training set? Luckily, there is a standard matrix factorization technique called *Singular Value Decomposition* (SVD) that can decompose the training set matrix $X$ into the matrix multiplication of three matrices $U \\sum V^T$, where $V$ contains all the principal components that we are looking for, as shown in the below equation.\n",
    "\n",
    "$$V = (c_1, c_2, ..., c_n)$$\n",
    "\n",
    "The following code uses numpy's `svd()` function to obtain all the principal components of the training set, then extracts the first two PCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caf1b2ef-2edc-48be-b199-2c5691b07309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2dc45-0776-49a6-b3ef-2525cbd5ef5d",
   "metadata": {},
   "source": [
    "## Projecting Down to *d* Dimensions\n",
    "\n",
    "Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to *d* dimensions by projecting it onto the hyperplane defined by the first *d* principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible. For example, when we projected our 3D dataset down to the 2D plane, defined by the first two principal components, preserving a large part of the dataset's variance. As a result, the 2D projection looks very much like the original 3D dataset.\n",
    "\n",
    "To project the training set onto the hyperplane, you can simply compute the matrix multiplication of the training set matrix $X$ by the matrix $W_d$, defined as the matrix containing the first $d$ principal components (i.e., the matrix composed of the first *d* columns of $V$), as shown in the below equation.\n",
    "\n",
    "$$X_{d-proj} - XW_d$$\n",
    "\n",
    "The following python code projects the training set onto the plane defined by the first two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04b2739-1f93-4132-b211-0078301c86cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d022da-81e8-4fd3-9667-e5f8efc39f2e",
   "metadata": {},
   "source": [
    "There you have it. You now know how to reduce the dimensionality of any dataset down to any number of dimensions, while preserving as much variance as possible.\n",
    "\n",
    "## Using Scikit-Learn\n",
    "\n",
    "Scikit-learn's PCA class implements PCA using SVD decomposition just like we did before. The following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically takes care of centering the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922d7b0b-7449-4e2b-8e4c-a288e44248a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255764d-e6e3-44b0-92fd-ae554c47d3b4",
   "metadata": {},
   "source": [
    "After fitting the `PCA` transformer to the dataset, you can access the principal components using the `components_` variable (not that it contains the PCs as horizontal vectors, so, for example, the first principal component is equal to `pca.components_.T[:, 0]`).\n",
    "\n",
    "## Explained Variance Ratio\n",
    "\n",
    "Another very useful piece of information is the *explained variance ration* of each principal component, available via the `explained_variance_ratio_` variable. It indicates the proportion of the dataset's variance that lies along the axis of each principal component. For example, let's look at the explained variance ratios of the first two components of the 3D dataset that we projected onto a 2D subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fa74e2-0583-40eb-aec5-b4206acfa77f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5390c-77c8-4473-b5c6-6b28e25195d2",
   "metadata": {},
   "source": [
    "This tells us that 84.2% of the dataset's variance lies along the first axis, & 14.6% lies along the second axis. This leaves less than 1.2% for the third axis, so it is reasonable to assume that it probably carries little information.\n",
    "\n",
    "## Choosing the Right Number of Dimensions\n",
    "\n",
    "Instead of arbitrarily choosing the number of dimensions to reduce it down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for data visualisation -- in that case, you will generally want to reduce the dimensionality down to 2 or 3. The following code computes PCA without reducing dimensionality, then computes the minimum of dimensions required to preserve 95% of the training set's variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6411c88a-bb3e-4e6b-877a-0297632acf2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version = 1, as_frame = False, parser = \"auto\")\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5889d-4121-4fde-81e1-f7456008a7a4",
   "metadata": {},
   "source": [
    "You could then set `n_components = d` & run PCA again. However, there is a much better option: instead of specifying the number of principal components you want to preserve, you can set `n_componets` to be a float between `0.0` & `1.0`, indicating the ratio of variance you wish to preserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ce1dd8-8311-4265-bc39-3d59f1dc8e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9f513-76cd-488a-a678-c47694fbfc51",
   "metadata": {},
   "source": [
    "Yet another option is to plot the exaplined variance as a function of the number of dimensions (simply plot `cumsum`). There will usually be an elbow in the curve, where the explained variance stops growing fast. You can think of this as the intrinsic dimensionality of the dataset. In this case, you can see that reducing the dimensionality down to about 100 dimensions wouldn't lose too much explained variance.\n",
    "\n",
    "<img src = \"Images/Explained Variance as Function of Number of Dimensions.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "## PCA for Compression\n",
    "\n",
    "Obviously after dimensionality reduction, the training set takes up much less space. For example, try appling PCA to the MNIST dataset while preserving 95% of its variance. You should find that each instance will have just over 150 features, instead of the original 784 features. So while most of the variance is preserved, the dataset is now less than 20% of its original size! This is a reasonable compression ratio, & you can see how this can speed up a classification algorithm (such as an SVM classifier) tremendously.\n",
    "\n",
    "It is almost possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection. Of course this won't give you back to the original data, since the projection lost a bit of information (within the 5% variance that was dropped), but it will likely be quite close to the original data. The mean squared distance between the original data & the reconstructed data (compressed & then decompressed) is called the *reconstruction error*. For example, the following code compresses the MNIST dataset down to 154 dimensions, then uses the `inverse_transform()` method to decompress it back to 784 dimensions. The below figure shows a few digits from the original training set (on the left) & the corresponding digits after compression & decompression. You can see that there is a slight image quality loss, but the digits are still mostly intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e9646c-fed3-4a14-99f9-c08e0476415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1abd8f6-e3e2-43a1-a7d1-407396712907",
   "metadata": {},
   "source": [
    "<img src = \"Images/MNIST Compression Preservation.png\" width = \"500\" style = \"margin:auto\"/>\n",
    "\n",
    "The equation of the inverse transformation is shown in the equation below.\n",
    "\n",
    "$$X_{recovered} = X_{d-proj}W_d^T$$\n",
    "\n",
    "## Randomised PCA\n",
    "\n",
    "If you set the `svd_solver` hyperparameter to `\"randomised\"`, scikit-learn uses a stochastic algorithm called *Randomised PCA* that quickly finds an approximation of the first *d* principal components. Its computational complexity is $O(m * d^2) + O(d^3)$, instead of $O(m * n^2) + O(n^3)$ for the full SVD approach, so it is dramatically faster than full SVD when *d* is much smaller than *n*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aafa848-5415-469c-92e2-c8691d1fbc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components = 154, svd_solver = \"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db79c14-ed9b-414a-8f76-cb6c9c011b48",
   "metadata": {},
   "source": [
    "By default, `svd_solver` is actually set to `\"auto\"`: scikit-learn automatically uses the randomised PCA algorithm if *m* or *n* is greated than 500 & *d* is less than 80% of *m* or *n*, or else it uses the full SVD approach. If you want to force scikit-learn to use full SVD, you can set the `svd_solver` hyperparameter to `\"full\"`.\n",
    "\n",
    "## Incremental PCA\n",
    "\n",
    "One problem with preceding implementations of PCA is that they require the whole training set to fit in memory in order for the algorithm to run. Fortunately, *Incremental PCA* (IPCA) algorithms have been developed: you can split the training set into mini-batches & feed an IPCA algorithm one mini-batch at a time. This is useful for large training sets, & also to apply PCA online (i.e., on the fly, as new instances arrive). The following code splits the MNIST dataset into 100 mini-batches (using numpy's `array_split()` function) & feeds them to scikit-learn's `IncrementalPCA` class to reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like before). Not that you must call the `partial_fit()` method with each mini-batch rather than the `fit()` method with the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ef1ebdf-82b8-4d03-93a3-62ab24f25506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components = 154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d87b2-dc30-4ceb-ab34-7c9564d4f20e",
   "metadata": {},
   "source": [
    "Alternatively, you can use numpy's `memmap` class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory; the class loads only the dataset it needs in memory, when it needs it. Since the `IncrementalPCA` class uses only a small part of the array at any given time, the memory usage remains under control. This makes it possible to call the usual `fit()` method, as you can see in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d5c0f3-2ba0-4985-b4b7-2b96500fdc8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IncrementalPCA(batch_size=525, n_components=154)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;IncrementalPCA<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.decomposition.IncrementalPCA.html\">?<span>Documentation for IncrementalPCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>IncrementalPCA(batch_size=525, n_components=154)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "IncrementalPCA(batch_size=525, n_components=154)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"my_mnist.data\"\n",
    "m, n = X_train.shape\n",
    "\n",
    "X_mm = np.memmap(filename, dtype = 'float32', mode = 'write', shape = (m, n))\n",
    "X_mm[:] = X_train\n",
    "\n",
    "del X_mm\n",
    "\n",
    "X_mm = np.memmap(filename, dtype = \"float32\", mode = \"readonly\", shape = (m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components = 154, batch_size = batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f68ae8-e714-485f-afce-4320c50e5b7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9e689-f563-4df5-90d4-fb9c760e87b2",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "\n",
    "In previous lessons, we discussed the kernel trick, a mathematical technique that implicitly maps instances into a very high-dimensional space (called the *feature space*), enabling nonlinear classification & regression with support vector machines. Recall that a linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the *original space*. \n",
    "\n",
    "It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. This is called *Kernel PCA* (kPCA). It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.\n",
    "\n",
    "For example, the following code uses scikit-learn's `KernelPCA` class to perform kPCA with an RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95e9efe3-be17-4a16-a5e1-5cc7315ac799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Takes way too long\n",
    "#rbf_pca = KernelPCA(n_components = 2, kernel = \"rbf\", gamma = 0.04)\n",
    "#X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc6dfc-f412-4d32-99ed-56d658cc1d22",
   "metadata": {},
   "source": [
    "The below figure shows the Swiss roll, reduced to two dimensions using a linear kernel (equivalent to simply using the `PCA` class), an RBF kernel, & a sigmoid kernel (logistic).\n",
    "\n",
    "## Selecting a Kernel & Tuning Hyperparameters\n",
    "\n",
    "As kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you select the best kernel & hyperparameter values. However, dimensioality reduction is often a preparation step for a supervised learning task (e.g., classification), so you can simply use grid search the kernel & hyperparameters that lead to the best performance on that task. For example, the following code creates a two-step pipeline, first reducing dimensionality to two dimensions using kPCA, then applying logistic regression for classification. Then it uses `GridSearchCV` to find the best kernel & gamma value for kPCA in order to get the best classification accuracy at the end of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a28f8ead-a3b8-4fa4-a051-0f5186c6b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Takes way too long\n",
    "#classifier = Pipeline([(\"kpca\", KernelPCA(n_components = 2)),\n",
    "#                       (\"log_reg\", LogisticRegression())])\n",
    "#param_search_space = [{\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "#                       \"kpca__kernel\":[\"rbf\", \"sigmoid\"]}]\n",
    "#grid_search = GridSearchCV(classifier, param_search_space, cv = 3)\n",
    "#grid_search.fit(X, y)\n",
    "#grid_Search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f2649-cf43-48a5-bb59-46b92e61b4e5",
   "metadata": {},
   "source": [
    "Another approach, this time entirely unsupervised, is to select a kernel & hyperparameters that yield the lowest reconstruction error. However, reconstruction is not as easy as with linear PCA. Here's why. The below figure shows the original Swiss roll 3D dataset (top left), & the resulting 2D dataset after kPCA is applied using an RBF kernel (top right). Thanks to the kernel trick, this is mathematically equivalent to mapping the training set to an infinite-dimensional feature space (bottom right) using the *feature map* $\\varphi$, then projecting the transformed training set down to 2D using linear PCA. Notice that if we could invert the linear PCA step for a given instance in the reduced space, the reconstructed point would lie in the feature space, not in the original space (e.g., like the one represented by an x in the diagram). Since the feature space is infinite-dimensional, we cannot compute the reconstructed point, & therefore we cannot compute the true reconstruction error. Fortunately, it is possible to find a point in the original space that would map close to the reconstructed point. This is called the reconstruction *pre-image*. Once you have this pre-image, you can measure its squared distance to the original instance. You can then select the kernel & hyperparameters that minimise this reconstruction pre-image error.\n",
    "\n",
    "<img src = \"Images/Kernel PCA & Reconstruction Pre-Image Error.png\" width = \"450\" style = \"margin:auto\"/>\n",
    "\n",
    "You may be wondering how to perform this reconstruction. One solution is to train a supervised regression model, with the projected instances as the training set & the original instances as the targets. Scikit-lelarn will do this automatically if you set `fit_inverse_transform = True`, as shown in the folowing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414a93d0-a2de-40b9-8a7b-1204bb460f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes way too long\n",
    "#rbf_pca = KernelPCA(n_components = 2, kernel = \"rbf\", gamma = 0.0433,\n",
    "#                    fit_inverse_transform = True)\n",
    "#X_reduced = rbf_pca.fit_transform(X)\n",
    "#X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ebbc8-37c9-4e04-ad40-cbe169b168ef",
   "metadata": {},
   "source": [
    "You can then compute the reconstruction pre-image error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4359d68c-97e6-49f6-8333-4407fcc5eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Takes way too long\n",
    "#mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0543b-fe9d-4338-9af4-c7946a23b34e",
   "metadata": {},
   "source": [
    "Now you can use grid search with cross-validation to find the kernel & hyperparameters that minimise this pre-image reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1266e70-a775-4884-a196-b55e86b3a899",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26196804-f7f7-4940-b06d-84d834902bd4",
   "metadata": {},
   "source": [
    "# LLE\n",
    "\n",
    "*Local Linear Embedding* (LLE) is another very powerful *nonlinear dimensionality reduction* (NLDR) technique. It is a manifold learning technique that does not rely on projections like the previous algorithms. In a nuteshel, LLE works by first measuring how each training instance linearly relates to its closest neighbors, & then looking for a low-dimensional representation of the training set where these local relationships are best preserved. This makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise.\n",
    "\n",
    "For example, the following code uses scikit-learn's `LocallyLinearEmbedding` class to unroll the Swiss roll. The resulting 2D dataset is shown.\n",
    "\n",
    "<img src = \"Images/Unroll Swiss Roll with LLE.png\" width = \"450\" style = \"margin:auto\"/>\n",
    "\n",
    "As you can see, the Swiss roll is completely unrolled & the distances between instances are locally well preserved. However, distances are not preserved on a larger scale: the left part of the unrolled swiss roll is stretched, while the right part is squeezed. Nevertheless, LLE did a pretty good job at modeling the manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a371489-6c16-496a-a2bb-aec096361f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "# Takes too long\n",
    "#lle = LocallyLinearEmbedding(n_components = 2, n_neighbors = 10)\n",
    "#X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0432951-f53c-42ba-a92b-07b4a6c9b9e0",
   "metadata": {},
   "source": [
    "Here's how LLE works: first, for each training instance $x^{(i)}$, the algorithm identifies its *k* closest neighbors (in the preceding code *k* = 10), then tries to reconstruct $x^{(i)}$ as a linear function of these neighbors. More specifically, it finds the weights $w_{i, j}$ such that the squared distance between $x^{(i)}$ & $\\sum^{m}_{j = 1}w_{i, j}x^{(j)}$ is as small as possible, assuming $w_{i, j} = 0$ if $x^{(j)}$ is not one of the *k* closest neighbors of $x^{(i)}$. Thus the first step of LLE is the constrained optimisation problem described in the below eqution, where $W$ is the weight matrix containing all the weights $w_{i, j}$. The second constraint simply normalises the weights for each training instance $x^{(i)}$.\n",
    "\n",
    "$$\\begin{split}\n",
    "\\hat{W} = \\underset{w}{argmin} \\sum^{m}_{i = 1} (x^{(i)} - \\sum^{m}_{j = 1} w_{i, j}x^{(j)})^2 \\\\\n",
    "subject\\ to\\ \\Biggl\\{\\begin{split}\n",
    "w_{i, j} = 0 \\quad if\\ x^{(j)}\\ is\\ not\\ one\\ of\\ the\\ k\\ closest\\ neighbors\\ of\\ x^{(i)}\\\\\n",
    "\\sum^{m}_{j = 1} m_{i, j} = 1 \\quad for\\ i = 1, 2, ..., m\n",
    "\\end{split}\n",
    "\\end{split}$$\n",
    "\n",
    "After this step, the weight matrix $\\hat{W}$ (containing the weights $\\hat{w}_{i, j}$) encodes the local linear relationships between the training instances. Now the second step is to map the training instances into a *d*-dimensional space (where *d* < *n*) while preserving these local relationships as much as possible. If $z^{(i)}$ is the image of $x^{(i)}$ in this *d*-dimensional space, then we want the squared distance between $z^{(i)}$ & $\\sum^{m}_{j = 1}\\hat{w}_{i, j}z^{(j)}$ to be as small as possible. This idea leads to the unconstrained optimisation problem described in the below equation. It looks similar to the first step, but instead of keeping the instances fixed & finding the optimal position of the instance's images in the low-dimensional space. Note that $Z$ is the matrix containing all $z^{(i)}$.\n",
    "\n",
    "$$\\hat{Z} = \\underset{Z}{argmin} \\sum^{m}_{i = 1} (z^{(i)} - \\sum^{m}_{j = 1} \\hat{w}_{i, j}z^{(j)})^2$$\n",
    "\n",
    "Scikit-learn's LLE implementation has the following computational complexity: $O(m\\ log(m)n\\ log(k))$ for finding the *k* nearest neighbors, $O(mnk^3)$ for optimising the weights, & $O(dm^2)$ for constructing the low-dimensional representations. Unfortunately, the $m^2$ in the last term makes this algorithm scale poorly to very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec96aee-9324-44c5-bdc3-23f884ae12d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ef9d7-e514-4229-bf70-60b800058800",
   "metadata": {},
   "source": [
    "# Other Dimensionality Reduction Techniques\n",
    "\n",
    "There are many other dimensionality reduction techniques, several of which are available in scikit-learn. Here are some of the most popular:\n",
    "\n",
    "* *Multidimensional Scaling* (MDS) reduces dimensionality while trying to preserve the distances between the instances.\n",
    "* *Isomap* creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the *geodesic distances* between the instances.\n",
    "* *t_Distributed Stochastic Neighbors Embedding* (t-SNE) reduces dimensionality while trying to keep similar instances close & dissimilar instances apart. It is mostly used for visualisation, in particular to visualise clusters of instances in high-dimensional space (e.g., to visualise the MNIST images in 2D).\n",
    "* *Linear Discriminant Analysis* (LDA) is actually a classification algorithm, but during training, it learns the most discriminative axes between the classes, & these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier.\n",
    "\n",
    "<img src = \"Images/Various Dimensionality Reduction Techniques.png\" width = \"600\" style = \"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb995a-f2a7-4ca6-aabd-40a2f891114d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
